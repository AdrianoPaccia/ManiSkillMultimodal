diff --git a/examples/baselines/sac/sac.py b/examples/baselines/sac/sac.py
index 89a629e..3154b3a 100644
--- a/examples/baselines/sac/sac.py
+++ b/examples/baselines/sac/sac.py
@@ -344,7 +344,7 @@ if __name__ == "__main__":
     global_steps_per_iteration = args.num_envs * (args.steps_per_env)
 
     while global_step < args.total_timesteps:
-        print(f"Global Step: {global_step}")
+        print(f"Global Step: {global_step}", end='\r')
         if args.eval_freq > 0 and (global_step - args.training_freq) // args.eval_freq < global_step // args.eval_freq:
             # evaluate
             actor.eval()
diff --git a/scripts/baselines/README.md b/scripts/baselines/README.md
new file mode 100644
index 0000000..1805b44
--- /dev/null
+++ b/scripts/baselines/README.md
@@ -0,0 +1,9 @@
+# ManiSkill Baselines
+
+This folder contains code for all implemented ManiSkill baselines which currently include online Reinforcement Learning and Imitation Learning. All baseline results are published to our [public wandb page](https://wandb.ai/stonet2000/ManiSkill?nw=a37ldrsc2y).
+
+For more details on baselines (e.g. how to setup maniskill for RL, run baselines etc.) follow the links below in our documentation:
+
+- Online Reinforcement Learning: https://maniskill.readthedocs.io/en/latest/user_guide/reinforcement_learning/index.html
+- Learning From Demonstrations / Imitation Learning: https://maniskill.readthedocs.io/en/latest/user_guide/learning_from_demos/index.html
+
diff --git a/scripts/baselines/diffusion_policy/.gitignore b/scripts/baselines/diffusion_policy/.gitignore
new file mode 100644
index 0000000..6f99979
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/.gitignore
@@ -0,0 +1,4 @@
+__pycache__/
+runs/
+wandb/
+*.egg-info/
diff --git a/scripts/baselines/diffusion_policy/README.md b/scripts/baselines/diffusion_policy/README.md
new file mode 100644
index 0000000..bbfcc83
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/README.md
@@ -0,0 +1,92 @@
+# Diffusion Policy
+
+Code for running the Diffusion Policy algorithm based on ["Diffusion Policy: Visuomotor Policy Learning via Action Diffusion"](https://arxiv.org/abs/2303.04137v4). It is adapted from the [original code](https://github.com/real-stanford/diffusion_policy).
+
+## Installation
+
+To get started, we recommend using conda/mamba to create a new environment and install the dependencies
+
+```bash
+conda create -n diffusion-policy-ms python=3.9
+conda activate diffusion-policy-ms
+pip install -e .
+```
+
+## Demonstration Download and Preprocessing
+
+By default for fast downloads and smaller file sizes, ManiSkill demonstrations are stored in a highly reduced/compressed format which includes not keeping any observation data. Run the command to download the demonstration and convert it to a format that includes observation data and the desired action space.
+
+```bash
+python -m mani_skill.utils.download_demo "PickCube-v1"
+```
+
+```bash
+env_id="PickCube-v1"
+python -m mani_skill.trajectory.replay_trajectory \
+  --traj-path ~/.maniskill/demos/${env_id}/motionplanning/trajectory.h5 \
+  --use-first-env-state \
+  -c pd_joint_delta_pos -o state \
+  --save-traj --num-procs 10
+```
+
+## State-Based Training
+
+Example training, learning from 100 demonstrations generated via motionplanning in the PickCube-v1 task
+```bash
+seed=42
+demos=100
+env_id="PickCube-v1"
+python train.py --env-id ${env_id} --max_episode_steps 100 --total_iters 30000 \
+  --control-mode "pd_joint_delta_pos" --num-demos ${demos} --seed ${seed} \
+  --demo-path ~/.maniskill/demos/${env_id}/motionplanning/trajectory.state.pd_joint_delta_pos.cpu.h5 \
+  --exp-name diffusion_policy-${env_id}-state-${demos}_motionplanning_demos-${seed} \
+  --demo_type="motionplanning" --track # additional tag for logging purposes on wandb
+```
+In tensorboard/wandb there are two success rates reported, `success_once` and `success_at_end`. `success_once` considers success when the episode achieves success at any point in the episode, and `success_at_end` considers success only when the episode achieves success at the last step after `max_episode_steps` are reached.
+
+Note that we further add a `--max_episode_steps` argument to the training script to allow for longer demonstrations to be learned from (such as motionplanning / teleoperated demonstrations). By default the max episode steps of most environments are tuned lower so reinforcement learning agents can learn faster. You may need to increase this value depending on the task and the demonstrations you are using. 
+
+## Train and Evaluate with GPU Simulation
+
+You can also choose to train on trajectories generated in the GPU simulation and evaluate faster with the GPU simulation. However as most demonstrations are usually generated in the CPU simulation (via motionplanning or teleoperation), you may observe worse performance when evaluating on the GPU simulation vs the CPU simulation.
+
+It is also recommended to not save videos if you are using a lot of parallel environments as the video size can get very large.
+
+```bash
+seed=42
+demos=100
+python train.py --env-id ${env_id} --max_episode_steps 100 --total_iters 30000 \
+  --control-mode "pd_joint_delta_pos" --num-demos ${demos} --seed ${seed} \
+  --demo-path ~/.maniskill/demos/${env_id}/motionplanning/trajectory.state.pd_joint_delta_pos.cuda.h5 \
+  --exp-name diffusion_policy-${env_id}-state-${demos}_motionplanning_demos-${seed} \
+  --sim-backend="gpu" --num-eval-envs 100 --no-capture-video \
+  --demo_type="motionplanning" --track # additional tag for logging purposes on wandb
+```
+
+## Citation
+
+If you use this baseline please cite the following
+```
+@inproceedings{DBLP:conf/rss/ChiFDXCBS23,
+  author       = {Cheng Chi and
+                  Siyuan Feng and
+                  Yilun Du and
+                  Zhenjia Xu and
+                  Eric Cousineau and
+                  Benjamin Burchfiel and
+                  Shuran Song},
+  editor       = {Kostas E. Bekris and
+                  Kris Hauser and
+                  Sylvia L. Herbert and
+                  Jingjin Yu},
+  title        = {Diffusion Policy: Visuomotor Policy Learning via Action Diffusion},
+  booktitle    = {Robotics: Science and Systems XIX, Daegu, Republic of Korea, July
+                  10-14, 2023},
+  year         = {2023},
+  url          = {https://doi.org/10.15607/RSS.2023.XIX.026},
+  doi          = {10.15607/RSS.2023.XIX.026},
+  timestamp    = {Mon, 29 Apr 2024 21:28:50 +0200},
+  biburl       = {https://dblp.org/rec/conf/rss/ChiFDXCBS23.bib},
+  bibsource    = {dblp computer science bibliography, https://dblp.org}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/diffusion_policy/baselines.sh b/scripts/baselines/diffusion_policy/baselines.sh
new file mode 100644
index 0000000..4e697f0
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/baselines.sh
@@ -0,0 +1,10 @@
+seed=42
+demos=100
+for env_id in PickCube-v1
+do
+  python train.py --env-id ${env_id} --max_episode_steps 100 --total_iters 50000 \
+    --control-mode "pd_ee_delta_pose" --num-demos ${demos} --seed ${seed} \
+    --demo-path ~/.maniskill/demos/${env_id}/motionplanning/trajectory.state.pd_ee_delta_pose.h5 \
+    --exp-name diffusion_policy-${env_id}-state-${demos}_motionplanning_demos-${seed} \
+    --demo_type="motionplanning" --track # additional tag for logging purposes on wandb
+done
\ No newline at end of file
diff --git a/scripts/baselines/diffusion_policy/diffusion_policy/conditional_unet1d.py b/scripts/baselines/diffusion_policy/diffusion_policy/conditional_unet1d.py
new file mode 100644
index 0000000..f2adc11
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/diffusion_policy/conditional_unet1d.py
@@ -0,0 +1,264 @@
+#@markdown ### **Network**
+#@markdown
+#@markdown Defines a 1D UNet architecture `ConditionalUnet1D`
+#@markdown as the noies prediction network
+#@markdown
+#@markdown Components
+#@markdown - `SinusoidalPosEmb` Positional encoding for the diffusion iteration k
+#@markdown - `Downsample1d` Strided convolution to reduce temporal resolution
+#@markdown - `Upsample1d` Transposed convolution to increase temporal resolution
+#@markdown - `Conv1dBlock` Conv1d --> GroupNorm --> Mish
+#@markdown - `ConditionalResidualBlock1D` Takes two inputs `x` and `cond`. \
+#@markdown `x` is passed through 2 `Conv1dBlock` stacked together with residual connection.
+#@markdown `cond` is applied to `x` with [FiLM](https://arxiv.org/abs/1709.07871) conditioning.
+
+"""
+Note: This is copied from the colab notebook.
+The main difference with the github repo code is in `class ConditionalUnet1D` -- this version makes some simplifications.
+"""
+
+
+from typing import Union
+
+import torch
+import torch.nn as nn
+import math
+
+class SinusoidalPosEmb(nn.Module):
+    def __init__(self, dim):
+        super().__init__()
+        self.dim = dim
+
+    def forward(self, x):
+        device = x.device
+        half_dim = self.dim // 2
+        emb = math.log(10000) / (half_dim - 1)
+        emb = torch.exp(torch.arange(half_dim, device=device) * -emb)
+        emb = x[:, None] * emb[None, :]
+        emb = torch.cat((emb.sin(), emb.cos()), dim=-1)
+        return emb
+
+
+class Downsample1d(nn.Module):
+    def __init__(self, dim):
+        super().__init__()
+        self.conv = nn.Conv1d(dim, dim, 3, 2, 1)
+
+    def forward(self, x):
+        return self.conv(x)
+
+class Upsample1d(nn.Module):
+    def __init__(self, dim):
+        super().__init__()
+        self.conv = nn.ConvTranspose1d(dim, dim, 4, 2, 1)
+
+    def forward(self, x):
+        return self.conv(x)
+
+class Conv1dBlock(nn.Module):
+    '''
+        Conv1d --> GroupNorm --> Mish
+    '''
+
+    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):
+        super().__init__()
+
+        self.block = nn.Sequential(
+            nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2),
+            nn.GroupNorm(n_groups, out_channels),
+            nn.Mish(),
+        )
+
+    def forward(self, x):
+        return self.block(x)
+
+
+class ConditionalResidualBlock1D(nn.Module):
+    def __init__(self,
+            in_channels,
+            out_channels,
+            cond_dim,
+            kernel_size=3,
+            n_groups=8):
+        super().__init__()
+
+        self.blocks = nn.ModuleList([
+            Conv1dBlock(in_channels, out_channels, kernel_size, n_groups=n_groups),
+            Conv1dBlock(out_channels, out_channels, kernel_size, n_groups=n_groups),
+        ])
+
+        # FiLM modulation https://arxiv.org/abs/1709.07871
+        # predicts per-channel scale and bias
+        cond_channels = out_channels * 2
+        self.out_channels = out_channels
+        self.cond_encoder = nn.Sequential(
+            nn.Mish(),
+            nn.Linear(cond_dim, cond_channels),
+            nn.Unflatten(-1, (-1, 1))
+        )
+
+        # make sure dimensions compatible
+        self.residual_conv = nn.Conv1d(in_channels, out_channels, 1) \
+            if in_channels != out_channels else nn.Identity()
+
+    def forward(self, x, cond):
+        '''
+            x : [ batch_size x in_channels x horizon ]
+            cond : [ batch_size x cond_dim]
+
+            returns:
+            out : [ batch_size x out_channels x horizon ]
+        '''
+        out = self.blocks[0](x)
+        embed = self.cond_encoder(cond)
+
+        embed = embed.reshape(
+            embed.shape[0], 2, self.out_channels, 1)
+        scale = embed[:,0,...]
+        bias = embed[:,1,...]
+        out = scale * out + bias
+
+        out = self.blocks[1](out)
+        out = out + self.residual_conv(x)
+        return out
+
+
+class ConditionalUnet1D(nn.Module):
+    def __init__(self,
+        input_dim,
+        global_cond_dim,
+        diffusion_step_embed_dim=256,
+        down_dims=[256,512,1024],
+        kernel_size=5,
+        n_groups=8
+        ):
+        """
+        input_dim: Dim of actions.
+        global_cond_dim: Dim of global conditioning applied with FiLM
+          in addition to diffusion step embedding. This is usually obs_horizon * obs_dim
+        diffusion_step_embed_dim: Size of positional encoding for diffusion iteration k
+        down_dims: Channel size for each UNet level.
+          The length of this array determines numebr of levels.
+        kernel_size: Conv kernel size
+        n_groups: Number of groups for GroupNorm
+        """
+
+        super().__init__()
+        all_dims = [input_dim] + list(down_dims)
+        start_dim = down_dims[0]
+
+        dsed = diffusion_step_embed_dim
+        diffusion_step_encoder = nn.Sequential(
+            SinusoidalPosEmb(dsed),
+            nn.Linear(dsed, dsed * 4),
+            nn.Mish(),
+            nn.Linear(dsed * 4, dsed),
+        )
+        cond_dim = dsed + global_cond_dim
+
+        in_out = list(zip(all_dims[:-1], all_dims[1:]))
+        mid_dim = all_dims[-1]
+        self.mid_modules = nn.ModuleList([
+            ConditionalResidualBlock1D(
+                mid_dim, mid_dim, cond_dim=cond_dim,
+                kernel_size=kernel_size, n_groups=n_groups
+            ),
+            ConditionalResidualBlock1D(
+                mid_dim, mid_dim, cond_dim=cond_dim,
+                kernel_size=kernel_size, n_groups=n_groups
+            ),
+        ])
+
+        down_modules = nn.ModuleList([])
+        for ind, (dim_in, dim_out) in enumerate(in_out):
+            is_last = ind >= (len(in_out) - 1)
+            down_modules.append(nn.ModuleList([
+                ConditionalResidualBlock1D(
+                    dim_in, dim_out, cond_dim=cond_dim,
+                    kernel_size=kernel_size, n_groups=n_groups),
+                ConditionalResidualBlock1D(
+                    dim_out, dim_out, cond_dim=cond_dim,
+                    kernel_size=kernel_size, n_groups=n_groups),
+                Downsample1d(dim_out) if not is_last else nn.Identity()
+            ]))
+
+        up_modules = nn.ModuleList([])
+        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):
+            is_last = ind >= (len(in_out) - 1)
+            up_modules.append(nn.ModuleList([
+                ConditionalResidualBlock1D(
+                    dim_out*2, dim_in, cond_dim=cond_dim,
+                    kernel_size=kernel_size, n_groups=n_groups),
+                ConditionalResidualBlock1D(
+                    dim_in, dim_in, cond_dim=cond_dim,
+                    kernel_size=kernel_size, n_groups=n_groups),
+                Upsample1d(dim_in) if not is_last else nn.Identity()
+            ]))
+
+        final_conv = nn.Sequential(
+            Conv1dBlock(start_dim, start_dim, kernel_size=kernel_size),
+            nn.Conv1d(start_dim, input_dim, 1),
+        )
+
+        self.diffusion_step_encoder = diffusion_step_encoder
+        self.up_modules = up_modules
+        self.down_modules = down_modules
+        self.final_conv = final_conv
+
+        n_params = sum(p.numel() for p in self.parameters())
+        print(f"number of parameters: {n_params / 1e6:.2f}M")
+
+    def forward(self,
+            sample: torch.Tensor,
+            timestep: Union[torch.Tensor, float, int],
+            global_cond=None):
+        """
+        x: (B,T,input_dim)
+        timestep: (B,) or int, diffusion step
+        global_cond: (B,global_cond_dim)
+        output: (B,T,input_dim)
+        """
+        # (B,T,C)
+        sample = sample.moveaxis(-1,-2)
+        # (B,C,T)
+
+        # 1. time
+        timesteps = timestep
+        if not torch.is_tensor(timesteps):
+            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can
+            timesteps = torch.tensor([timesteps], dtype=torch.long, device=sample.device)
+        elif torch.is_tensor(timesteps) and len(timesteps.shape) == 0:
+            timesteps = timesteps[None].to(sample.device)
+        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML
+        timesteps = timesteps.expand(sample.shape[0])
+
+        global_feature = self.diffusion_step_encoder(timesteps)
+
+        if global_cond is not None:
+            global_feature = torch.cat([
+                global_feature, global_cond
+            ], axis=-1)
+
+        x = sample
+        h = []
+        for idx, (resnet, resnet2, downsample) in enumerate(self.down_modules):
+            x = resnet(x, global_feature)
+            x = resnet2(x, global_feature)
+            h.append(x)
+            x = downsample(x)
+
+        for mid_module in self.mid_modules:
+            x = mid_module(x, global_feature)
+
+        for idx, (resnet, resnet2, upsample) in enumerate(self.up_modules):
+            x = torch.cat((x, h.pop()), dim=1)
+            x = resnet(x, global_feature)
+            x = resnet2(x, global_feature)
+            x = upsample(x)
+
+        x = self.final_conv(x)
+
+        # (B,C,T)
+        x = x.moveaxis(-1,-2)
+        # (B,T,C)
+        return x
diff --git a/scripts/baselines/diffusion_policy/diffusion_policy/evaluate.py b/scripts/baselines/diffusion_policy/diffusion_policy/evaluate.py
new file mode 100644
index 0000000..3e91d42
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/diffusion_policy/evaluate.py
@@ -0,0 +1,71 @@
+from collections import defaultdict
+import gymnasium
+import numpy as np
+import torch
+
+def collect_episode_info(infos, result):
+    if "final_info" in infos: # infos is a dict
+
+        indices = np.where(infos["_final_info"])[0] # not all envs are done at the same time
+        for i in indices:
+            info = infos["final_info"][i] # info is also a dict
+            ep = info['episode']
+            result['return'].append(ep['r'][0])
+            result['episode_len'].append(ep["l"][0])
+            if "success" in info:
+                result['success'].append(info['success'])
+            if "fail" in info:
+                result['fail'].append(info['fail'])
+    return result
+
+def evaluate(n, agent, eval_envs, device):
+    agent.eval()
+    is_cpu_sim = isinstance(eval_envs, gymnasium.vector.AsyncVectorEnv)
+    with torch.no_grad():
+        result = defaultdict(list)
+        # reset with reconfigure=True to ensure sufficient randomness in object geometries otherwise they will be fixed in GPU sim.
+        obs, info = eval_envs.reset(options=dict(reconfigure=True))
+        eps_rets = np.zeros(eval_envs.num_envs)
+        eps_lens = np.zeros(eval_envs.num_envs)
+        eps_success_once = np.zeros(eval_envs.num_envs)
+        eps_count = 0
+        while eps_count < n:
+            if is_cpu_sim:
+                action_seq = agent.get_eval_action(torch.Tensor(obs).to(device)).cpu().numpy()
+                for i in range(action_seq.shape[1]):
+                    eps_lens += 1
+                    obs, rew, terminated, truncated, info = eval_envs.step(action_seq[:, i])
+                    eps_rets += rew
+                    if truncated.any():
+                        break
+                    eps_success_once += info["success"]
+            else:
+                action_seq = agent.get_eval_action(obs)
+                for i in range(action_seq.shape[1]):
+                    eps_lens += 1
+                    obs, rew, terminated, truncated, info = eval_envs.step(action_seq[:, i])
+                    eps_rets += rew.cpu().numpy()
+                    if truncated.any():
+                        break
+                    eps_success_once += info["success"].cpu().numpy()
+
+            if truncated.any():
+                assert truncated.all() == truncated.any(), "all episodes should truncate at the same time for fair evaluation with other algorithms"
+                if is_cpu_sim:
+                    eps_success = []
+                    for i in range(eval_envs.num_envs):
+                        eps_success.append(info["final_info"][i]['success'])
+                else:
+                    eps_success = info["final_info"]['success'].cpu().numpy()
+                eps_success_once += eps_success
+                result['success_once'].append(eps_success_once > 0)
+                result['success_at_end'].append(eps_success)
+                result['episode_len'].append(eps_lens)
+                result['return'].append(eps_rets)
+                eps_count += eval_envs.num_envs
+                eps_rets = np.zeros(eval_envs.num_envs)
+                eps_lens = np.zeros(eval_envs.num_envs)
+                eps_success_once = np.zeros(eval_envs.num_envs)
+    agent.train()
+    result = {k: np.concatenate(v) for k, v in result.items()}
+    return result
diff --git a/scripts/baselines/diffusion_policy/diffusion_policy/make_env.py b/scripts/baselines/diffusion_policy/diffusion_policy/make_env.py
new file mode 100644
index 0000000..537f3ac
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/diffusion_policy/make_env.py
@@ -0,0 +1,34 @@
+from typing import Optional
+import gymnasium as gym
+import mani_skill.envs
+from mani_skill.utils import gym_utils
+from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
+from mani_skill.utils.wrappers import RecordEpisode, FrameStack, CPUGymWrapper
+from diffusion_policy.wrappers import ContinuousTaskWrapper, SeqActionWrapper
+
+
+def make_env(env_id, num_envs: int, sim_backend: str, seed: int, env_kwargs: dict, other_kwargs: dict,video_dir: Optional[str] = None):
+    if sim_backend == "cpu":
+        def cpu_make_env(env_id, seed, video_dir=None, env_kwargs = dict(), other_kwargs = dict()):
+            def thunk():
+                env = gym.make(env_id, **env_kwargs)
+                env = CPUGymWrapper(env)
+                if video_dir:
+                    env = RecordEpisode(env, output_dir=video_dir, save_trajectory=False, info_on_video=True, source_type="diffusion_policy", source_desc="diffusion_policy evaluation rollout")
+                env = gym.wrappers.FrameStack(env, other_kwargs['obs_horizon'])
+                env = ContinuousTaskWrapper(env)
+                env.action_space.seed(seed)
+                env.observation_space.seed(seed)
+                return env
+
+            return thunk
+        vector_cls = gym.vector.SyncVectorEnv if num_envs == 1 else lambda x : gym.vector.AsyncVectorEnv(x, context="forkserver")
+        env = vector_cls([cpu_make_env(env_id, seed, video_dir if seed == 0 else None, env_kwargs, other_kwargs) for seed in range(num_envs)])
+    else:
+        env = gym.make(env_id, num_envs=num_envs, sim_backend=sim_backend, **env_kwargs)
+        max_episode_steps = gym_utils.find_max_episode_steps_value(env)
+        env = FrameStack(env, num_stack=other_kwargs['obs_horizon'])
+        if video_dir:
+            env = RecordEpisode(env, output_dir=video_dir, save_trajectory=False, save_video=True, source_type="diffusion_policy", source_desc="diffusion_policy evaluation rollout", max_steps_per_video=max_episode_steps)
+        env = ManiSkillVectorEnv(env, ignore_terminations=True)
+    return env
diff --git a/scripts/baselines/diffusion_policy/diffusion_policy/utils.py b/scripts/baselines/diffusion_policy/diffusion_policy/utils.py
new file mode 100644
index 0000000..af3876f
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/diffusion_policy/utils.py
@@ -0,0 +1,119 @@
+from torch.utils.data.sampler import Sampler
+import numpy as np
+import torch
+from h5py import File, Group, Dataset
+
+class IterationBasedBatchSampler(Sampler):
+    """Wraps a BatchSampler.
+    Resampling from it until a specified number of iterations have been sampled
+    References:
+        https://github.com/facebookresearch/maskrcnn-benchmark/blob/master/maskrcnn_benchmark/data/samplers/iteration_based_batch_sampler.py
+    """
+
+    def __init__(self, batch_sampler, num_iterations, start_iter=0):
+        self.batch_sampler = batch_sampler
+        self.num_iterations = num_iterations
+        self.start_iter = start_iter
+
+    def __iter__(self):
+        iteration = self.start_iter
+        while iteration < self.num_iterations:
+            # if the underlying sampler has a set_epoch method, like
+            # DistributedSampler, used for making each process see
+            # a different split of the dataset, then set it
+            if hasattr(self.batch_sampler.sampler, "set_epoch"):
+                self.batch_sampler.sampler.set_epoch(iteration)
+            for batch in self.batch_sampler:
+                yield batch
+                iteration += 1
+                if iteration >= self.num_iterations:
+                    break
+
+    def __len__(self):
+        return self.num_iterations - self.start_iter
+
+
+def worker_init_fn(worker_id, base_seed=None):
+    """The function is designed for pytorch multi-process dataloader.
+    Note that we use the pytorch random generator to generate a base_seed.
+    Please try to be consistent.
+    References:
+        https://pytorch.org/docs/stable/notes/faq.html#dataloader-workers-random-seed
+    """
+    if base_seed is None:
+        base_seed = torch.IntTensor(1).random_().item()
+    # print(worker_id, base_seed)
+    np.random.seed(base_seed + worker_id)
+
+TARGET_KEY_TO_SOURCE_KEY = {
+    'states': 'env_states',
+    'observations': 'obs',
+    'success': 'success',
+    'next_observations': 'obs',
+    # 'dones': 'dones',
+    # 'rewards': 'rewards',
+    'actions': 'actions',
+}
+def load_content_from_h5_file(file):
+    if isinstance(file, (File, Group)):
+        return {key: load_content_from_h5_file(file[key]) for key in list(file.keys())}
+    elif isinstance(file, Dataset):
+        return file[()]
+    else:
+        raise NotImplementedError(f"Unspported h5 file type: {type(file)}")
+
+def load_hdf5(path, ):
+    print('Loading HDF5 file', path)
+    file = File(path, 'r')
+    ret = load_content_from_h5_file(file)
+    file.close()
+    print('Loaded')
+    return ret
+
+def load_traj_hdf5(path, num_traj=None):
+    print('Loading HDF5 file', path)
+    file = File(path, 'r')
+    keys = list(file.keys())
+    if num_traj is not None:
+        assert num_traj <= len(keys), f"num_traj: {num_traj} > len(keys): {len(keys)}"
+        keys = sorted(keys, key=lambda x: int(x.split('_')[-1]))
+        keys = keys[:num_traj]
+    ret = {
+        key: load_content_from_h5_file(file[key]) for key in keys
+    }
+    file.close()
+    print('Loaded')
+    return ret
+def load_demo_dataset(path, keys=['observations', 'actions'], num_traj=None, concat=True):
+    # assert num_traj is None
+    raw_data = load_traj_hdf5(path, num_traj)
+    # raw_data has keys like: ['traj_0', 'traj_1', ...]
+    # raw_data['traj_0'] has keys like: ['actions', 'dones', 'env_states', 'infos', ...]
+    _traj = raw_data['traj_0']
+    for key in keys:
+        source_key = TARGET_KEY_TO_SOURCE_KEY[key]
+        assert source_key in _traj, f"key: {source_key} not in traj_0: {_traj.keys()}"
+    dataset = {}
+    for target_key in keys:
+        # if 'next' in target_key:
+        #     raise NotImplementedError('Please carefully deal with the length of trajectory')
+        source_key = TARGET_KEY_TO_SOURCE_KEY[target_key]
+        dataset[target_key] = [ raw_data[idx][source_key] for idx in raw_data ]
+        if isinstance(dataset[target_key][0], np.ndarray) and concat:
+            if target_key in ['observations', 'states'] and \
+                    len(dataset[target_key][0]) > len(raw_data['traj_0']['actions']):
+                dataset[target_key] = np.concatenate([
+                    t[:-1] for t in dataset[target_key]
+                ], axis=0)
+            elif target_key in ['next_observations', 'next_states'] and \
+                    len(dataset[target_key][0]) > len(raw_data['traj_0']['actions']):
+                dataset[target_key] = np.concatenate([
+                    t[1:] for t in dataset[target_key]
+                ], axis=0)
+            else:
+                dataset[target_key] = np.concatenate(dataset[target_key], axis=0)
+
+            print('Load', target_key, dataset[target_key].shape)
+        else:
+            print('Load', target_key, len(dataset[target_key]), type(dataset[target_key][0]))
+    return dataset
diff --git a/scripts/baselines/diffusion_policy/diffusion_policy/wrappers.py b/scripts/baselines/diffusion_policy/diffusion_policy/wrappers.py
new file mode 100644
index 0000000..96cfdfd
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/diffusion_policy/wrappers.py
@@ -0,0 +1,14 @@
+import gymnasium as gym
+class SeqActionWrapper(gym.Wrapper):
+    def step(self, action_seq):
+        rew_sum = 0
+        for action in action_seq:
+            obs, rew, terminated, truncated, info = self.env.step(action)
+            rew_sum += rew
+            if terminated or truncated:
+                break
+        return obs, rew_sum, terminated, truncated, info
+class ContinuousTaskWrapper(gym.Wrapper):
+    def step(self, action):
+        obs, rew, terminated, truncated, info = self.env.step(action)
+        return obs, rew, False, truncated, info
diff --git a/scripts/baselines/diffusion_policy/setup.py b/scripts/baselines/diffusion_policy/setup.py
new file mode 100644
index 0000000..65baff6
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/setup.py
@@ -0,0 +1,16 @@
+from setuptools import setup, find_packages
+
+setup(
+    name="diffusion_policy",
+    version="0.1.0",
+    packages=find_packages(),
+    install_requires=[
+        "diffusers",
+        "tensorboard",
+        "wandb",
+        "mani_skill"
+    ],
+    description="A minimal setup for Diffusion Policy for ManiSkill",
+    long_description=open("README.md").read(),
+    long_description_content_type="text/markdown",
+)
diff --git a/scripts/baselines/diffusion_policy/train.py b/scripts/baselines/diffusion_policy/train.py
new file mode 100644
index 0000000..7cd8075
--- /dev/null
+++ b/scripts/baselines/diffusion_policy/train.py
@@ -0,0 +1,427 @@
+ALGO_NAME = 'BC_Diffusion_state_UNet'
+
+import argparse
+import os
+import random
+from distutils.util import strtobool
+import time
+import gymnasium as gym
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import torch.nn.functional as F
+from torch.utils.tensorboard import SummaryWriter
+from diffusion_policy.evaluate import evaluate
+from mani_skill.utils.registration import REGISTERED_ENVS
+
+from collections import defaultdict
+
+from torch.utils.data.dataset import Dataset
+from torch.utils.data.sampler import RandomSampler, BatchSampler
+from torch.utils.data.dataloader import DataLoader
+from diffusion_policy.utils import IterationBasedBatchSampler, worker_init_fn
+from diffusion_policy.make_env import make_env
+from diffusers.schedulers.scheduling_ddpm import DDPMScheduler
+from diffusers.training_utils import EMAModel
+from diffusers.optimization import get_scheduler
+from diffusion_policy.conditional_unet1d import ConditionalUnet1D
+from dataclasses import dataclass, field
+from typing import Optional, List
+import tyro
+@dataclass
+class Args:
+    exp_name: Optional[str] = None
+    """the name of this experiment"""
+    seed: int = 1
+    """seed of the experiment"""
+    torch_deterministic: bool = True
+    """if toggled, `torch.backends.cudnn.deterministic=False`"""
+    cuda: bool = True
+    """if toggled, cuda will be enabled by default"""
+    track: bool = False
+    """if toggled, this experiment will be tracked with Weights and Biases"""
+    wandb_project_name: str = "ManiSkill"
+    """the wandb's project name"""
+    wandb_entity: Optional[str] = None
+    """the entity (team) of wandb's project"""
+    capture_video: bool = True
+    """whether to capture videos of the agent performances (check out `videos` folder)"""
+
+    env_id: str = "PegInsertionSide-v0"
+    """the id of the environment"""
+    demo_path: str = 'data/ms2_official_demos/rigid_body/PegInsertionSide-v0/trajectory.state.pd_ee_delta_pose.h5'
+    """the path of demo dataset (pkl or h5)"""
+    num_demos: Optional[int] = None
+    """number of trajectories to load from the demo dataset"""
+    total_iters: int = 1_000_000
+    """total timesteps of the experiments"""
+    batch_size: int = 1024
+    """the batch size of sample from the replay memory"""
+
+    # Diffusion Policy specific arguments
+    lr: float = 1e-4
+    obs_horizon: int = 2 # Seems not very important in ManiSkill, 1, 2, 4 work well
+    act_horizon: int = 8 # Seems not very important in ManiSkill, 4, 8, 15 work well
+    pred_horizon: int = 16 # 16->8 leads to worse performance, maybe it is like generate a half image; 16->32, improvement is very marginal
+    diffusion_step_embed_dim: int = 64 # not very important
+    unet_dims: List[int] = field(default_factory=lambda: [64, 128, 256]) # default setting is about ~4.5M params
+    n_groups: int = 8 # jigu says it is better to let each group have at least 8 channels; it seems 4 and 8 are simila
+    max_episode_steps: Optional[int] = None
+    """Change the environments' max_episode_steps to this value. Sometimes necessary if the demonstrations being imitated are too short. Typically the default
+    max episode steps of environments in ManiSkill are tuned lower so reinforcement learning agents can learn faster."""
+
+    # Environment/experiment specific arguments
+    output_dir: str = 'output'
+    log_freq: int = 1000
+    eval_freq: int = 5000
+    save_freq: Optional[int] = None
+    num_eval_episodes: int = 100
+    num_eval_envs: int = 10
+    sim_backend: str = "cpu"
+    num_dataload_workers: int = 0
+    control_mode: str = 'pd_joint_delta_pos'
+    obj_ids: List[str] = field(default_factory=list)
+
+    # additional tags/configs for logging purposes to wandb and shared comparisons with other algorithms
+    demo_type: Optional[str] = None
+
+
+class SmallDemoDataset_DiffusionPolicy(Dataset): # Load everything into GPU memory
+    def __init__(self, data_path, device, num_traj):
+        if data_path[-4:] == '.pkl':
+            raise NotImplementedError()
+        else:
+            from diffusion_policy.utils import load_demo_dataset
+            trajectories = load_demo_dataset(data_path, num_traj=num_traj, concat=False)
+            # trajectories['observations'] is a list of np.ndarray (L+1, obs_dim)
+            # trajectories['actions'] is a list of np.ndarray (L, act_dim)
+
+        for k, v in trajectories.items():
+            for i in range(len(v)):
+                trajectories[k][i] = torch.Tensor(v[i]).to(device)
+
+        # Pre-compute all possible (traj_idx, start, end) tuples, this is very specific to Diffusion Policy
+        if 'delta_pos' in args.control_mode or args.control_mode == 'base_pd_joint_vel_arm_pd_joint_vel':
+            self.pad_action_arm = torch.zeros((trajectories['actions'][0].shape[1]-1,), device=device)
+            # to make the arm stay still, we pad the action with 0 in 'delta_pos' control mode
+            # gripper action needs to be copied from the last action
+        # else:
+        #     raise NotImplementedError(f'Control Mode {args.control_mode} not supported')
+        self.obs_horizon, self.pred_horizon = obs_horizon, pred_horizon = args.obs_horizon, args.pred_horizon
+        self.slices = []
+        num_traj = len(trajectories['actions'])
+        total_transitions = 0
+        for traj_idx in range(num_traj):
+            L = trajectories['actions'][traj_idx].shape[0]
+            assert trajectories['observations'][traj_idx].shape[0] == L + 1
+            total_transitions += L
+
+            # |o|o|                             observations: 2
+            # | |a|a|a|a|a|a|a|a|               actions executed: 8
+            # |p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16
+            pad_before = obs_horizon - 1
+            # Pad before the trajectory, so the first action of an episode is in "actions executed"
+            # obs_horizon - 1 is the number of "not used actions"
+            pad_after = pred_horizon - obs_horizon
+            # Pad after the trajectory, so all the observations are utilized in training
+            # Note that in the original code, pad_after = act_horizon - 1, but I think this is not the best choice
+            self.slices += [
+                (traj_idx, start, start + pred_horizon) for start in range(-pad_before, L - pred_horizon + pad_after)
+            ]  # slice indices follow convention [start, end)
+
+        print(f"Total transitions: {total_transitions}, Total obs sequences: {len(self.slices)}")
+
+        self.trajectories = trajectories
+
+    def __getitem__(self, index):
+        traj_idx, start, end = self.slices[index]
+        L, act_dim = self.trajectories['actions'][traj_idx].shape
+
+        obs_seq = self.trajectories['observations'][traj_idx][max(0, start):start+self.obs_horizon]
+        # start+self.obs_horizon is at least 1
+        act_seq = self.trajectories['actions'][traj_idx][max(0, start):end]
+        if start < 0: # pad before the trajectory
+            obs_seq = torch.cat([obs_seq[0].repeat(-start, 1), obs_seq], dim=0)
+            act_seq = torch.cat([act_seq[0].repeat(-start, 1), act_seq], dim=0)
+        if end > L: # pad after the trajectory
+            gripper_action = act_seq[-1, -1]
+            pad_action = torch.cat((self.pad_action_arm, gripper_action[None]), dim=0)
+            act_seq = torch.cat([act_seq, pad_action.repeat(end-L, 1)], dim=0)
+            # making the robot (arm and gripper) stay still
+        assert obs_seq.shape[0] == self.obs_horizon and act_seq.shape[0] == self.pred_horizon
+        return {
+            'observations': obs_seq,
+            'actions': act_seq,
+        }
+
+    def __len__(self):
+        return len(self.slices)
+
+
+class Agent(nn.Module):
+    def __init__(self, env, args):
+        super().__init__()
+        self.obs_horizon = args.obs_horizon
+        self.act_horizon = args.act_horizon
+        self.pred_horizon = args.pred_horizon
+        assert len(env.single_observation_space.shape) == 2 # (obs_horizon, obs_dim)
+        assert len(env.single_action_space.shape) == 1 # (act_dim, )
+        assert (env.single_action_space.high == 1).all() and (env.single_action_space.low == -1).all()
+        # denoising results will be clipped to [-1,1], so the action should be in [-1,1] as well
+        self.act_dim = env.single_action_space.shape[0]
+
+        self.noise_pred_net = ConditionalUnet1D(
+            input_dim=self.act_dim, # act_horizon is not used (U-Net doesn't care)
+            global_cond_dim=np.prod(env.single_observation_space.shape), # obs_horizon * obs_dim
+            diffusion_step_embed_dim=args.diffusion_step_embed_dim,
+            down_dims=args.unet_dims,
+            n_groups=args.n_groups,
+        )
+        self.num_diffusion_iters = 100
+        self.noise_scheduler = DDPMScheduler(
+            num_train_timesteps=self.num_diffusion_iters,
+            beta_schedule='squaredcos_cap_v2', # has big impact on performance, try not to change
+            clip_sample=True, # clip output to [-1,1] to improve stability
+            prediction_type='epsilon' # predict noise (instead of denoised action)
+        )
+
+        self.get_eval_action = self.get_action
+
+    # def forward(self, obs_seq):
+    #     raise NotImplementedError()
+
+    def compute_loss(self, obs_seq, action_seq):
+        B = obs_seq.shape[0]
+
+        # observation as FiLM conditioning
+        obs_cond = obs_seq.flatten(start_dim=1) # (B, obs_horizon * obs_dim)
+
+        # sample noise to add to actions
+        noise = torch.randn((B, self.pred_horizon, self.act_dim), device=device)
+
+        # sample a diffusion iteration for each data point
+        timesteps = torch.randint(
+            0, self.noise_scheduler.config.num_train_timesteps,
+            (B,), device=device
+        ).long()
+
+        # add noise to the clean images(actions) according to the noise magnitude at each diffusion iteration
+        # (this is the forward diffusion process)
+        noisy_action_seq = self.noise_scheduler.add_noise(
+            action_seq, noise, timesteps)
+
+        # predict the noise residual
+        noise_pred = self.noise_pred_net(
+            noisy_action_seq, timesteps, global_cond=obs_cond)
+
+        return F.mse_loss(noise_pred, noise)
+
+    def get_action(self, obs_seq):
+        # init scheduler
+        # self.noise_scheduler.set_timesteps(self.num_diffusion_iters)
+        # set_timesteps will change noise_scheduler.timesteps is only used in noise_scheduler.step()
+        # noise_scheduler.step() is only called during inference
+        # if we use DDPM, and inference_diffusion_steps == train_diffusion_steps, then we can skip this
+
+        # obs_seq: (B, obs_horizon, obs_dim)
+        B = obs_seq.shape[0]
+        with torch.no_grad():
+            obs_cond = obs_seq.flatten(start_dim=1) # (B, obs_horizon * obs_dim)
+
+            # initialize action from Guassian noise
+            noisy_action_seq = torch.randn((B, self.pred_horizon, self.act_dim), device=obs_seq.device)
+
+            for k in self.noise_scheduler.timesteps:
+                # predict noise
+                noise_pred = self.noise_pred_net(
+                    sample=noisy_action_seq,
+                    timestep=k,
+                    global_cond=obs_cond,
+                )
+
+                # inverse diffusion step (remove noise)
+                noisy_action_seq = self.noise_scheduler.step(
+                    model_output=noise_pred,
+                    timestep=k,
+                    sample=noisy_action_seq,
+                ).prev_sample
+
+        # only take act_horizon number of actions
+        start = self.obs_horizon - 1
+        end = start + self.act_horizon
+        return noisy_action_seq[:, start:end] # (B, act_horizon, act_dim)
+
+def save_ckpt(run_name, tag):
+    os.makedirs(f'runs/{run_name}/checkpoints', exist_ok=True)
+    ema.copy_to(ema_agent.parameters())
+    torch.save({
+        'agent': agent.state_dict(),
+        'ema_agent': ema_agent.state_dict(),
+    }, f'runs/{run_name}/checkpoints/{tag}.pt')
+
+if __name__ == "__main__":
+    args = tyro.cli(Args)
+    if args.exp_name is None:
+        args.exp_name = os.path.basename(__file__)[: -len(".py")]
+        run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    else:
+        run_name = args.exp_name
+
+    if args.demo_path.endswith('.h5'):
+        import json
+        json_file = args.demo_path[:-2] + 'json'
+        with open(json_file, 'r') as f:
+            demo_info = json.load(f)
+            if 'control_mode' in demo_info['env_info']['env_kwargs']:
+                control_mode = demo_info['env_info']['env_kwargs']['control_mode']
+            elif 'control_mode' in demo_info['episodes'][0]:
+                control_mode = demo_info['episodes'][0]['control_mode']
+            else:
+                raise Exception('Control mode not found in json')
+            assert control_mode == args.control_mode, f"Control mode mismatched. Dataset has control mode {control_mode}, but args has control mode {args.control_mode}"
+    assert args.obs_horizon + args.act_horizon - 1 <= args.pred_horizon
+    assert args.obs_horizon >= 1 and args.act_horizon >= 1 and args.pred_horizon >= 1
+
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    # env setup
+    max_episode_steps = args.max_episode_steps if args.max_episode_steps is not None else REGISTERED_ENVS[args.env_id].max_episode_steps
+    # note we set reconfiguration_freq to 1 to ensure that the env is reconfigured after each episode to sufficiently randomize object geometries
+    env_kwargs = dict(control_mode=args.control_mode, reward_mode="sparse", obs_mode="state", render_mode="rgb_array", max_episode_steps=max_episode_steps, reconfiguration_freq=1)
+    other_kwargs = dict(obs_horizon=args.obs_horizon)
+    eval_envs = make_env(args.env_id, args.num_eval_envs, args.sim_backend, args.seed, env_kwargs, other_kwargs, video_dir=f'runs/{run_name}/videos' if args.capture_video else None)
+    eval_envs.reset(seed=args.seed) # seed eval_envs here
+    envs = eval_envs
+    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
+
+    # dataloader setup
+    dataset = SmallDemoDataset_DiffusionPolicy(args.demo_path, device, num_traj=args.num_demos)
+    sampler = RandomSampler(dataset, replacement=False)
+    batch_sampler = BatchSampler(sampler, batch_size=args.batch_size, drop_last=True)
+    batch_sampler = IterationBasedBatchSampler(batch_sampler, args.total_iters)
+    train_dataloader = DataLoader(
+        dataset,
+        batch_sampler=batch_sampler,
+        num_workers=args.num_dataload_workers,
+        worker_init_fn=lambda worker_id: worker_init_fn(worker_id, base_seed=args.seed),
+    )
+    if args.num_demos is None:
+        args.num_demos = len(dataset)
+
+    # agent setup
+    agent = Agent(envs, args).to(device)
+    optimizer = optim.AdamW(params=agent.parameters(),
+        lr=args.lr, betas=(0.95, 0.999), weight_decay=1e-6)
+
+    # Cosine LR schedule with linear warmup
+    lr_scheduler = get_scheduler(
+        name='cosine',
+        optimizer=optimizer,
+        num_warmup_steps=500,
+        num_training_steps=args.total_iters,
+    )
+
+    # Exponential Moving Average
+    # accelerates training and improves stability
+    # holds a copy of the model weights
+    ema = EMAModel(parameters=agent.parameters(), power=0.75)
+    ema_agent = Agent(envs, args).to(device)
+
+
+
+    if args.track:
+        import wandb
+        config = vars(args)
+        config["eval_env_cfg"] = dict(**env_kwargs, num_envs=args.num_eval_envs, env_id=args.env_id, env_horizon=max_episode_steps)
+        wandb.init(
+            project=args.wandb_project_name,
+            entity=args.wandb_entity,
+            sync_tensorboard=True,
+            config=config,
+            name=run_name,
+            save_code=True,
+            group="DiffusionPolicy",
+            tags=["diffusion_policy"]
+        )
+    writer = SummaryWriter(f"runs/{run_name}")
+    writer.add_text(
+        "hyperparameters",
+        "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+    )
+    import json
+    with open(f'runs/{run_name}/args.json', 'w') as f:
+        json.dump(vars(args), f, indent=4)
+
+    # ---------------------------------------------------------------------------- #
+    # Training begins.
+    # ---------------------------------------------------------------------------- #
+    agent.train()
+    best_success_once_rate = -1
+    best_success_at_end_rate = -1
+
+    timings = defaultdict(float)
+
+    for iteration, data_batch in enumerate(train_dataloader):
+        cur_iter = iteration + 1
+
+        # # copy data from cpu to gpu
+        # data_batch = {k: v.cuda(non_blocking=True) for k, v in data_batch.items()}
+
+        # forward and compute loss
+        total_loss = agent.compute_loss(
+            obs_seq=data_batch['observations'], # (B, L, obs_dim)
+            action_seq=data_batch['actions'], # (B, L, act_dim)
+        )
+
+        # backward
+        optimizer.zero_grad()
+        total_loss.backward()
+        optimizer.step()
+        lr_scheduler.step() # step lr scheduler every batch, this is different from standard pytorch behavior
+        last_tick = time.time()
+
+        # update Exponential Moving Average of the model weights
+        ema.step(agent.parameters())
+        # TRY NOT TO MODIFY: record rewards for plotting purposes
+        if cur_iter % args.log_freq == 0:
+            print(f"Iteration {cur_iter}, loss: {total_loss.item()}")
+            writer.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], cur_iter)
+            writer.add_scalar("losses/total_loss", total_loss.item(), cur_iter)
+            for k, v in timings.items():
+                writer.add_scalar(f"time/{k}", v, cur_iter)
+        # Evaluation
+        if cur_iter % args.eval_freq == 0:
+            print('Evaluating')
+            last_tick = time.time()
+
+            ema.copy_to(ema_agent.parameters())
+            result = evaluate(args.num_eval_episodes, ema_agent, eval_envs, device)
+            timings["eval"] += time.time() - last_tick
+
+            for k, v in result.items():
+                writer.add_scalar(f"eval/{k}", np.mean(v), cur_iter)
+            sr_once = np.mean(result['success_once'])
+            sr_at_end = np.mean(result['success_at_end'])
+            print(f"Evaluated {len(result['return'])} episodes")
+            print(f"Success Once Rate: {sr_once:.4f}")
+            print(f"Success At End Rate: {sr_at_end:.4f}")
+            if sr_once > best_success_once_rate:
+                best_success_once_rate = sr_once
+                save_ckpt(run_name, 'best_eval_success_rate')
+                print(f'New best success rate: {sr_once:.4f}. Saving checkpoint.')
+            if sr_at_end > best_success_at_end_rate:
+                best_success_at_end_rate = sr_at_end
+                save_ckpt(run_name, 'best_eval_success_rate_at_end')
+                print(f'New best success rate at end: {sr_at_end:.4f}. Saving checkpoint.')
+        # Checkpoint
+        if args.save_freq and cur_iter % args.save_freq == 0:
+            save_ckpt(run_name, str(cur_iter))
+    envs.close()
+    writer.close()
diff --git a/scripts/baselines/ppo/.gitignore b/scripts/baselines/ppo/.gitignore
new file mode 100644
index 0000000..07c0730
--- /dev/null
+++ b/scripts/baselines/ppo/.gitignore
@@ -0,0 +1,4 @@
+/runs
+/videos
+/pretrained
+/wandb
\ No newline at end of file
diff --git a/scripts/baselines/ppo/README.md b/scripts/baselines/ppo/README.md
new file mode 100644
index 0000000..27c2de6
--- /dev/null
+++ b/scripts/baselines/ppo/README.md
@@ -0,0 +1,113 @@
+# Proximal Policy Optimization (PPO)
+
+Code for running the PPO RL algorithm is adapted from [CleanRL](https://github.com/vwxyzjn/cleanrl/). It is written to be single-file and easy to follow/read, and supports state-based RL and visual-based RL code.
+
+Note that ManiSkill is still in beta, so we have not finalized training scripts for every pre-built task (some of which are simply too hard to solve with RL anyway).
+
+Official baseline results can be run by using the scripts in the baselines.sh file. Results are organized and published to our [wandb page](https://wandb.ai/stonet2000/ManiSkill/groups/PPO/workspace?nw=0pe9ybwmza7)
+
+## State Based RL
+
+Below is a sample of various commands you can run to train a state-based policy to solve various tasks with PPO that are lightly tuned already. The fastest one is the PushCube-v1 task which can take less than a minute to train on the GPU and the PickCube-v1 task which can take 2-5 minutes on the GPU.
+
+The PPO baseline is not guaranteed to work for all tasks as some tasks do not have dense rewards yet or well tuned ones, or simply are too hard with standard PPO.
+
+
+```bash
+python ppo.py --env_id="PushCube-v1" \
+  --num_envs=2048 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=2_000_000 --eval_freq=10 --num-steps=20
+```
+
+To evaluate, you can run
+```bash
+python ppo.py --env_id="PushCube-v1" \
+   --evaluate --checkpoint=path/to/model.pt \
+   --num_eval_envs=1 --num-eval-steps=1000
+```
+
+Note that with `--evaluate`, trajectories are saved from a GPU simulation. In order to support replaying these trajectories correctly with the `maniskill.trajectory.replay_trajectory` tool for some task, the number of evaluation environments must be fixed to `1`. This is necessary in order to ensure reproducibility for tasks that have randomizations on geometry (e.g. PickSingleYCB). Other tasks without geometrical randomization like PushCube are fine and you can increase the number of evaluation environments. 
+
+The examples.sh file has a full list of tested commands for running state based PPO successfully on many tasks.
+
+The results of running the baseline scripts for state based PPO are here: https://wandb.ai/stonet2000/ManiSkill/groups/PPO/workspace?nw=0pe9ybwmza7.
+
+## Visual (RGB) Based RL
+
+Below is a sample of various commands for training a image-based policy with PPO that are lightly tuned. The fastest again is also PushCube-v1 which can take about 1-5 minutes and PickCube-v1 which takes 15-45 minutes. You will need to tune the `--num_envs` argument according to how much GPU memory you have as rendering visual observations uses a lot of memory. The settings below should all take less than 15GB of GPU memory. The examples.sh file has a full list of tested commands for running visual based PPO successfully on many tasks.
+
+
+```bash
+python ppo_rgb.py --env_id="PushCube-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=8 \
+  --total_timesteps=1_000_000 --eval_freq=10 --num-steps=20
+python ppo_rgb.py --env_id="PickCube-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=8 \
+  --total_timesteps=10_000_000
+python ppo_rgb.py --env_id="AnymalC-Reach-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=10_000_000 --num-steps=200 --num-eval-steps=200 \
+  --gamma=0.99 --gae_lambda=0.95
+```
+
+To evaluate a trained policy you can run
+
+```bash
+python ppo_rgb.py --env_id="PickCube-v1" \
+  --evaluate --checkpoint=path/to/model.pt \
+  --num_eval_envs=1 --num-eval-steps=1000
+```
+
+and it will save videos to the `path/to/test_videos`.
+
+The examples.sh file has a full list of tested commands for running RGB based PPO successfully on many tasks.
+
+The results of running the baseline scripts for RGB based PPO are here: https://wandb.ai/stonet2000/ManiSkill/groups/PPO/workspace?nw=69soa2dqa9h
+
+## Visual (RGB+Depth) Based RL
+
+WIP
+
+## Visual (Pointcloud) Based RL
+
+WIP
+
+## Replaying Evaluation Trajectories
+
+It might be useful to get some nicer looking videos. A simple way to do that is to first use the evaluation scripts provided above. It will then save a .h5 and .json file with a name equal to the date and time that you can then replay with different settings as so
+
+```bash
+python -m mani_skill.trajectory.replay_trajectory \
+  --traj-path=path/to/trajectory.h5 --use-env-states --shader="rt-fast" \
+  --save-video --allow-failure -o "none"
+```
+
+This will use environment states to replay trajectories, turn on the ray-tracer (There is also "rt" which is higher quality but slower), and save all videos including failed trajectories.
+
+## Some Notes
+
+- Evaluation with GPU simulation (especially with randomized objects) is a bit tricky. We recommend reading through [our docs](https://maniskill.readthedocs.io/en/latest/user_guide/reinforcement_learning/baselines.html#evaluation) on online RL evaluation in order to understand how to fairly evaluate policies with GPU simulation.
+- Many tasks support visual observations, however we have not carefully verified yet if the camera poses for the tasks are setup in a way that makes it possible to solve some tasks from visual observations.
+
+## Citation
+
+If you use this baseline please cite the following
+```
+@article{DBLP:journals/corr/SchulmanWDRK17,
+  author       = {John Schulman and
+                  Filip Wolski and
+                  Prafulla Dhariwal and
+                  Alec Radford and
+                  Oleg Klimov},
+  title        = {Proximal Policy Optimization Algorithms},
+  journal      = {CoRR},
+  volume       = {abs/1707.06347},
+  year         = {2017},
+  url          = {http://arxiv.org/abs/1707.06347},
+  eprinttype    = {arXiv},
+  eprint       = {1707.06347},
+  timestamp    = {Mon, 13 Aug 2018 16:47:34 +0200},
+  biburl       = {https://dblp.org/rec/journals/corr/SchulmanWDRK17.bib},
+  bibsource    = {dblp computer science bibliography, https://dblp.org}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/ppo/baselines.sh b/scripts/baselines/ppo/baselines.sh
new file mode 100644
index 0000000..88826d8
--- /dev/null
+++ b/scripts/baselines/ppo/baselines.sh
@@ -0,0 +1,99 @@
+# for fair and correct RL evaluation against most RL algorithms, we do not do partial environment / early resets (which speed up training)
+# moreover evaluation environments will reconfigure each reset in order to randomize the task completely as some
+# tasks have different objects which are not changed at during normal resets.
+# Furthermore, because of how these are evaluated, the hyperparameters here are tuned differently compared to with partial resets
+
+seeds=(9351 4796 1788)
+
+### State Based PPO Baselines ###
+for seed in ${seeds[@]}
+do
+  python ppo.py --env_id="PushCube-v1" --seed=${seed} \
+    --num_envs=1024 --update_epochs=8 --num_minibatches=32 --reward_scale=1 \
+    --total_timesteps=50_000_000 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PushCube-v1-state-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo.py --env_id="PickCube-v1" --seed=${seed} \
+    --num_envs=1024 --update_epochs=8 --num_minibatches=32 --reward_scale=1 \
+    --total_timesteps=50_000_000 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PickCube-v1-state-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo.py --env_id="PickSingleYCB-v1" --seed=${seed} \
+    --num_envs=1024 --update_epochs=8 --num_minibatches=32 --reward_scale=1 \
+    --total_timesteps=50_000_000 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PickSingleYCB-v1-state-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo.py --env_id="PushT-v1" --seed=${seed} \
+    --num_envs=1024 --update_epochs=8 --num_minibatches=32 --gamma=0.99 --reward_scale=0.1 \
+    --total_timesteps=50_000_000 --num-steps=100 --num_eval_steps=100 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PushT-v1-state-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo.py --env_id="AnymalC-Reach-v1" --seed=${seed} \
+    --num_envs=1024 --update_epochs=8 --num_minibatches=32 --gamma=0.99 --gae_lambda=0.95 --reward_scale=0.1 \
+    --total_timesteps=50_000_000 --num-steps=200 --num-eval-steps=200 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-AnymalC-Reach-v1-state-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+### RGB Based PPO Baselines ###
+for seed in ${seeds[@]}
+do
+  python ppo_rgb.py --env_id="PushCube-v1" --seed=${seed} \
+    --num_envs=256 --update_epochs=8 --num_minibatches=8 --reward_scale=1 \
+    --total_timesteps=50_000_000 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PushCube-v1-rgb-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo_rgb.py --env_id="PickCube-v1" --seed=${seed} \
+    --num_envs=256 --update_epochs=8 --num_minibatches=8 --reward_scale=1 \
+    --total_timesteps=50_000_000 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PickCube-v1-rgb-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo_rgb.py --env_id="AnymalC-Reach-v1" --seed=${seed} \
+    --num_envs=256 --update_epochs=8 --num_minibatches=32 --reward_scale=0.1 \
+    --total_timesteps=50_000_000 --num-steps=200 --num-eval-steps=200 \
+    --gamma=0.99 --gae_lambda=0.95 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-AnymalC-Reach-v1-rgb-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+done
+
+for seed in ${seeds[@]}
+do
+  python ppo_rgb.py --env_id="PushT-v1" --seed=${seed} \
+    --num_envs=256 --update_epochs=8 --num_minibatches=8 --reward_scale=0.1 \
+    --total_timesteps=50_000_000 --num-steps=100 --num_eval_steps=100 --gamma=0.99 \
+    --no_partial_reset --reconfiguration_freq=1 --num_eval_envs=16 \
+    --exp-name="ppo-PushT-v1-rgb-${seed}-walltime_efficient" \
+    --wandb_entity="stonet2000" --track
+  done
\ No newline at end of file
diff --git a/scripts/baselines/ppo/examples.sh b/scripts/baselines/ppo/examples.sh
new file mode 100644
index 0000000..fb3b46f
--- /dev/null
+++ b/scripts/baselines/ppo/examples.sh
@@ -0,0 +1,130 @@
+# This file is a giant collection of tested example commands for PPO
+# Note these are tuned for wall time speed. For official baseline results which run
+# more fair comparisons of RL algorithms see the baselines.sh file
+
+### State Based PPO ###
+python ppo.py --env_id="PickCube-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=10_000_000
+python ppo.py --env_id="StackCube-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=25_000_000
+python ppo.py --env_id="PushT-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=25_000_000 --num-steps=100 --num_eval_steps=100 --gamma=0.99
+python ppo.py --env_id="PickSingleYCB-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=25_000_000
+python ppo.py --env_id="PegInsertionSide-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=250_000_000 --num-steps=100 --num-eval-steps=100
+python ppo.py --env_id="TwoRobotStackCube-v1" \
+   --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=40_000_000 --num-steps=100 --num-eval-steps=100
+python ppo.py --env_id="TriFingerRotateCubeLevel0-v1" \
+   --num_envs=128 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=50_000_000 --num-steps=250 --num-eval-steps=250
+python ppo.py --env_id="TriFingerRotateCubeLevel1-v1" \
+   --num_envs=128 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=50_000_000 --num-steps=250 --num-eval-steps=250
+python ppo.py --env_id="TriFingerRotateCubeLevel2-v1" \
+   --num_envs=128 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=50_000_000 --num-steps=250 --num-eval-steps=250
+python ppo.py --env_id="TriFingerRotateCubeLevel3-v1" \
+   --num_envs=128 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=50_000_000 --num-steps=250 --num-eval-steps=250
+python ppo.py --env_id="TriFingerRotateCubeLevel4-v1" \
+   --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=500_000_000 --num-steps=250 --num-eval-steps=250
+python ppo.py --env_id="PokeCube-v1" --update_epochs=8 --num_minibatches=32 \
+  --num_envs=1024 --total_timesteps=3_000_000 --eval_freq=10 --num-steps=20
+python ppo.py --env_id="MS-CartpoleBalance-v1" \
+   --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=4_000_000 --num-steps=250 --num-eval-steps=1000 \
+   --gamma=0.99 --gae_lambda=0.95 \
+   --eval_freq=5
+
+python ppo.py --env_id="MS-CartpoleSwingUp-v1" \
+   --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+   --total_timesteps=10_000_000 --num-steps=250 --num-eval-steps=1000 \
+   --gamma=0.99 --gae_lambda=0.95 \
+   --eval_freq=5
+python ppo.py --env_id="MS-AntWalk-v1" --num_envs=2048 --eval_freq=10 \
+  --update_epochs=8 --num_minibatches=32 --total_timesteps=20_000_000 \
+  --num_eval_steps=1000 --num_steps=200 --gamma=0.97 --ent_coef=1e-3
+python ppo.py --env_id="MS-AntRun-v1" --num_envs=2048 --eval_freq=10 \
+  --update_epochs=8 --num_minibatches=32 --total_timesteps=20_000_000 \
+  --num_eval_steps=1000 --num_steps=200 --gamma=0.97 --ent_coef=1e-3
+python ppo.py --env_id="MS-HumanoidStand-v1" --num_envs=2048 --eval_freq=10 \
+  --update_epochs=8 --num_minibatches=32 --total_timesteps=40_000_000 \
+  --num_eval_steps=1000 --num_steps=200 --gamma=0.95
+python ppo.py --env_id="MS-HumanoidWalk-v1" --num_envs=2048 --eval_freq=10 \
+  --update_epochs=8 --num_minibatches=32 --total_timesteps=80_000_000 \
+  --num_eval_steps=1000 --num_steps=200 --gamma=0.97 --ent_coef=1e-3
+python ppo.py --env_id="MS-HumanoidRun-v1" --num_envs=2048 --eval_freq=10 \
+  --update_epochs=8 --num_minibatches=32 --total_timesteps=60_000_000 \
+  --num_eval_steps=1000 --num_steps=200 --gamma=0.97 --ent_coef=1e-3
+python ppo.py --env_id="UnitreeG1PlaceAppleInBowl-v1" \
+  --num_envs=512 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=50_000_000 --num-steps=100 --num-eval-steps=100
+python ppo.py --env_id="AnymalC-Reach-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=25_000_000 --num-steps=200 --num-eval-steps=200 \
+  --gamma=0.99 --gae_lambda=0.95
+python ppo.py --env_id="AnymalC-Spin-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=50_000_000 --num-steps=200 --num-eval-steps=200 \
+  --gamma=0.99 --gae_lambda=0.95
+python ppo.py --env_id="UnitreeGo2-Reach-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=50_000_000 --num-steps=200 --num-eval-steps=200 \
+  --gamma=0.99 --gae_lambda=0.95
+python ppo.py --env_id="UnitreeH1Stand-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=100_000_000 --num-steps=100 --num-eval-steps=1000 \
+  --gamma=0.99 --gae_lambda=0.95
+python ppo.py --env_id="UnitreeG1Stand-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=100_000_000 --num-steps=100 --num-eval-steps=1000 \
+  --gamma=0.99 --gae_lambda=0.95
+
+python ppo.py --env_id="OpenCabinetDrawer-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=10_000_000 --num-steps=100 --num-eval-steps=100   
+
+python ppo.py --env_id="RollBall-v1" \
+  --num_envs=1024 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=20_000_000 --num-steps=80 --num_eval_steps=80 --gamma=0.95
+
+### RGB Based PPO ###
+python ppo_rgb.py --env_id="PushCube-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=8 \
+  --total_timesteps=1_000_000 --eval_freq=10 --num-steps=20
+python ppo_rgb.py --env_id="PickCube-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=8 \
+  --total_timesteps=10_000_000
+python ppo_rgb.py --env_id="AnymalC-Reach-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=10_000_000 --num-steps=200 --num-eval-steps=200 \
+  --gamma=0.99 --gae_lambda=0.95
+python ppo_rgb.py --env_id="PickSingleYCB-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=8 \
+  --total_timesteps=10_000_000
+python ppo_rgb.py --env_id="PushT-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=8 \
+  --total_timesteps=25_000_000 --num-steps=100 --num_eval_steps=100 --gamma=0.99
+python ppo_rgb.py --env_id="MS-AntWalk-v1" \
+ --num_envs=256 --update_epochs=8 --num_minibatches=32 \
+ --total_timesteps=5_000_000 --eval_freq=15 --num_eval_steps=1000 \
+ --num_steps=200 --gamma=0.97 --no-include-state --render_mode="rgb_array" \
+ --ent_coef=1e-3
+python ppo_rgb.py --env_id="MS-AntRun-v1" \
+ --num_envs=256 --update_epochs=8 --num_minibatches=32 \
+ --total_timesteps=15_000_000 --eval_freq=15 --num_eval_steps=1000 \
+ --num_steps=200 --gamma=0.97 --no-include-state --render_mode="rgb_array" \
+ --ent_coef=1e-3
+python ppo_rgb.py --env_id="MS-HumanoidRun-v1" \
+  --num_envs=256 --update_epochs=8 --num_minibatches=32 \
+  --total_timesteps=80_000_000 --eval_freq=15 --num_eval_steps=1000 \
+  --num_steps=200 --gamma=0.98 --no-include-state --render_mode="rgb_array" \
+  --ent_coef=1e-3
diff --git a/scripts/baselines/ppo/ppo.py b/scripts/baselines/ppo/ppo.py
new file mode 100644
index 0000000..2fe1aff
--- /dev/null
+++ b/scripts/baselines/ppo/ppo.py
@@ -0,0 +1,489 @@
+import os
+import random
+import time
+from dataclasses import dataclass
+from typing import Optional
+
+import gymnasium as gym
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import tyro
+from torch.distributions.normal import Normal
+from torch.utils.tensorboard import SummaryWriter
+
+# ManiSkill specific imports
+import mani_skill.envs
+from mani_skill.utils import gym_utils
+from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper
+from mani_skill.utils.wrappers.record import RecordEpisode
+from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
+
+@dataclass
+class Args:
+    exp_name: Optional[str] = None
+    """the name of this experiment"""
+    seed: int = 1
+    """seed of the experiment"""
+    torch_deterministic: bool = True
+    """if toggled, `torch.backends.cudnn.deterministic=False`"""
+    cuda: bool = True
+    """if toggled, cuda will be enabled by default"""
+    track: bool = False
+    """if toggled, this experiment will be tracked with Weights and Biases"""
+    wandb_project_name: str = "ManiSkill"
+    """the wandb's project name"""
+    wandb_entity: str = None
+    """the entity (team) of wandb's project"""
+    capture_video: bool = True
+    """whether to capture videos of the agent performances (check out `videos` folder)"""
+    save_model: bool = True
+    """whether to save model into the `runs/{run_name}` folder"""
+    evaluate: bool = False
+    """if toggled, only runs evaluation with the given model checkpoint and saves the evaluation trajectories"""
+    checkpoint: str = None
+    """path to a pretrained checkpoint file to start evaluation/training from"""
+
+    # Algorithm specific arguments
+    env_id: str = "PickCube-v1"
+    """the id of the environment"""
+    total_timesteps: int = 10000000
+    """total timesteps of the experiments"""
+    learning_rate: float = 3e-4
+    """the learning rate of the optimizer"""
+    num_envs: int = 512
+    """the number of parallel environments"""
+    num_eval_envs: int = 8
+    """the number of parallel evaluation environments"""
+    partial_reset: bool = True
+    """whether to let parallel environments reset upon termination instead of truncation"""
+    num_steps: int = 50
+    """the number of steps to run in each environment per policy rollout"""
+    num_eval_steps: int = 50
+    """the number of steps to run in each evaluation environment during evaluation"""
+    reconfiguration_freq: Optional[int] = None
+    """for benchmarking purposes we want to reconfigure the eval environment each reset to ensure objects are randomized in some tasks"""
+    anneal_lr: bool = False
+    """Toggle learning rate annealing for policy and value networks"""
+    gamma: float = 0.8
+    """the discount factor gamma"""
+    gae_lambda: float = 0.9
+    """the lambda for the general advantage estimation"""
+    num_minibatches: int = 32
+    """the number of mini-batches"""
+    update_epochs: int = 4
+    """the K epochs to update the policy"""
+    norm_adv: bool = True
+    """Toggles advantages normalization"""
+    clip_coef: float = 0.2
+    """the surrogate clipping coefficient"""
+    clip_vloss: bool = False
+    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
+    ent_coef: float = 0.0
+    """coefficient of the entropy"""
+    vf_coef: float = 0.5
+    """coefficient of the value function"""
+    max_grad_norm: float = 0.5
+    """the maximum norm for the gradient clipping"""
+    target_kl: float = 0.1
+    """the target KL divergence threshold"""
+    reward_scale: float = 1.0
+    """Scale the reward by this factor"""
+    eval_freq: int = 25
+    """evaluation frequency in terms of iterations"""
+    save_train_video_freq: Optional[int] = None
+    """frequency to save training videos in terms of iterations"""
+    finite_horizon_gae: bool = True
+
+
+    # to be filled in runtime
+    batch_size: int = 0
+    """the batch size (computed in runtime)"""
+    minibatch_size: int = 0
+    """the mini-batch size (computed in runtime)"""
+    num_iterations: int = 0
+    """the number of iterations (computed in runtime)"""
+
+def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
+    torch.nn.init.orthogonal_(layer.weight, std)
+    torch.nn.init.constant_(layer.bias, bias_const)
+    return layer
+
+
+class Agent(nn.Module):
+    def __init__(self, envs):
+        super().__init__()
+        self.critic = nn.Sequential(
+            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 256)),
+            nn.Tanh(),
+            layer_init(nn.Linear(256, 256)),
+            nn.Tanh(),
+            layer_init(nn.Linear(256, 256)),
+            nn.Tanh(),
+            layer_init(nn.Linear(256, 1)),
+        )
+        self.actor_mean = nn.Sequential(
+            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 256)),
+            nn.Tanh(),
+            layer_init(nn.Linear(256, 256)),
+            nn.Tanh(),
+            layer_init(nn.Linear(256, 256)),
+            nn.Tanh(),
+            layer_init(nn.Linear(256, np.prod(envs.single_action_space.shape)), std=0.01*np.sqrt(2)),
+        )
+        self.actor_logstd = nn.Parameter(torch.ones(1, np.prod(envs.single_action_space.shape)) * -0.5)
+
+    def get_value(self, x):
+        return self.critic(x)
+    def get_action(self, x, deterministic=False):
+        action_mean = self.actor_mean(x)
+        if deterministic:
+            return action_mean
+        action_logstd = self.actor_logstd.expand_as(action_mean)
+        action_std = torch.exp(action_logstd)
+        probs = Normal(action_mean, action_std)
+        return probs.sample()
+    def get_action_and_value(self, x, action=None):
+        action_mean = self.actor_mean(x)
+        action_logstd = self.actor_logstd.expand_as(action_mean)
+        action_std = torch.exp(action_logstd)
+        probs = Normal(action_mean, action_std)
+        if action is None:
+            action = probs.sample()
+        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
+
+class Logger:
+    def __init__(self, log_wandb=False, tensorboard: SummaryWriter = None) -> None:
+        self.writer = tensorboard
+        self.log_wandb = log_wandb
+    def add_scalar(self, tag, scalar_value, step):
+        if self.log_wandb:
+            wandb.log({tag: scalar_value}, step=step)
+        self.writer.add_scalar(tag, scalar_value, step)
+    def close(self):
+        self.writer.close()
+
+if __name__ == "__main__":
+    args = tyro.cli(Args)
+    args.batch_size = int(args.num_envs * args.num_steps)
+    args.minibatch_size = int(args.batch_size // args.num_minibatches)
+    args.num_iterations = args.total_timesteps // args.batch_size
+    if args.exp_name is None:
+        args.exp_name = os.path.basename(__file__)[: -len(".py")]
+        run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    else:
+        run_name = args.exp_name
+
+
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    # env setup
+    env_kwargs = dict(obs_mode="state", control_mode="pd_joint_delta_pos", render_mode="rgb_array", sim_backend="gpu")
+    envs = gym.make(args.env_id, num_envs=args.num_envs if not args.evaluate else 1, **env_kwargs)
+    eval_envs = gym.make(args.env_id, num_envs=args.num_eval_envs, reconfiguration_freq=args.reconfiguration_freq, **env_kwargs)
+    if isinstance(envs.action_space, gym.spaces.Dict):
+        envs = FlattenActionSpaceWrapper(envs)
+        eval_envs = FlattenActionSpaceWrapper(eval_envs)
+    if args.capture_video:
+        eval_output_dir = f"runs/{run_name}/videos"
+        if args.evaluate:
+            eval_output_dir = f"{os.path.dirname(args.checkpoint)}/test_videos"
+        print(f"Saving eval videos to {eval_output_dir}")
+        if args.save_train_video_freq is not None:
+            save_video_trigger = lambda x : (x // args.num_steps) % args.save_train_video_freq == 0
+            envs = RecordEpisode(envs, output_dir=f"runs/{run_name}/train_videos", save_trajectory=False, save_video_trigger=save_video_trigger, max_steps_per_video=args.num_steps, video_fps=30)
+        eval_envs = RecordEpisode(eval_envs, output_dir=eval_output_dir, save_trajectory=args.evaluate, trajectory_name="trajectory", max_steps_per_video=args.num_eval_steps, video_fps=30)
+    envs = ManiSkillVectorEnv(envs, args.num_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
+    eval_envs = ManiSkillVectorEnv(eval_envs, args.num_eval_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
+    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
+
+    max_episode_steps = gym_utils.find_max_episode_steps_value(envs._env)
+    logger = None
+    if not args.evaluate:
+        print("Running training")
+        if args.track:
+            import wandb
+            config = vars(args)
+            config["env_cfg"] = dict(**env_kwargs, num_envs=args.num_envs, env_id=args.env_id, reward_mode="normalized_dense", env_horizon=max_episode_steps, partial_reset=args.partial_reset)
+            config["eval_env_cfg"] = dict(**env_kwargs, num_envs=args.num_eval_envs, env_id=args.env_id, reward_mode="normalized_dense", env_horizon=max_episode_steps, partial_reset=args.partial_reset)
+            wandb.init(
+                project=args.wandb_project_name,
+                entity=args.wandb_entity,
+                sync_tensorboard=False,
+                config=config,
+                name=run_name,
+                save_code=True,
+                group="PPO",
+                tags=["ppo", "walltime_efficient"]
+            )
+        writer = SummaryWriter(f"runs/{run_name}")
+        writer.add_text(
+            "hyperparameters",
+            "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+        )
+        logger = Logger(log_wandb=args.track, tensorboard=writer)
+    else:
+        print("Running evaluation")
+
+    agent = Agent(envs).to(device)
+    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
+
+    # ALGO Logic: Storage setup
+    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)
+    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
+    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
+
+    # TRY NOT TO MODIFY: start the game
+    global_step = 0
+    start_time = time.time()
+    next_obs, _ = envs.reset(seed=args.seed)
+    eval_obs, _ = eval_envs.reset(seed=args.seed)
+    next_done = torch.zeros(args.num_envs, device=device)
+    eps_returns = torch.zeros(args.num_envs, dtype=torch.float, device=device)
+    eps_lens = np.zeros(args.num_envs)
+    place_rew = torch.zeros(args.num_envs, device=device)
+    print(f"####")
+    print(f"args.num_iterations={args.num_iterations} args.num_envs={args.num_envs} args.num_eval_envs={args.num_eval_envs}")
+    print(f"args.minibatch_size={args.minibatch_size} args.batch_size={args.batch_size} args.update_epochs={args.update_epochs}")
+    print(f"####")
+    action_space_low, action_space_high = torch.from_numpy(envs.single_action_space.low).to(device), torch.from_numpy(envs.single_action_space.high).to(device)
+    def clip_action(action: torch.Tensor):
+        return torch.clamp(action.detach(), action_space_low, action_space_high)
+
+    if args.checkpoint:
+        agent.load_state_dict(torch.load(args.checkpoint))
+
+    for iteration in range(1, args.num_iterations + 1):
+        print(f"Epoch: {iteration}, global_step={global_step}")
+        final_values = torch.zeros((args.num_steps, args.num_envs), device=device)
+        agent.eval()
+        if iteration % args.eval_freq == 1:
+            # evaluate
+            print("Evaluating")
+            eval_obs, _ = eval_envs.reset()
+            eval_total_reward = 0
+            returns = []
+            eps_lens = []
+            successes = []
+            failures = []
+            for _ in range(args.num_eval_steps):
+                with torch.no_grad():
+                    eval_obs, eval_rew, eval_terminations, eval_truncations, eval_infos = eval_envs.step(agent.get_action(eval_obs, deterministic=True))
+                    eval_total_reward += eval_rew.sum()
+                    if "final_info" in eval_infos:
+                        mask = eval_infos["_final_info"]
+                        eps_lens.append(eval_infos["final_info"]["elapsed_steps"][mask].cpu().numpy())
+                        returns.append(eval_infos["final_info"]["episode"]["r"][mask].cpu().numpy())
+                        if "success" in eval_infos:
+                            successes.append(eval_infos["final_info"]["success"][mask].cpu().numpy())
+                        if "fail" in eval_infos:
+                            failures.append(eval_infos["final_info"]["fail"][mask].cpu().numpy())
+            returns = np.concatenate(returns)
+            eps_lens = np.concatenate(eps_lens)
+            print(f"Evaluated {args.num_eval_steps * args.num_eval_envs} steps resulting in {len(eps_lens)} episodes")
+            if len(successes) > 0:
+                successes = np.concatenate(successes)
+                if logger is not None: logger.add_scalar("eval/success", successes.mean(), global_step)
+                print(f"eval_success_rate={successes.mean()}")
+            if len(failures) > 0:
+                failures = np.concatenate(failures)
+                if logger is not None: logger.add_scalar("eval/fail", failures.mean(), global_step)
+                print(f"eval_fail_rate={failures.mean()}")
+
+            print(f"eval_episodic_return={returns.mean()}")
+            if logger is not None:
+                logger.add_scalar("eval/reward", (eval_total_reward / (args.num_eval_steps * args.num_eval_envs)).mean().cpu().numpy(), global_step)
+                logger.add_scalar("eval/return", returns.mean(), global_step)
+                logger.add_scalar("eval/episode_len", eps_lens.mean(), global_step)
+            if args.evaluate:
+                break
+        if args.save_model and iteration % args.eval_freq == 1:
+            model_path = f"runs/{run_name}/ckpt_{iteration}.pt"
+            torch.save(agent.state_dict(), model_path)
+            print(f"model saved to {model_path}")
+        # Annealing the rate if instructed to do so.
+        if args.anneal_lr:
+            frac = 1.0 - (iteration - 1.0) / args.num_iterations
+            lrnow = frac * args.learning_rate
+            optimizer.param_groups[0]["lr"] = lrnow
+
+        rollout_time = time.time()
+        for step in range(0, args.num_steps):
+            global_step += args.num_envs
+            obs[step] = next_obs
+            dones[step] = next_done
+
+            # ALGO LOGIC: action logic
+            with torch.no_grad():
+                action, logprob, _, value = agent.get_action_and_value(next_obs)
+                values[step] = value.flatten()
+            actions[step] = action
+            logprobs[step] = logprob
+
+            # TRY NOT TO MODIFY: execute the game and log data.
+            next_obs, reward, terminations, truncations, infos = envs.step(clip_action(action))
+            next_done = torch.logical_or(terminations, truncations).to(torch.float32)
+            rewards[step] = reward.view(-1) * args.reward_scale
+
+            if "final_info" in infos:
+                final_info = infos["final_info"]
+                done_mask = infos["_final_info"]
+                episode_len = final_info["elapsed_steps"][done_mask]
+                episodic_return = final_info['episode']['r'][done_mask]
+                if "success" in final_info:
+                    logger.add_scalar("train/success", final_info["success"][done_mask].cpu().numpy().mean(), global_step)
+                if "fail" in final_info:
+                    logger.add_scalar("train/fail", final_info["fail"][done_mask].cpu().numpy().mean(), global_step)
+                logger.add_scalar("train/return", episodic_return.cpu().numpy().mean(), global_step)
+                logger.add_scalar("train/episode_len", episode_len.cpu().numpy().mean(), global_step)
+                logger.add_scalar("train/reward", (episodic_return.sum() / episode_len.sum()).cpu().numpy().mean(), global_step)
+                with torch.no_grad():
+                    final_values[step, torch.arange(args.num_envs, device=device)[done_mask]] = agent.get_value(infos["final_observation"][done_mask]).view(-1)
+        rollout_time = time.time() - rollout_time
+        # bootstrap value according to termination and truncation
+        with torch.no_grad():
+            next_value = agent.get_value(next_obs).reshape(1, -1)
+            advantages = torch.zeros_like(rewards).to(device)
+            lastgaelam = 0
+            for t in reversed(range(args.num_steps)):
+                if t == args.num_steps - 1:
+                    next_not_done = 1.0 - next_done
+                    nextvalues = next_value
+                else:
+                    next_not_done = 1.0 - dones[t + 1]
+                    nextvalues = values[t + 1]
+                real_next_values = next_not_done * nextvalues + final_values[t] # t instead of t+1
+                # next_not_done means nextvalues is computed from the correct next_obs
+                # if next_not_done is 1, final_values is always 0
+                # if next_not_done is 0, then use final_values, which is computed according to bootstrap_at_done
+                if args.finite_horizon_gae:
+                    """
+                    See GAE paper equation(16) line 1, we will compute the GAE based on this line only
+                    1             *(  -V(s_t)  + r_t                                                               + gamma * V(s_{t+1})   )
+                    lambda        *(  -V(s_t)  + r_t + gamma * r_{t+1}                                             + gamma^2 * V(s_{t+2}) )
+                    lambda^2      *(  -V(s_t)  + r_t + gamma * r_{t+1} + gamma^2 * r_{t+2}                         + ...                  )
+                    lambda^3      *(  -V(s_t)  + r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + gamma^3 * r_{t+3}
+                    We then normalize it by the sum of the lambda^i (instead of 1-lambda)
+                    """
+                    if t == args.num_steps - 1: # initialize
+                        lam_coef_sum = 0.
+                        reward_term_sum = 0. # the sum of the second term
+                        value_term_sum = 0. # the sum of the third term
+                    lam_coef_sum = lam_coef_sum * next_not_done
+                    reward_term_sum = reward_term_sum * next_not_done
+                    value_term_sum = value_term_sum * next_not_done
+
+                    lam_coef_sum = 1 + args.gae_lambda * lam_coef_sum
+                    reward_term_sum = args.gae_lambda * args.gamma * reward_term_sum + lam_coef_sum * rewards[t]
+                    value_term_sum = args.gae_lambda * args.gamma * value_term_sum + args.gamma * real_next_values
+
+                    advantages[t] = (reward_term_sum + value_term_sum) / lam_coef_sum - values[t]
+                else:
+                    delta = rewards[t] + args.gamma * real_next_values - values[t]
+                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * next_not_done * lastgaelam # Here actually we should use next_not_terminated, but we don't have lastgamlam if terminated
+            returns = advantages + values
+
+        # flatten the batch
+        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)
+        b_logprobs = logprobs.reshape(-1)
+        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
+        b_advantages = advantages.reshape(-1)
+        b_returns = returns.reshape(-1)
+        b_values = values.reshape(-1)
+
+        # Optimizing the policy and value network
+        agent.train()
+        b_inds = np.arange(args.batch_size)
+        clipfracs = []
+        update_time = time.time()
+        for epoch in range(args.update_epochs):
+            np.random.shuffle(b_inds)
+            for start in range(0, args.batch_size, args.minibatch_size):
+                end = start + args.minibatch_size
+                mb_inds = b_inds[start:end]
+
+                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+
+                with torch.no_grad():
+                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
+                    old_approx_kl = (-logratio).mean()
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
+
+                if args.target_kl is not None and approx_kl > args.target_kl:
+                    break
+
+                mb_advantages = b_advantages[mb_inds]
+                if args.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
+
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
+                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
+
+                # Value loss
+                newvalue = newvalue.view(-1)
+                if args.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + torch.clamp(
+                        newvalue - b_values[mb_inds],
+                        -args.clip_coef,
+                        args.clip_coef,
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+
+                entropy_loss = entropy.mean()
+                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
+
+                optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
+                optimizer.step()
+
+            if args.target_kl is not None and approx_kl > args.target_kl:
+                break
+        update_time = time.time() - update_time
+
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        logger.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+        logger.add_scalar("losses/value_loss", v_loss.item(), global_step)
+        logger.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+        logger.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+        logger.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
+        logger.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
+        logger.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
+        logger.add_scalar("losses/explained_variance", explained_var, global_step)
+        print("SPS:", int(global_step / (time.time() - start_time)))
+        logger.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+        logger.add_scalar("time/step", global_step, global_step)
+        logger.add_scalar("time/update_time", update_time, global_step)
+        logger.add_scalar("time/rollout_time", rollout_time, global_step)
+        logger.add_scalar("time/rollout_fps", args.num_envs * args.num_steps / rollout_time, global_step)
+    if not args.evaluate:
+        if args.save_model:
+            model_path = f"runs/{run_name}/final_ckpt.pt"
+            torch.save(agent.state_dict(), model_path)
+            print(f"model saved to {model_path}")
+        logger.close()
+    envs.close()
+    eval_envs.close()
diff --git a/scripts/baselines/ppo/ppo_rgb.py b/scripts/baselines/ppo/ppo_rgb.py
new file mode 100644
index 0000000..79e3d9e
--- /dev/null
+++ b/scripts/baselines/ppo/ppo_rgb.py
@@ -0,0 +1,594 @@
+# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_continuous_actionpy
+import os
+import random
+import time
+from dataclasses import dataclass
+from typing import Optional
+
+import gymnasium as gym
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.optim as optim
+import tyro
+from torch.distributions.normal import Normal
+from torch.utils.tensorboard import SummaryWriter
+
+# ManiSkill specific imports
+import mani_skill.envs
+from mani_skill.utils import gym_utils
+from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper, FlattenRGBDObservationWrapper
+from mani_skill.utils.wrappers.record import RecordEpisode
+from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
+
+@dataclass
+class Args:
+    exp_name: Optional[str] = None
+    """the name of this experiment"""
+    seed: int = 1
+    """seed of the experiment"""
+    torch_deterministic: bool = True
+    """if toggled, `torch.backends.cudnn.deterministic=False`"""
+    cuda: bool = True
+    """if toggled, cuda will be enabled by default"""
+    track: bool = False
+    """if toggled, this experiment will be tracked with Weights and Biases"""
+    wandb_project_name: str = "ManiSkill"
+    """the wandb's project name"""
+    wandb_entity: str = None
+    """the entity (team) of wandb's project"""
+    capture_video: bool = True
+    """whether to capture videos of the agent performances (check out `videos` folder)"""
+    save_model: bool = True
+    """whether to save model into the `runs/{run_name}` folder"""
+    evaluate: bool = False
+    """if toggled, only runs evaluation with the given model checkpoint and saves the evaluation trajectories"""
+    checkpoint: str = None
+    """path to a pretrained checkpoint file to start evaluation/training from"""
+    render_mode: str = "all"
+    """the environment rendering mode"""
+
+    # Algorithm specific arguments
+    env_id: str = "PickCube-v1"
+    """the id of the environment"""
+    include_state: bool = True
+    """whether to include state information in observations"""
+    total_timesteps: int = 10000000
+    """total timesteps of the experiments"""
+    learning_rate: float = 3e-4
+    """the learning rate of the optimizer"""
+    num_envs: int = 512
+    """the number of parallel environments"""
+    num_eval_envs: int = 8
+    """the number of parallel evaluation environments"""
+    partial_reset: bool = True
+    """whether to let parallel environments reset upon termination instead of truncation"""
+    num_steps: int = 50
+    """the number of steps to run in each environment per policy rollout"""
+    num_eval_steps: int = 50
+    """the number of steps to run in each evaluation environment during evaluation"""
+    reconfiguration_freq: Optional[int] = None
+    """for benchmarking purposes we want to reconfigure the eval environment each reset to ensure objects are randomized in some tasks"""
+    anneal_lr: bool = False
+    """Toggle learning rate annealing for policy and value networks"""
+    gamma: float = 0.8
+    """the discount factor gamma"""
+    gae_lambda: float = 0.9
+    """the lambda for the general advantage estimation"""
+    num_minibatches: int = 32
+    """the number of mini-batches"""
+    update_epochs: int = 4
+    """the K epochs to update the policy"""
+    norm_adv: bool = True
+    """Toggles advantages normalization"""
+    clip_coef: float = 0.2
+    """the surrogate clipping coefficient"""
+    clip_vloss: bool = False
+    """Toggles whether or not to use a clipped loss for the value function, as per the paper."""
+    ent_coef: float = 0.0
+    """coefficient of the entropy"""
+    vf_coef: float = 0.5
+    """coefficient of the value function"""
+    max_grad_norm: float = 0.5
+    """the maximum norm for the gradient clipping"""
+    target_kl: float = 0.2
+    """the target KL divergence threshold"""
+    reward_scale: float = 1.0
+    """Scale the reward by this factor"""
+    eval_freq: int = 25
+    """evaluation frequency in terms of iterations"""
+    save_train_video_freq: Optional[int] = None
+    """frequency to save training videos in terms of iterations"""
+    finite_horizon_gae: bool = True
+
+    # to be filled in runtime
+    batch_size: int = 0
+    """the batch size (computed in runtime)"""
+    minibatch_size: int = 0
+    """the mini-batch size (computed in runtime)"""
+    num_iterations: int = 0
+    """the number of iterations (computed in runtime)"""
+
+def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
+    torch.nn.init.orthogonal_(layer.weight, std)
+    torch.nn.init.constant_(layer.bias, bias_const)
+    return layer
+
+class DictArray(object):
+    def __init__(self, buffer_shape, element_space, data_dict=None, device=None):
+        self.buffer_shape = buffer_shape
+        if data_dict:
+            self.data = data_dict
+        else:
+            assert isinstance(element_space, gym.spaces.dict.Dict)
+            self.data = {}
+            for k, v in element_space.items():
+                if isinstance(v, gym.spaces.dict.Dict):
+                    self.data[k] = DictArray(buffer_shape, v)
+                else:
+                    self.data[k] = torch.zeros(buffer_shape + v.shape).to(device)
+
+    def keys(self):
+        return self.data.keys()
+
+    def __getitem__(self, index):
+        if isinstance(index, str):
+            return self.data[index]
+        return {
+            k: v[index] for k, v in self.data.items()
+        }
+
+    def __setitem__(self, index, value):
+        if isinstance(index, str):
+            self.data[index] = value
+        for k, v in value.items():
+            self.data[k][index] = v
+
+    @property
+    def shape(self):
+        return self.buffer_shape
+
+    def reshape(self, shape):
+        t = len(self.buffer_shape)
+        new_dict = {}
+        for k,v in self.data.items():
+            if isinstance(v, DictArray):
+                new_dict[k] = v.reshape(shape)
+            else:
+                new_dict[k] = v.reshape(shape + v.shape[t:])
+        new_buffer_shape = next(iter(new_dict.values())).shape[:len(shape)]
+        return DictArray(new_buffer_shape, None, data_dict=new_dict)
+
+class NatureCNN(nn.Module):
+    def __init__(self, sample_obs):
+        super().__init__()
+
+        extractors = {}
+
+        self.out_features = 0
+        feature_size = 256
+        in_channels=sample_obs["rgb"].shape[-1]
+        image_size=(sample_obs["rgb"].shape[1], sample_obs["rgb"].shape[2])
+
+
+        # here we use a NatureCNN architecture to process images, but any architecture is permissble here
+        cnn = nn.Sequential(
+            nn.Conv2d(
+                in_channels=in_channels,
+                out_channels=32,
+                kernel_size=8,
+                stride=4,
+                padding=0,
+            ),
+            nn.ReLU(),
+            nn.Conv2d(
+                in_channels=32, out_channels=64, kernel_size=4, stride=2, padding=0
+            ),
+            nn.ReLU(),
+            nn.Conv2d(
+                in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=0
+            ),
+            nn.ReLU(),
+            nn.Flatten(),
+        )
+
+        # to easily figure out the dimensions after flattening, we pass a test tensor
+        with torch.no_grad():
+            n_flatten = cnn(sample_obs["rgb"].float().permute(0,3,1,2).cpu()).shape[1]
+            fc = nn.Sequential(nn.Linear(n_flatten, feature_size), nn.ReLU())
+        extractors["rgb"] = nn.Sequential(cnn, fc)
+        self.out_features += feature_size
+
+        if "state" in sample_obs:
+            # for state data we simply pass it through a single linear layer
+            state_size = sample_obs["state"].shape[-1]
+            extractors["state"] = nn.Linear(state_size, 256)
+            self.out_features += 256
+
+        self.extractors = nn.ModuleDict(extractors)
+
+    def forward(self, observations) -> torch.Tensor:
+        encoded_tensor_list = []
+        # self.extractors contain nn.Modules that do all the processing.
+        for key, extractor in self.extractors.items():
+            obs = observations[key]
+            if key == "rgb":
+                obs = obs.float().permute(0,3,1,2)
+                obs = obs / 255
+            encoded_tensor_list.append(extractor(obs))
+        return torch.cat(encoded_tensor_list, dim=1)
+
+class Agent(nn.Module):
+    def __init__(self, envs, sample_obs):
+        super().__init__()
+        self.feature_net = NatureCNN(sample_obs=sample_obs)
+        # latent_size = np.array(envs.unwrapped.single_observation_space.shape).prod()
+        latent_size = self.feature_net.out_features
+        self.critic = nn.Sequential(
+            layer_init(nn.Linear(latent_size, 512)),
+            nn.ReLU(inplace=True),
+            layer_init(nn.Linear(512, 1)),
+        )
+        self.actor_mean = nn.Sequential(
+            layer_init(nn.Linear(latent_size, 512)),
+            nn.ReLU(inplace=True),
+            layer_init(nn.Linear(512, np.prod(envs.unwrapped.single_action_space.shape)), std=0.01*np.sqrt(2)),
+        )
+        self.actor_logstd = nn.Parameter(torch.ones(1, np.prod(envs.unwrapped.single_action_space.shape)) * -0.5)
+    def get_features(self, x):
+        return self.feature_net(x)
+    def get_value(self, x):
+        x = self.feature_net(x)
+        return self.critic(x)
+    def get_action(self, x, deterministic=False):
+        x = self.feature_net(x)
+        action_mean = self.actor_mean(x)
+        if deterministic:
+            return action_mean
+        action_logstd = self.actor_logstd.expand_as(action_mean)
+        action_std = torch.exp(action_logstd)
+        probs = Normal(action_mean, action_std)
+        return probs.sample()
+    def get_action_and_value(self, x, action=None):
+        x = self.feature_net(x)
+        action_mean = self.actor_mean(x)
+        action_logstd = self.actor_logstd.expand_as(action_mean)
+        action_std = torch.exp(action_logstd)
+        probs = Normal(action_mean, action_std)
+        if action is None:
+            action = probs.sample()
+        return action, probs.log_prob(action).sum(1), probs.entropy().sum(1), self.critic(x)
+
+class Logger:
+    def __init__(self, log_wandb=False, tensorboard: SummaryWriter = None) -> None:
+        self.writer = tensorboard
+        self.log_wandb = log_wandb
+    def add_scalar(self, tag, scalar_value, step):
+        if self.log_wandb:
+            wandb.log({tag: scalar_value}, step=step)
+        self.writer.add_scalar(tag, scalar_value, step)
+    def close(self):
+        self.writer.close()
+
+if __name__ == "__main__":
+    args = tyro.cli(Args)
+    args.batch_size = int(args.num_envs * args.num_steps)
+    args.minibatch_size = int(args.batch_size // args.num_minibatches)
+    args.num_iterations = args.total_timesteps // args.batch_size
+    if args.exp_name is None:
+        args.exp_name = os.path.basename(__file__)[: -len(".py")]
+        run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    else:
+        run_name = args.exp_name
+
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    # env setup
+    env_kwargs = dict(obs_mode="rgb", control_mode="pd_joint_delta_pos", render_mode=args.render_mode, sim_backend="gpu")
+    eval_envs = gym.make(args.env_id, num_envs=args.num_eval_envs, **env_kwargs)
+    envs = gym.make(args.env_id, num_envs=args.num_envs if not args.evaluate else 1, **env_kwargs)
+
+    # rgbd obs mode returns a dict of data, we flatten it so there is just a rgbd key and state key
+    envs = FlattenRGBDObservationWrapper(envs, rgb=True, depth=False, state=args.include_state)
+    eval_envs = FlattenRGBDObservationWrapper(eval_envs, rgb=True, depth=False, state=args.include_state)
+
+    if isinstance(envs.action_space, gym.spaces.Dict):
+        envs = FlattenActionSpaceWrapper(envs)
+        eval_envs = FlattenActionSpaceWrapper(eval_envs)
+    if args.capture_video:
+        eval_output_dir = f"runs/{run_name}/videos"
+        if args.evaluate:
+            eval_output_dir = f"{os.path.dirname(args.checkpoint)}/test_videos"
+        print(f"Saving eval videos to {eval_output_dir}")
+        if args.save_train_video_freq is not None:
+            save_video_trigger = lambda x : (x // args.num_steps) % args.save_train_video_freq == 0
+            envs = RecordEpisode(envs, output_dir=f"runs/{run_name}/train_videos", save_trajectory=False, save_video_trigger=save_video_trigger, max_steps_per_video=args.num_steps, video_fps=30)
+        eval_envs = RecordEpisode(eval_envs, output_dir=eval_output_dir, save_trajectory=args.evaluate, trajectory_name="trajectory", max_steps_per_video=args.num_eval_steps, video_fps=30)
+    envs = ManiSkillVectorEnv(envs, args.num_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
+    eval_envs = ManiSkillVectorEnv(eval_envs, args.num_eval_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
+    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
+
+    max_episode_steps = gym_utils.find_max_episode_steps_value(envs._env)
+    logger = None
+    if not args.evaluate:
+        print("Running training")
+        if args.track:
+            import wandb
+            config = vars(args)
+            config["env_cfg"] = dict(**env_kwargs, num_envs=args.num_envs, env_id=args.env_id, reward_mode="normalized_dense", env_horizon=max_episode_steps, partial_reset=args.partial_reset)
+            config["eval_env_cfg"] = dict(**env_kwargs, num_envs=args.num_eval_envs, env_id=args.env_id, reward_mode="normalized_dense", env_horizon=max_episode_steps, partial_reset=args.partial_reset)
+            wandb.init(
+                project=args.wandb_project_name,
+                entity=args.wandb_entity,
+                sync_tensorboard=False,
+                config=config,
+                name=run_name,
+                save_code=True,
+                group="PPO",
+                tags=["ppo", "walltime_efficient"]
+            )
+        writer = SummaryWriter(f"runs/{run_name}")
+        writer.add_text(
+            "hyperparameters",
+            "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+        )
+        logger = Logger(log_wandb=args.track, tensorboard=writer)
+    else:
+        print("Running evaluation")
+
+    # ALGO Logic: Storage setup
+    obs = DictArray((args.num_steps, args.num_envs), envs.single_observation_space, device=device)
+    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)
+    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)
+    values = torch.zeros((args.num_steps, args.num_envs)).to(device)
+
+    # TRY NOT TO MODIFY: start the game
+    global_step = 0
+    start_time = time.time()
+    next_obs, _ = envs.reset(seed=args.seed)
+    eval_obs, _ = eval_envs.reset(seed=args.seed)
+    next_done = torch.zeros(args.num_envs, device=device)
+    eps_returns = torch.zeros(args.num_envs, dtype=torch.float, device=device)
+    eps_lens = np.zeros(args.num_envs)
+    place_rew = torch.zeros(args.num_envs, device=device)
+    print(f"####")
+    print(f"args.num_iterations={args.num_iterations} args.num_envs={args.num_envs} args.num_eval_envs={args.num_eval_envs}")
+    print(f"args.minibatch_size={args.minibatch_size} args.batch_size={args.batch_size} args.update_epochs={args.update_epochs}")
+    print(f"####")
+    agent = Agent(envs, sample_obs=next_obs).to(device)
+    optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)
+
+    if args.checkpoint:
+        agent.load_state_dict(torch.load(args.checkpoint))
+
+    for iteration in range(1, args.num_iterations + 1):
+        print(f"Epoch: {iteration}, global_step={global_step}")
+        final_values = torch.zeros((args.num_steps, args.num_envs), device=device)
+        agent.eval()
+        if iteration % args.eval_freq == 1:
+            # evaluate
+            print("Evaluating")
+            eval_total_reward = 0
+            eval_obs, _ = eval_envs.reset()
+            returns = []
+            eps_lens = []
+            successes = []
+            failures = []
+            for _ in range(args.num_eval_steps):
+                with torch.no_grad():
+                    eval_obs, eval_rew, eval_terminations, eval_truncations, eval_infos = eval_envs.step(agent.get_action(eval_obs, deterministic=True))
+                    eval_total_reward += eval_rew.sum()
+                    if "final_info" in eval_infos:
+                        mask = eval_infos["_final_info"]
+                        eps_lens.append(eval_infos["final_info"]["elapsed_steps"][mask].cpu().numpy())
+                        returns.append(eval_infos["final_info"]["episode"]["r"][mask].cpu().numpy())
+                        if "success" in eval_infos:
+                            successes.append(eval_infos["final_info"]["success"][mask].cpu().numpy())
+                        if "fail" in eval_infos:
+                            failures.append(eval_infos["final_info"]["fail"][mask].cpu().numpy())
+            returns = np.concatenate(returns)
+            eps_lens = np.concatenate(eps_lens)
+            print(f"Evaluated {args.num_eval_steps * args.num_eval_envs} steps resulting in {len(eps_lens)} episodes")
+            if len(successes) > 0:
+                successes = np.concatenate(successes)
+                if logger is not None: logger.add_scalar("eval/success", successes.mean(), global_step)
+                print(f"eval_success_rate={successes.mean()}")
+            if len(failures) > 0:
+                failures = np.concatenate(failures)
+                if logger is not None: logger.add_scalar("eval/fail", failures.mean(), global_step)
+                print(f"eval_fail_rate={failures.mean()}")
+
+            print(f"eval_episodic_return={returns.mean()}")
+            if logger is not None:
+                logger.add_scalar("eval/reward", (eval_total_reward / (args.num_eval_steps * args.num_eval_envs)).mean().cpu().numpy(), global_step)
+                logger.add_scalar("eval/return", returns.mean(), global_step)
+                logger.add_scalar("eval/episode_len", eps_lens.mean(), global_step)
+            if args.evaluate:
+                break
+        if args.save_model and iteration % args.eval_freq == 1:
+            model_path = f"runs/{run_name}/ckpt_{iteration}.pt"
+            torch.save(agent.state_dict(), model_path)
+            print(f"model saved to {model_path}")
+        # Annealing the rate if instructed to do so.
+        if args.anneal_lr:
+            frac = 1.0 - (iteration - 1.0) / args.num_iterations
+            lrnow = frac * args.learning_rate
+            optimizer.param_groups[0]["lr"] = lrnow
+        rollout_time = time.time()
+        for step in range(0, args.num_steps):
+            global_step += args.num_envs
+            obs[step] = next_obs
+            dones[step] = next_done
+
+            # ALGO LOGIC: action logic
+            with torch.no_grad():
+                action, logprob, _, value = agent.get_action_and_value(next_obs)
+                values[step] = value.flatten()
+            actions[step] = action
+            logprobs[step] = logprob
+
+            # TRY NOT TO MODIFY: execute the game and log data.
+            next_obs, reward, terminations, truncations, infos = envs.step(action)
+            next_done = torch.logical_or(terminations, truncations).to(torch.float32)
+            rewards[step] = reward.view(-1) * args.reward_scale
+
+            if "final_info" in infos:
+                final_info = infos["final_info"]
+                done_mask = infos["_final_info"]
+                episode_len = final_info["elapsed_steps"][done_mask]
+                episodic_return = final_info['episode']['r'][done_mask]
+                if "success" in final_info:
+                    logger.add_scalar("train/success", final_info["success"][done_mask].cpu().numpy().mean(), global_step)
+                if "fail" in final_info:
+                    logger.add_scalar("train/fail", final_info["fail"][done_mask].cpu().numpy().mean(), global_step)
+                logger.add_scalar("train/return", episodic_return.cpu().numpy().mean(), global_step)
+                logger.add_scalar("train/episode_len", episode_len.cpu().numpy().mean(), global_step)
+                logger.add_scalar("train/reward", (episodic_return.sum() / episode_len.sum()).cpu().numpy().mean(), global_step)
+                for k in infos["final_observation"]:
+                    infos["final_observation"][k] = infos["final_observation"][k][done_mask]
+                with torch.no_grad():
+                    final_values[step, torch.arange(args.num_envs, device=device)[done_mask]] = agent.get_value(infos["final_observation"]).view(-1)
+        rollout_time = time.time() - rollout_time
+        # bootstrap value according to termination and truncation
+        with torch.no_grad():
+            next_value = agent.get_value(next_obs).reshape(1, -1)
+            advantages = torch.zeros_like(rewards).to(device)
+            lastgaelam = 0
+            for t in reversed(range(args.num_steps)):
+                if t == args.num_steps - 1:
+                    next_not_done = 1.0 - next_done
+                    nextvalues = next_value
+                else:
+                    next_not_done = 1.0 - dones[t + 1]
+                    nextvalues = values[t + 1]
+                real_next_values = next_not_done * nextvalues + final_values[t] # t instead of t+1
+                # next_not_done means nextvalues is computed from the correct next_obs
+                # if next_not_done is 1, final_values is always 0
+                # if next_not_done is 0, then use final_values, which is computed according to bootstrap_at_done
+                if args.finite_horizon_gae:
+                    """
+                    See GAE paper equation(16) line 1, we will compute the GAE based on this line only
+                    1             *(  -V(s_t)  + r_t                                                               + gamma * V(s_{t+1})   )
+                    lambda        *(  -V(s_t)  + r_t + gamma * r_{t+1}                                             + gamma^2 * V(s_{t+2}) )
+                    lambda^2      *(  -V(s_t)  + r_t + gamma * r_{t+1} + gamma^2 * r_{t+2}                         + ...                  )
+                    lambda^3      *(  -V(s_t)  + r_t + gamma * r_{t+1} + gamma^2 * r_{t+2} + gamma^3 * r_{t+3}
+                    We then normalize it by the sum of the lambda^i (instead of 1-lambda)
+                    """
+                    if t == args.num_steps - 1: # initialize
+                        lam_coef_sum = 0.
+                        reward_term_sum = 0. # the sum of the second term
+                        value_term_sum = 0. # the sum of the third term
+                    lam_coef_sum = lam_coef_sum * next_not_done
+                    reward_term_sum = reward_term_sum * next_not_done
+                    value_term_sum = value_term_sum * next_not_done
+
+                    lam_coef_sum = 1 + args.gae_lambda * lam_coef_sum
+                    reward_term_sum = args.gae_lambda * args.gamma * reward_term_sum + lam_coef_sum * rewards[t]
+                    value_term_sum = args.gae_lambda * args.gamma * value_term_sum + args.gamma * real_next_values
+
+                    advantages[t] = (reward_term_sum + value_term_sum) / lam_coef_sum - values[t]
+                else:
+                    delta = rewards[t] + args.gamma * real_next_values - values[t]
+                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * next_not_done * lastgaelam # Here actually we should use next_not_terminated, but we don't have lastgamlam if terminated
+            returns = advantages + values
+
+        # flatten the batch
+        b_obs = obs.reshape((-1,))
+        b_logprobs = logprobs.reshape(-1)
+        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)
+        b_advantages = advantages.reshape(-1)
+        b_returns = returns.reshape(-1)
+        b_values = values.reshape(-1)
+
+        # Optimizing the policy and value network
+        agent.train()
+        b_inds = np.arange(args.batch_size)
+        clipfracs = []
+        update_time = time.time()
+        for epoch in range(args.update_epochs):
+            np.random.shuffle(b_inds)
+            for start in range(0, args.batch_size, args.minibatch_size):
+                end = start + args.minibatch_size
+                mb_inds = b_inds[start:end]
+
+                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions[mb_inds])
+                logratio = newlogprob - b_logprobs[mb_inds]
+                ratio = logratio.exp()
+
+                with torch.no_grad():
+                    # calculate approx_kl http://joschu.net/blog/kl-approx.html
+                    old_approx_kl = (-logratio).mean()
+                    approx_kl = ((ratio - 1) - logratio).mean()
+                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]
+
+                if args.target_kl is not None and approx_kl > args.target_kl:
+                    break
+
+                mb_advantages = b_advantages[mb_inds]
+                if args.norm_adv:
+                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)
+
+                # Policy loss
+                pg_loss1 = -mb_advantages * ratio
+                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)
+                pg_loss = torch.max(pg_loss1, pg_loss2).mean()
+
+                # Value loss
+                newvalue = newvalue.view(-1)
+                if args.clip_vloss:
+                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2
+                    v_clipped = b_values[mb_inds] + torch.clamp(
+                        newvalue - b_values[mb_inds],
+                        -args.clip_coef,
+                        args.clip_coef,
+                    )
+                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2
+                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)
+                    v_loss = 0.5 * v_loss_max.mean()
+                else:
+                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()
+
+                entropy_loss = entropy.mean()
+                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef
+
+                optimizer.zero_grad()
+                loss.backward()
+                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)
+                optimizer.step()
+
+            if args.target_kl is not None and approx_kl > args.target_kl:
+                break
+        update_time = time.time() - update_time
+        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()
+        var_y = np.var(y_true)
+        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y
+
+        logger.add_scalar("charts/learning_rate", optimizer.param_groups[0]["lr"], global_step)
+        logger.add_scalar("losses/value_loss", v_loss.item(), global_step)
+        logger.add_scalar("losses/policy_loss", pg_loss.item(), global_step)
+        logger.add_scalar("losses/entropy", entropy_loss.item(), global_step)
+        logger.add_scalar("losses/old_approx_kl", old_approx_kl.item(), global_step)
+        logger.add_scalar("losses/approx_kl", approx_kl.item(), global_step)
+        logger.add_scalar("losses/clipfrac", np.mean(clipfracs), global_step)
+        logger.add_scalar("losses/explained_variance", explained_var, global_step)
+        print("SPS:", int(global_step / (time.time() - start_time)))
+        logger.add_scalar("charts/SPS", int(global_step / (time.time() - start_time)), global_step)
+        logger.add_scalar("time/step", global_step, global_step)
+        logger.add_scalar("time/update_time", update_time, global_step)
+        logger.add_scalar("time/rollout_time", rollout_time, global_step)
+        logger.add_scalar("time/rollout_fps", args.num_envs * args.num_steps / rollout_time, global_step)
+    if args.save_model and not args.evaluate:
+        model_path = f"runs/{run_name}/final_ckpt.pt"
+        torch.save(agent.state_dict(), model_path)
+        print(f"model saved to {model_path}")
+
+    envs.close()
+    if logger is not None: logger.close()
diff --git a/scripts/baselines/rfcl/.gitignore b/scripts/baselines/rfcl/.gitignore
new file mode 100644
index 0000000..34f45d7
--- /dev/null
+++ b/scripts/baselines/rfcl/.gitignore
@@ -0,0 +1,4 @@
+/rfcl_jax
+/wandb
+/exps
+/videos
\ No newline at end of file
diff --git a/scripts/baselines/rfcl/README.md b/scripts/baselines/rfcl/README.md
new file mode 100644
index 0000000..a308b38
--- /dev/null
+++ b/scripts/baselines/rfcl/README.md
@@ -0,0 +1,123 @@
+# Reverse Forward Curriculum Learning
+
+Fast offline/online imitation learning from sparse rewards from very few demonstrations in simulation based on ["Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency in Reinforcement Learning (ICLR 2024)"](https://arxiv.org/abs/2405.03379). Code adapted from https://github.com/StoneT2000/rfcl/
+
+This code can be useful for solving tasks, verifying tasks are solvable via neural nets, generating infinite demonstrations via trained neural nets, all without using dense rewards and optionally without action labels.
+
+## Installation
+To get started run `git clone https://github.com/StoneT2000/rfcl.git rfcl_jax` which contains the code for RFCL written in jax. While ManiSkill3 does run on torch, the jax implementation is much more optimized and trains faster.
+
+We recommend using conda/mamba and you can install the dependencies as so:
+
+```bash
+conda create -n "rfcl" "python==3.9"
+conda activate rfcl
+pip install "jax[cuda12_pip]==0.4.28" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
+pip install -e rfcl_jax
+```
+
+Then you can install ManiSkill and its dependencies
+
+```bash
+pip install mani_skill
+pip install torch==2.3.1
+```
+
+We recommend installing the specific jax/torch versions in order to ensure they run correctly.
+
+## Download and Process Dataset
+
+Download demonstrations for a desired task e.g. PickCube-v1
+```bash
+python -m mani_skill.utils.download_demo "PickCube-v1"
+```
+
+<!-- TODO (stao): note how this part can be optional if user wants to do action free learning -->
+Process the demonstrations in preparation for the learning workflow. We will use the teleoperated trajectories to train. Other provided demonstration sources (like motion planning and RL generated) can work as well but may require modifying a few hyperparameters. RFCL is extremely demonstration efficient and so we only need to process and save 5 demonstrations for training here.
+
+```bash
+env_id="PickCube-v1"
+python -m mani_skill.trajectory.replay_trajectory \
+  --traj-path ~/.maniskill/demos/${env_id}/motionplanning/trajectory.h5 \
+  --use-first-env-state \
+  -c pd_joint_delta_pos -o state \
+  --save-traj --count 5
+```
+
+## Train
+
+To train with CPU vectorization (faster with a small number of parallel environments) with walltime efficient hyperparameters run
+
+```bash
+env_id=PickCube-v1
+demos=5 # number of demos to train on
+seed=42
+XLA_PYTHON_CLIENT_PREALLOCATE=false python train.py configs/base_sac_ms3.yml \
+  logger.exp_name=rfcl-${env_id}-state-${demos}_motionplanning_demos-${seed}-walltime_efficient logger.wandb=True \
+  seed=${seed} train.num_demos=${demos} train.steps=1_000_000 \
+  env.env_id=${env_id} \
+  train.dataset_path="~/.maniskill/demos/${env_id}/motionplanning/trajectory.state.pd_joint_delta_pos.h5" \
+  demo_type="motionplanning" config_type="walltime_efficient" # additional tags for logging purposes on wandb
+```
+
+You can add `train.train_on_demo_actions=False` to train on demonstrations without any action labels, just environment states. This may be useful if you can only download a dataset but can't convert the actions to the desired action space (some tasks can't easily convert actions)/
+
+To train with sample efficient hyperparameters run
+
+```bash
+env_id=PickCube-v1
+demos=5 # number of demos to train on
+seed=42
+XLA_PYTHON_CLIENT_PREALLOCATE=false python train.py configs/base_sac_ms3_sample_efficient.yml \
+  logger.exp_name=rfcl-${env_id}-state-${demos}_motionplanning_demos-${seed}-sample_efficient logger.wandb=True \
+  seed=${seed} train.num_demos=${demos} train.steps=1_000_000 \
+  env.env_id=${env_id} \
+  train.dataset_path="~/.maniskill/demos/${env_id}/motionplanning/trajectory.state.pd_joint_delta_pos.h5" \
+  demo_type="motionplanning" config_type="sample_efficient" # additional tags for logging purposes on wandb
+```
+
+Version of RFCL that runs on the GPU vectorized environments is currently not implemented as the current code is already quite fast and will require future research to investigate how to leverage GPU simulation with RFCL.
+
+
+## Generating Demonstrations with Learned Policy 
+
+
+To generate 1000 demonstrations with a trained policy you can run
+
+```bash
+XLA_PYTHON_CLIENT_PREALLOCATE=false python rfcl_jax/scripts/collect_demos.py exps/path/to/model.jx \
+  num_envs=8 num_episodes=1000
+```
+This saves the demos which uses CPU vectorization to generate demonstrations in parallel. Note that while the demos are generated on the CPU, you can always convert them to demonstrations on the GPU via the [replay trajectory tool](https://maniskill.readthedocs.io/en/latest/user_guide/datasets/replay.html) as so
+
+```bash
+python -m mani_skill.trajectory.replay_trajectory \
+  --traj-path exps/<exp_name>/eval_videos/trajectory.h5 \
+  -b gpu --use-first-env-state --save-traj
+```
+
+The replay_trajectory tool can also be used to generate videos
+
+See the rlpd_jax/scripts/collect_demos.py code for details on how to load the saved policies and modify it to your needs.
+
+
+## Citation
+
+If you use this baseline please cite the following
+```
+@inproceedings{DBLP:conf/iclr/TaoSC024,
+  author       = {Stone Tao and
+                  Arth Shukla and
+                  Tse{-}kai Chan and
+                  Hao Su},
+  title        = {Reverse Forward Curriculum Learning for Extreme Sample and Demo Efficiency},
+  booktitle    = {The Twelfth International Conference on Learning Representations,
+                  {ICLR} 2024, Vienna, Austria, May 7-11, 2024},
+  publisher    = {OpenReview.net},
+  year         = {2024},
+  url          = {https://openreview.net/forum?id=w4rODxXsmM},
+  timestamp    = {Wed, 07 Aug 2024 17:11:53 +0200},
+  biburl       = {https://dblp.org/rec/conf/iclr/TaoSC024.bib},
+  bibsource    = {dblp computer science bibliography, https://dblp.org}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/rfcl/configs/base_sac_ms3.yml b/scripts/baselines/rfcl/configs/base_sac_ms3.yml
new file mode 100644
index 0000000..c5ad472
--- /dev/null
+++ b/scripts/baselines/rfcl/configs/base_sac_ms3.yml
@@ -0,0 +1,97 @@
+jax_env: False
+
+seed: 0
+algo: sac
+verbose: 1
+# Environment configuration
+env:
+  env_id: None
+  max_episode_steps: 50
+  num_envs: 8
+  env_type: "gym:cpu"
+  env_kwargs:
+    control_mode: "pd_joint_delta_pos"
+    render_mode: "rgb_array"
+    reward_mode: "sparse"
+eval_env:
+  num_envs: 2
+  max_episode_steps: 50
+
+sac:
+  num_seed_steps: 5_000
+  seed_with_policy: False
+  replay_buffer_capacity: 1_000_000
+  batch_size: 256
+  steps_per_env: 4
+  grad_updates_per_step: 16
+  actor_update_freq: 1
+
+  num_qs: 2
+  num_min_qs: 2
+
+  discount: 0.9
+  tau: 0.005
+  backup_entropy: False
+
+  eval_freq: 50_000
+  eval_steps: 250
+
+  log_freq: 1000
+  save_freq: 50_000
+
+  learnable_temp: True
+  initial_temperature: 1.0
+  
+network:
+  actor:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+  critic:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+      use_layer_norm: True
+
+train:
+  actor_lr: 3e-4
+  critic_lr: 3e-4
+  steps: 100_000_000
+  dataset_path: None
+  shuffle_demos: True
+  num_demos: 1000
+
+  data_action_scale: null
+
+  ## Reverse curriculum configs
+  reverse_step_size: 4
+  start_step_sampler: "geometric"
+  curriculum_method: "per_demo"
+  per_demo_buffer_size: 3
+  demo_horizon_to_max_steps_ratio: 3
+  train_on_demo_actions: True
+
+  load_actor: True
+  load_critic: True
+  load_as_offline_buffer: True
+  load_as_online_buffer: False
+
+  ## Forward curriculum configs
+  forward_curriculum: "success_once_score"
+  staleness_coef: 0.1
+  staleness_temperature: 0.1
+  staleness_transform: "rankmin"
+  score_transform: "rankmin"
+  score_temperature: 0.1
+  num_seeds: 1000
+
+logger:
+  tensorboard: True
+  wandb: False
+
+  workspace: "exps"
+  project_name: "ManiSkill"
+  wandb_cfg:
+    group: "RFCL"
diff --git a/scripts/baselines/rfcl/configs/base_sac_ms3_sample_efficient.yml b/scripts/baselines/rfcl/configs/base_sac_ms3_sample_efficient.yml
new file mode 100644
index 0000000..09c5ec7
--- /dev/null
+++ b/scripts/baselines/rfcl/configs/base_sac_ms3_sample_efficient.yml
@@ -0,0 +1,97 @@
+jax_env: False
+
+seed: 0
+algo: sac
+verbose: 1
+# Environment configuration
+env:
+  env_id: None
+  max_episode_steps: 50
+  num_envs: 8
+  env_type: "gym:cpu"
+  env_kwargs:
+    control_mode: "pd_joint_delta_pos"
+    render_mode: "rgb_array"
+    reward_mode: "sparse"
+eval_env:
+  num_envs: 2
+  max_episode_steps: 50
+
+sac:
+  num_seed_steps: 5_000
+  seed_with_policy: False
+  replay_buffer_capacity: 1_000_000
+  batch_size: 256
+  steps_per_env: 1
+  grad_updates_per_step: 80
+  actor_update_freq: 20
+
+  num_qs: 10
+  num_min_qs: 2
+
+  discount: 0.9
+  tau: 0.005
+  backup_entropy: False
+
+  eval_freq: 5_000
+  eval_steps: 500
+
+  log_freq: 1000
+  save_freq: 10_000
+
+  learnable_temp: True
+  initial_temperature: 1.0
+  
+network:
+  actor:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+  critic:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+      use_layer_norm: True
+
+train:
+  actor_lr: 3e-4
+  critic_lr: 3e-4
+  steps: 100_000_000
+  dataset_path: None
+  shuffle_demos: True
+  num_demos: 1000
+
+  data_action_scale: null
+
+  ## Reverse curriculum configs
+  reverse_step_size: 4
+  start_step_sampler: "geometric"
+  curriculum_method: "per_demo"
+  per_demo_buffer_size: 3
+  demo_horizon_to_max_steps_ratio: 3
+  train_on_demo_actions: True
+
+  load_actor: True
+  load_critic: True
+  load_as_offline_buffer: True
+  load_as_online_buffer: False
+
+  ## Forward curriculum configs
+  forward_curriculum: "success_once_score"
+  staleness_coef: 0.1
+  staleness_temperature: 0.1
+  staleness_transform: "rankmin"
+  score_transform: "rankmin"
+  score_temperature: 0.1
+  num_seeds: 1000
+
+logger:
+  tensorboard: True
+  wandb: False
+
+  workspace: "exps"
+  project_name: "ManiSkill"
+  wandb_cfg:
+    group: "RFCL"
diff --git a/scripts/baselines/rfcl/train.py b/scripts/baselines/rfcl/train.py
new file mode 100644
index 0000000..320db51
--- /dev/null
+++ b/scripts/baselines/rfcl/train.py
@@ -0,0 +1,405 @@
+"""
+Code to run Reverse Forward Curriculum Learning.
+Configs can be a bit complicated, we recommend directly looking at configs/ms2/base_sac_ms2_sample_efficient.yml for what options are available.
+Alternatively, go to the file defining each of the nested configurations and see the comments.
+"""
+import copy
+import os
+import os.path as osp
+import sys
+import warnings
+from dataclasses import asdict, dataclass
+from typing import Optional
+
+import gymnasium as gym
+import jax
+import numpy as np
+import optax
+from omegaconf import OmegaConf
+import wandb as wb
+
+from rfcl.agents.sac import SAC, ActorCritic, SACConfig
+from rfcl.agents.sac.networks import DiagGaussianActor
+from rfcl.data.dataset import ReplayDataset, get_states_dataset
+from rfcl.envs.make_env import EnvConfig, get_initial_state_wrapper, make_env_from_cfg
+from rfcl.envs.wrappers.curriculum import ReverseCurriculumWrapper
+from rfcl.envs.wrappers.forward_curriculum import SeedBasedForwardCurriculumWrapper
+from rfcl.logger import LoggerConfig
+from rfcl.models import NetworkConfig, build_network_from_cfg
+from rfcl.utils.parse import parse_cfg
+from rfcl.utils.spaces import get_action_dim
+
+
+os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
+
+
+@dataclass
+class TrainConfig:
+    steps: int
+    actor_lr: float
+    critic_lr: float
+    dataset_path: str
+    shuffle_demos: bool
+    num_demos: int
+
+    data_action_scale: Optional[float]
+
+    # reverse curriculum wrapper configs
+    reverse_step_size: int
+    curriculum_method: str
+    start_step_sampler: str
+    per_demo_buffer_size: int
+    demo_horizon_to_max_steps_ratio: float
+    train_on_demo_actions: bool
+
+    # forward curriculum configs
+    forward_curriculum: str
+    staleness_transform: str
+    staleness_coef: float
+    staleness_temperature: float
+    score_transform: str
+    score_temperature: float
+    num_seeds: int
+
+    # stage 2 training configs
+    load_actor: bool
+    load_critic: bool
+    load_as_offline_buffer: bool
+    load_as_online_buffer: bool
+
+    # other configs that are generally used for experimentation
+    use_orig_env_for_eval: bool = True
+    eval_start_of_demos: bool = False
+
+
+@dataclass
+class SACNetworkConfig:
+    actor: NetworkConfig
+    critic: NetworkConfig
+
+
+@dataclass
+class SACExperiment:
+    seed: int
+    sac: SACConfig
+    env: EnvConfig
+    eval_env: EnvConfig
+    train: TrainConfig
+    network: SACNetworkConfig
+    logger: Optional[LoggerConfig]
+    verbose: int
+    algo: str = "sac"
+    stage_1_model_path: str = None  # if not None, will load pretrained stage 1 model and skip to stage 2 of training
+    save_eval_video: bool = True  # whether to save eval videos
+    stage_1_only: bool = False  # stop training after reverse curriculum completes
+    stage_2_only: bool = False # skip stage 1 training
+    demo_seed: int = None  # fix a seed to fix which demonstrations are sampled from a dataset
+
+    """additional tags/configs for logging purposes to wandb and shared comparisons with other algorithms"""
+    demo_type: str = None
+    config_type: str = None # "sample_efficient" or "walltime_efficient"
+
+from dacite import from_dict
+
+
+def main(cfg: SACExperiment):
+    np.random.seed(cfg.seed)
+
+    ### Setup the experiment parameters ###
+
+    # Setup training and evaluation environment configs
+    env_cfg = cfg.env
+    if "env_kwargs" not in env_cfg:
+        env_cfg["env_kwargs"] = dict()
+    cfg.eval_env = {**env_cfg, **cfg.eval_env}
+    cfg = from_dict(data_class=SACExperiment, data=OmegaConf.to_container(cfg))
+    env_cfg = cfg.env
+    eval_env_cfg = cfg.eval_env
+
+    # change exp name if it exists
+    orig_exp_name = cfg.logger.exp_name
+    exp_path = osp.join(cfg.logger.workspace, orig_exp_name)
+    if osp.exists(exp_path):
+        i = 1
+        prev_exp_path = exp_path
+        while osp.exists(exp_path):
+            prev_exp_path = exp_path
+            cfg.logger.exp_name = f"{orig_exp_name}_{i}"
+            exp_path = osp.join(cfg.logger.workspace, cfg.logger.exp_name)
+            i += 1
+        warnings.warn(f"{prev_exp_path} already exists. Changing exp_name to {cfg.logger.exp_name}")
+    video_path = osp.join(cfg.logger.workspace, cfg.logger.exp_name, "stage_1_videos")
+
+    cfg.sac.num_envs = cfg.env.num_envs
+    cfg.sac.num_eval_envs = cfg.eval_env.num_envs
+
+    ### Create Environments ###
+    if cfg.demo_seed is not None:
+        np.random.seed(cfg.demo_seed)
+
+    states_dataset = get_states_dataset(cfg.train.dataset_path, num_demos=cfg.train.num_demos, shuffle=cfg.train.shuffle_demos, skip_failed=True)
+
+    if "reward_mode" in cfg.env.env_kwargs:
+        reward_mode = cfg.env.env_kwargs["reward_mode"]
+    elif "reward_type" in cfg.env.env_kwargs:
+        reward_mode = cfg.env.env_kwargs["reward_type"]
+    else:
+        raise ValueError("reward_mode is not specified")
+
+    if cfg.train.train_on_demo_actions:
+        demo_replay_dataset = ReplayDataset(
+            cfg.train.dataset_path,
+            shuffle=cfg.train.shuffle_demos,
+            skip_failed=False,
+            num_demos=cfg.train.num_demos,
+            reward_mode=reward_mode,
+            eps_ids=states_dataset.keys(), # forces the demo replay dataset used as the offline buffer to use the same demos as the reverse curriculum
+            data_action_scale=cfg.train.data_action_scale,
+        )
+        if demo_replay_dataset.action_scale is not None:
+            env_cfg.action_scale = demo_replay_dataset.action_scale.tolist()
+            eval_env_cfg.action_scale = env_cfg.action_scale
+    InitialStateWrapper = get_initial_state_wrapper(cfg.env.env_id)
+    np.random.seed(cfg.seed)
+    wrappers = [
+        lambda env: InitialStateWrapper(
+            env,
+            states_dataset=states_dataset,
+            demo_horizon_to_max_steps_ratio=cfg.train.demo_horizon_to_max_steps_ratio,
+        )
+    ]
+    env, env_meta = make_env_from_cfg(env_cfg, seed=cfg.seed, wrappers=wrappers)
+    eval_env = None
+    use_orig_env_for_eval = cfg.train.use_orig_env_for_eval
+    if cfg.sac.num_eval_envs > 0:
+        eval_wrappers = []
+        if not use_orig_env_for_eval:
+            eval_wrappers = wrappers
+        eval_env, _ = make_env_from_cfg(
+            eval_env_cfg,
+            seed=cfg.seed + 1_000_000,
+            video_path=video_path if cfg.save_eval_video else None,
+            wrappers=eval_wrappers,
+        )
+    # add reverse curriculum wrapper
+    link_envs = []
+    if not use_orig_env_for_eval:
+        eval_env = ReverseCurriculumWrapper(
+            eval_env,
+            eval_mode=True,
+            eval_start_of_demos=cfg.train.eval_start_of_demos,
+            states_dataset=states_dataset,
+            reverse_step_size=cfg.train.reverse_step_size,
+            curriculum_method=cfg.train.curriculum_method,
+            per_demo_buffer_size=cfg.train.per_demo_buffer_size,
+            start_step_sampler=cfg.train.start_step_sampler,
+        )
+        link_envs = [eval_env]
+    env = ReverseCurriculumWrapper(
+        env,
+        states_dataset=states_dataset,
+        reverse_step_size=cfg.train.reverse_step_size,
+        curriculum_method=cfg.train.curriculum_method,
+        per_demo_buffer_size=cfg.train.per_demo_buffer_size,
+        start_step_sampler=cfg.train.start_step_sampler,
+        link_envs=link_envs,
+    )
+
+    sample_obs, sample_acts = env_meta.sample_obs, env_meta.sample_acts
+
+    # create actor and critics models
+    act_dims = get_action_dim(env_meta.act_space)
+
+    def create_ac_model():
+        actor = DiagGaussianActor(
+            feature_extractor=build_network_from_cfg(cfg.network.actor),
+            act_dims=act_dims,
+            state_dependent_std=True,
+        )
+        ac = ActorCritic.create(
+            jax.random.PRNGKey(cfg.seed),
+            actor=actor,
+            critic_feature_extractor=build_network_from_cfg(cfg.network.critic),
+            sample_obs=sample_obs,
+            sample_acts=sample_acts,
+            initial_temperature=cfg.sac.initial_temperature,
+            actor_optim=optax.adam(learning_rate=cfg.train.actor_lr),
+            critic_optim=optax.adam(learning_rate=cfg.train.critic_lr),
+        )
+        return ac
+
+    # create our algorithm
+    ac = create_ac_model()
+    cfg.logger.cfg = asdict(cfg)
+    logger_cfg = cfg.logger
+    algo = SAC(
+        env=env,
+        eval_env=eval_env,
+        env_type=cfg.env.env_type,
+        ac=ac,
+        logger_cfg=logger_cfg,
+        cfg=cfg.sac,
+    )
+
+    # for ManiSkill 3 baselines, try to modify the wandb config to match other baselines env_cfg setups.
+    if algo.logger.wandb:
+        import wandb as wb
+        sim_backend = "cpu"
+        if cfg.env.env_type == "gym:cpu":
+            sim_backend = "cpu"
+        elif cfg.env.env_type == "gym:gpu":
+            sim_backend = "gpu"
+        def parse_env_cfg(env_cfg):
+            return {
+                "env_id": cfg.env.env_id,
+                "env_kwargs": cfg.env.env_kwargs,
+                "num_envs": cfg.env.num_envs,
+                "env_horizon": cfg.env.max_episode_steps,
+                "sim_backend": sim_backend,
+                "reward_mode": cfg.env.env_kwargs.get("reward_mode"),
+                "obs_mode": cfg.env.env_kwargs.get("obs_mode"),
+                "control_mode": cfg.env.env_kwargs.get("control_mode"),
+            }
+        fixed_wb_cfgs = {"env_cfg": parse_env_cfg(env_cfg), "eval_env_cfg": parse_env_cfg(eval_env_cfg), "num_demos": cfg.train.num_demos, "demo_type": cfg.demo_type}
+        wb.config.update({**fixed_wb_cfgs}, allow_val_change=True)
+        algo.logger.wandb_run.tags = ["rfcl", cfg.config_type]
+
+
+    ###########################################
+    # Stage 1 Training: Reverse Curriculum RL #
+    ###########################################
+
+    if cfg.train.train_on_demo_actions:
+        algo.offline_buffer = demo_replay_dataset  # create offline buffer to oversample from
+
+    if not cfg.stage_2_only:
+        def early_stop_fn(locals):
+            # callback function to log reverse curriculum metrics and stop training once reverse curriculum is done
+            nonlocal env, algo
+            logger = algo.logger
+            demo_metadata = env.demo_metadata
+            pts = []
+            solved_frac = 0
+            for k in demo_metadata:
+                pts.append(demo_metadata[k].start_step / (demo_metadata[k].total_steps - 1))
+                solved_frac += int(demo_metadata[k].solved)
+            solved_frac = solved_frac / len(demo_metadata)
+            mean_start_step = np.mean(pts)
+            logger.tb_writer.add_histogram("train_stats/start_step_frac_dist", pts, algo.state.total_env_steps)
+            logger.tb_writer.add_scalar("train_stats/start_step_frac_avg", mean_start_step, algo.state.total_env_steps)
+            if logger.wandb:
+                import wandb as wb
+
+                logger.wandb_run.log(data={"train_stats/start_step_frac_dist": wb.Histogram(pts)}, step=algo.state.total_env_steps)
+                logger.wandb_run.log(data={"train_stats/start_step_frac_avg": mean_start_step}, step=algo.state.total_env_steps)
+
+            if solved_frac > 0.9:
+                print("Reverse solved > 0.9 of demos. Stopping stage 1")
+                return True
+            return False
+
+        if cfg.stage_1_model_path is None:
+            rng_key, train_rng_key = jax.random.split(jax.random.PRNGKey(cfg.seed), 2)
+            algo.train(
+                rng_key=train_rng_key,
+                steps=cfg.train.steps,
+                callback_fn=early_stop_fn,
+                verbose=cfg.verbose,
+            )
+            algo.save(osp.join(algo.logger.model_path, "stage_1.jx"), with_buffer=True)
+            algo.logger.tb_writer.add_scalar("train_stats/stage_1_steps", algo.state.total_env_steps, algo.state.total_env_steps)
+            if algo.logger.wandb:
+                algo.logger.wandb_run.log(data={"train_stats/stage_1_steps": algo.state.total_env_steps}, step=algo.state.total_env_steps)
+        else:
+            print(f"Loading stage 1 model: {cfg.stage_1_model_path}")
+            algo.load_from_path(cfg.stage_1_model_path)
+
+    if cfg.stage_1_only:
+        exit()
+
+    ###############################
+    # Stage 2 Training: Normal RL with Forward Curriculums #
+    ###############################
+    print("Stage 2 Training starting")
+    # Optionally load actor/critic networks from stage 1 of training
+    ac = create_ac_model()
+    if cfg.train.load_actor:
+        ac = ac.load(algo.state.ac.state_dict(), load_critic=cfg.train.load_critic)
+        algo.state = algo.state.replace(ac=ac)
+
+    if not cfg.stage_2_only:
+        # if not stage 2 only, there is a stage 1 replay buffer we can use
+        # Load previous model's replay buffer as a separate offline buffer to sample from or directly into the online buffer
+        if cfg.train.load_as_offline_buffer:
+            print(
+                f"Loading replay buffer as offline buffer which contains {algo.replay_buffer.size() * algo.replay_buffer.num_envs} interactions. Reset online buffer"
+            )
+            algo.offline_buffer = copy.deepcopy(algo.replay_buffer)
+            algo.replay_buffer.reset()
+        if cfg.train.load_as_online_buffer:
+            print(
+                f"Loading replay buffer into online buffer which contains {algo.replay_buffer.size() * algo.replay_buffer.num_envs} interactions. No offline buffer"
+            )
+            algo.offline_buffer = None
+
+    # Switch environments from the reverse curriculum environments to a normal environment
+    env.close(), eval_env.close()
+
+    video_path = osp.join(cfg.logger.workspace, cfg.logger.exp_name, "stage_2_videos")
+    wrappers = []
+    if cfg.train.data_action_scale is not None:
+        rescale_action_wrapper = lambda x: gym.wrappers.RescaleAction(x, -demo_replay_dataset.action_scale, demo_replay_dataset.action_scale)
+        clip_wrapper = lambda x: gym.wrappers.ClipAction(x)
+        wrappers += [rescale_action_wrapper, clip_wrapper]
+
+    env, env_meta = make_env_from_cfg(env_cfg, seed=cfg.seed, wrappers=wrappers)
+    eval_env = None
+    if cfg.sac.num_eval_envs > 0:
+        eval_wrappers = []
+        if cfg.train.data_action_scale is not None:
+            eval_wrappers += [rescale_action_wrapper, clip_wrapper]
+        eval_env, _ = make_env_from_cfg(
+            eval_env_cfg,
+            seed=cfg.seed + 1_000_000,
+            video_path=video_path if cfg.save_eval_video else None,
+            wrappers=eval_wrappers,
+        )
+
+    print(f"Forward curriculum: {cfg.train.forward_curriculum}")
+    if cfg.train.forward_curriculum is not None and cfg.train.forward_curriculum != "None":
+        env = SeedBasedForwardCurriculumWrapper(
+            env,
+            score_transform=cfg.train.score_transform,
+            score_temperature=cfg.train.score_temperature,
+            staleness_transform=cfg.train.staleness_transform,
+            staleness_temperature=cfg.train.staleness_temperature,
+            staleness_coef=cfg.train.staleness_coef,
+            score_fn=cfg.train.forward_curriculum,
+            rho=0,
+            nu=0.95,
+            num_seeds=cfg.train.num_seeds,
+        )
+        env.reset(seed=cfg.seed)
+    algo.setup_envs(env, eval_env)
+    algo.state = algo.state.replace(initialized=False)
+
+    (
+        rng_key,
+        train_rng_key,
+    ) = jax.random.split(jax.random.PRNGKey(cfg.seed), 2)
+
+    # we seed with policy in stage 2 for algo.cfg.num_seed_steps
+    algo.cfg.seed_with_policy = True
+    algo.cfg.num_seed_steps = algo.state.total_env_steps + algo.cfg.num_seed_steps
+    print(f"Seeding until {algo.cfg.num_seed_steps}")
+    algo.train(
+        rng_key=train_rng_key,
+        steps=cfg.train.steps - algo.state.total_env_steps,
+        verbose=cfg.verbose,
+    )
+    algo.save(osp.join(algo.logger.model_path, "latest.jx"), with_buffer=False)
+
+
+if __name__ == "__main__":
+    cfg = parse_cfg(default_cfg_path=sys.argv[1])
+    main(cfg)
diff --git a/scripts/baselines/rlpd/.gitignore b/scripts/baselines/rlpd/.gitignore
new file mode 100644
index 0000000..b34b3f5
--- /dev/null
+++ b/scripts/baselines/rlpd/.gitignore
@@ -0,0 +1,4 @@
+/rlpd_jax
+/wandb
+/exps
+/videos
\ No newline at end of file
diff --git a/scripts/baselines/rlpd/README.md b/scripts/baselines/rlpd/README.md
new file mode 100644
index 0000000..d2828b4
--- /dev/null
+++ b/scripts/baselines/rlpd/README.md
@@ -0,0 +1,132 @@
+# Reinforcement Learning from Prior Data (RLPD)
+
+Sample-efficient offline/online imitation learning from sparse rewards leveraging prior data based on ["Efficient Online Reinforcement Learning with Offline Data
+(ICML 2023)"](https://arxiv.org/abs/2302.02948). Code adapted from https://github.com/ikostrikov/rlpd
+
+RLPD leverages prior collected trajectory data (expert and non-expert work) and trains on the prior data while collecting online data to sample efficiently learn a policy to solve a task with just sparse rewards.
+
+## Installation
+
+To get started run `git clone https://github.com/StoneT2000/rfcl.git rlpd_jax` which contains the code for RLPD written in jax (a partial fork of the original RLPD and JaxRL repos that has been optimized to run faster and support vectorized environments).
+
+We recommend using conda/mamba and you can install the dependencies as so:
+
+```bash
+conda create -n "rlpd_ms3" "python==3.9"
+conda activate rlpd_ms3
+pip install --upgrade "jax[cuda12_pip]==0.4.28" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
+pip install -e rlpd_jax
+```
+
+Then you can install ManiSkill and its dependencies
+
+```bash
+pip install mani_skill torch==2.3.1
+```
+Note that since jax and torch are used, we recommend installing the specific versions detailed in the commands above as those are tested to work together.
+
+## Download and Process Dataset
+
+Download demonstrations for a desired task e.g. PickCube-v1
+```bash
+python -m mani_skill.utils.download_demo "PickCube-v1"
+```
+
+<!-- TODO (stao): note how this part can be optional if user wants to do action free learning -->
+Process the demonstrations in preparation for the learning workflow. RLPD works well for harder tasks if sufficient data is provided and the data itself is not too multi-modal. Hence we will use the RL generated trajectories (lot of data and not multi-modal so much easier to learn) for the example below:
+
+
+The preprocessing step here simply replays all trajectories by environment state (so the exact same trajectory is returned) and save the state observations to train on. Moreover failed demos are also saved as RLPD can learn from sub-optimal data as well.
+
+```bash
+env_id="PickCube-v1"
+python -m mani_skill.trajectory.replay_trajectory \
+  --traj-path ~/.maniskill/demos/${env_id}/rl/trajectory.h5 \
+  --use-env-state --allow-failure \
+  -c pd_joint_delta_pos -o state \
+  --save-traj --num-procs 4
+```
+
+## Train
+
+To train with environment vectorization run
+
+```bash
+env_id=PickCube-v1
+demos=1000 # number of demos to train on.
+seed=42
+XLA_PYTHON_CLIENT_PREALLOCATE=false python train_ms3.py configs/base_rlpd_ms3.yml \
+  logger.exp_name="rlpd-${env_id}-state-${demos}_rl_demos-${seed}-walltime_efficient" logger.wandb=True \
+  seed=${seed} train.num_demos=${demos} train.steps=200_000 \
+  env.env_id=${env_id} \
+  train.dataset_path="~/.maniskill/demos/${env_id}/rl/trajectory.state.pd_joint_delta_pos.h5" \
+  demo_type="rl" config_type="walltime_efficient" # additional tags for logging purposes on wandb
+```
+
+This should solve the PickCube-v1 task in a few minutes, but won't get good sample efficiency.
+
+For sample-efficient settings you can use the sample-efficient configurations stored in configs/base_rlpd_ms3_sample_efficient.yml (no env parallelization, more critics, higher update-to-data ratio). This will take less environment samples (around 50K to solve) but runs slower.
+
+```bash
+env_id=PickCube-v1
+demos=1000 # number of demos to train on.
+seed=42
+XLA_PYTHON_CLIENT_PREALLOCATE=false python train_ms3.py configs/base_rlpd_ms3_sample_efficient.yml \
+  logger.exp_name="rlpd-${env_id}-state-${demos}_rl_demos-${seed}-sample_efficient" logger.wandb=True \
+  seed=${seed} train.num_demos=${demos} train.steps=100_000 \
+  env.env_id=${env_id} \
+  train.dataset_path="~/.maniskill/demos/${env_id}/rl/trajectory.state.pd_joint_delta_pos.h5" \
+  demo_type="rl" config_type="sample_efficient" # additional tags for logging purposes on wandb
+```
+
+evaluation videos are saved to `exps/<exp_name>/videos`.
+
+## Generating Demonstrations / Evaluating policies
+
+To generate 1000 demonstrations with a trained policy you can run
+
+```bash
+XLA_PYTHON_CLIENT_PREALLOCATE=false python rlpd_jax/scripts/collect_demos.py exps/path/to/model.jx \
+  num_envs=8 num_episodes=1000
+```
+This saves the demos which uses CPU vectorization to generate demonstrations in parallel. Note that while the demos are generated on the CPU, you can always convert them to demonstrations on the GPU via the [replay trajectory tool](https://maniskill.readthedocs.io/en/latest/user_guide/datasets/replay.html) as so
+
+```bash
+python -m mani_skill.trajectory.replay_trajectory \
+  --traj-path exps/<exp_name>/eval_videos/trajectory.h5 \
+  -b gpu --use-first-env-state
+```
+
+The replay_trajectory tool can also be used to generate videos
+
+See the rlpd_jax/scripts/collect_demos.py code for details on how to load the saved policies and modify it to your needs.
+
+## Citation
+
+If you use this baseline please cite the following
+```
+@inproceedings{DBLP:conf/icml/BallSKL23,
+  author       = {Philip J. Ball and
+                  Laura M. Smith and
+                  Ilya Kostrikov and
+                  Sergey Levine},
+  editor       = {Andreas Krause and
+                  Emma Brunskill and
+                  Kyunghyun Cho and
+                  Barbara Engelhardt and
+                  Sivan Sabato and
+                  Jonathan Scarlett},
+  title        = {Efficient Online Reinforcement Learning with Offline Data},
+  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
+                  2023, Honolulu, Hawaii, {USA}},
+  series       = {Proceedings of Machine Learning Research},
+  volume       = {202},
+  pages        = {1577--1594},
+  publisher    = {{PMLR}},
+  year         = {2023},
+  url          = {https://proceedings.mlr.press/v202/ball23a.html},
+  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
+  biburl       = {https://dblp.org/rec/conf/icml/BallSKL23.bib},
+  bibsource    = {dblp computer science bibliography, https://dblp.org}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/rlpd/configs/base_rlpd_ms3.yml b/scripts/baselines/rlpd/configs/base_rlpd_ms3.yml
new file mode 100644
index 0000000..00339d3
--- /dev/null
+++ b/scripts/baselines/rlpd/configs/base_rlpd_ms3.yml
@@ -0,0 +1,75 @@
+jax_env: False
+
+seed: 0
+algo: sac
+verbose: 1
+# Environment configuration
+env:
+  env_id: None
+  max_episode_steps: 50
+  num_envs: 8
+  env_type: "gym:cpu"
+  env_kwargs:
+    control_mode: "pd_joint_delta_pos"
+    render_mode: "rgb_array"
+    reward_mode: "sparse"
+eval_env:
+  num_envs: 2
+  max_episode_steps: 50
+
+sac:
+  num_seed_steps: 5_000
+  seed_with_policy: False
+  replay_buffer_capacity: 1_000_000
+  batch_size: 256
+  steps_per_env: 4
+  grad_updates_per_step: 16
+  actor_update_freq: 1
+
+  num_qs: 2
+  num_min_qs: 2
+
+  discount: 0.9
+  tau: 0.005
+  backup_entropy: False
+
+  eval_freq: 50_000
+  eval_steps: 250
+
+  log_freq: 1000
+  save_freq: 50_000
+
+  learnable_temp: True
+  initial_temperature: 1.0
+  
+network:
+  actor:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+  critic:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+      use_layer_norm: True
+
+train:
+  actor_lr: 3e-4
+  critic_lr: 3e-4
+  steps: 100_000_000
+  dataset_path: None
+  shuffle_demos: True
+  num_demos: 1000
+
+  data_action_scale: null
+
+logger:
+  tensorboard: True
+  wandb: False
+
+  workspace: "exps"
+  project_name: "ManiSkill"
+  wandb_cfg:
+    group: "RLPD"
diff --git a/scripts/baselines/rlpd/configs/base_rlpd_ms3_sample_efficient.yml b/scripts/baselines/rlpd/configs/base_rlpd_ms3_sample_efficient.yml
new file mode 100644
index 0000000..bf30baa
--- /dev/null
+++ b/scripts/baselines/rlpd/configs/base_rlpd_ms3_sample_efficient.yml
@@ -0,0 +1,75 @@
+jax_env: False
+
+seed: 0
+algo: sac
+verbose: 1
+# Environment configuration
+env:
+  env_id: None
+  max_episode_steps: 50
+  num_envs: 1
+  env_type: "gym:cpu"
+  env_kwargs:
+    control_mode: "pd_joint_delta_pos"
+    render_mode: "rgb_array"
+    reward_mode: "sparse"
+eval_env:
+  num_envs: 2
+  max_episode_steps: 50
+
+sac:
+  num_seed_steps: 5_000
+  seed_with_policy: False
+  replay_buffer_capacity: 1_000_000
+  batch_size: 256
+  steps_per_env: 1
+  grad_updates_per_step: 16
+  actor_update_freq: 1
+
+  num_qs: 10
+  num_min_qs: 2
+
+  discount: 0.9
+  tau: 0.005
+  backup_entropy: False
+
+  eval_freq: 5_000
+  eval_steps: 250
+
+  log_freq: 1000
+  save_freq: 5_000
+
+  learnable_temp: True
+  initial_temperature: 1.0
+  
+network:
+  actor:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+  critic:
+    type: "mlp"
+    arch_cfg:
+      features: [256, 256, 256]
+      output_activation: "relu"
+      use_layer_norm: True
+
+train:
+  actor_lr: 3e-4
+  critic_lr: 3e-4
+  steps: 100_000_000
+  dataset_path: None
+  shuffle_demos: True
+  num_demos: 1000
+
+  data_action_scale: null
+
+logger:
+  tensorboard: True
+  wandb: False
+
+  workspace: "exps"
+  project_name: "ManiSkill"
+  wandb_cfg:
+    group: "RLPD"
diff --git a/scripts/baselines/rlpd/train_ms3.py b/scripts/baselines/rlpd/train_ms3.py
new file mode 100644
index 0000000..aed85f5
--- /dev/null
+++ b/scripts/baselines/rlpd/train_ms3.py
@@ -0,0 +1,207 @@
+"""
+Code to run Reverse Forward Curriculum Learning.
+Configs can be a bit complicated, we recommend directly looking at configs/ms2/base_sac_ms2_sample_efficient.yml for what options are available.
+Alternatively, go to the file defining each of the nested configurations and see the comments.
+"""
+import copy
+import os
+import os.path as osp
+import sys
+import warnings
+from dataclasses import asdict, dataclass
+from typing import Optional
+
+import jax
+import numpy as np
+import optax
+from omegaconf import OmegaConf
+
+from rfcl.agents.sac import SAC, ActorCritic, SACConfig
+from rfcl.agents.sac.networks import DiagGaussianActor
+from rfcl.data.dataset import ReplayDataset
+from rfcl.envs.make_env import EnvConfig, make_env_from_cfg
+from rfcl.logger import LoggerConfig
+from rfcl.models import NetworkConfig, build_network_from_cfg
+from rfcl.utils.parse import parse_cfg
+from rfcl.utils.spaces import get_action_dim
+
+
+os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
+
+
+@dataclass
+class TrainConfig:
+    steps: int
+    actor_lr: float
+    critic_lr: float
+    dataset_path: str
+    shuffle_demos: bool
+    num_demos: int
+
+    data_action_scale: Optional[float]
+
+@dataclass
+class SACNetworkConfig:
+    actor: NetworkConfig
+    critic: NetworkConfig
+
+
+@dataclass
+class SACExperiment:
+    seed: int
+    sac: SACConfig
+    env: EnvConfig
+    eval_env: EnvConfig
+    train: TrainConfig
+    network: SACNetworkConfig
+    logger: Optional[LoggerConfig]
+    verbose: int
+    algo: str = "sac"
+    save_eval_video: bool = True  # whether to save eval videos
+    demo_seed: int = None  # fix a seed to fix which demonstrations are sampled from a dataset
+
+    """additional tags/configs for logging purposes to wandb and shared comparisons with other algorithms"""
+    demo_type: str = None
+    config_type: str = None # "sample_efficient" or "walltime_efficient"
+
+
+from dacite import from_dict
+
+
+def main(cfg: SACExperiment):
+    np.random.seed(cfg.seed)
+
+    ### Setup the experiment parameters ###
+
+    # Setup training and evaluation environment configs
+    env_cfg = cfg.env
+    if "env_kwargs" not in env_cfg:
+        env_cfg["env_kwargs"] = dict()
+    cfg.eval_env = {**env_cfg, **cfg.eval_env}
+    cfg = from_dict(data_class=SACExperiment, data=OmegaConf.to_container(cfg))
+    env_cfg = cfg.env
+    eval_env_cfg = cfg.eval_env
+
+    # change exp name if it exists
+    orig_exp_name = cfg.logger.exp_name
+    exp_path = osp.join(cfg.logger.workspace, orig_exp_name)
+    if osp.exists(exp_path):
+        i = 1
+        prev_exp_path = exp_path
+        while osp.exists(exp_path):
+            prev_exp_path = exp_path
+            cfg.logger.exp_name = f"{orig_exp_name}_{i}"
+            exp_path = osp.join(cfg.logger.workspace, cfg.logger.exp_name)
+            i += 1
+        warnings.warn(f"{prev_exp_path} already exists. Changing exp_name to {cfg.logger.exp_name}")
+    video_path = osp.join(cfg.logger.workspace, cfg.logger.exp_name, "videos")
+
+    cfg.sac.num_envs = cfg.env.num_envs
+    cfg.sac.num_eval_envs = cfg.eval_env.num_envs
+
+    ### Create Environments ###
+    if cfg.demo_seed is not None:
+        np.random.seed(cfg.demo_seed)
+
+    if "reward_mode" in cfg.env.env_kwargs:
+        reward_mode = cfg.env.env_kwargs["reward_mode"]
+    elif "reward_type" in cfg.env.env_kwargs:
+        reward_mode = cfg.env.env_kwargs["reward_type"]
+    else:
+        raise ValueError("reward_mode is not specified")
+
+    demo_replay_dataset = ReplayDataset(
+        cfg.train.dataset_path,
+        shuffle=cfg.train.shuffle_demos,
+        skip_failed=False,
+        num_demos=cfg.train.num_demos,
+        reward_mode=reward_mode,
+        data_action_scale=cfg.train.data_action_scale,
+    )
+    if demo_replay_dataset.action_scale is not None:
+        env_cfg.action_scale = demo_replay_dataset.action_scale.tolist()
+        eval_env_cfg.action_scale = env_cfg.action_scale
+    np.random.seed(cfg.seed)
+
+    env, env_meta = make_env_from_cfg(env_cfg, seed=cfg.seed)
+    eval_env = None
+    if cfg.sac.num_eval_envs > 0:
+        eval_env, _ = make_env_from_cfg(
+            eval_env_cfg,
+            seed=cfg.seed + 1_000_000,
+            video_path=video_path if cfg.save_eval_video else None,
+        )
+
+    sample_obs, sample_acts = env_meta.sample_obs, env_meta.sample_acts
+
+    # create actor and critics models
+    act_dims = get_action_dim(env_meta.act_space)
+
+    def create_ac_model():
+        actor = DiagGaussianActor(
+            feature_extractor=build_network_from_cfg(cfg.network.actor),
+            act_dims=act_dims,
+            state_dependent_std=True,
+        )
+        ac = ActorCritic.create(
+            jax.random.PRNGKey(cfg.seed),
+            actor=actor,
+            critic_feature_extractor=build_network_from_cfg(cfg.network.critic),
+            sample_obs=sample_obs,
+            sample_acts=sample_acts,
+            initial_temperature=cfg.sac.initial_temperature,
+            actor_optim=optax.adam(learning_rate=cfg.train.actor_lr),
+            critic_optim=optax.adam(learning_rate=cfg.train.critic_lr),
+        )
+        return ac
+
+    # create our algorithm
+    ac = create_ac_model()
+    cfg.logger.cfg = asdict(cfg)
+    logger_cfg = cfg.logger
+    algo = SAC(
+        env=env,
+        eval_env=eval_env,
+        env_type=cfg.env.env_type,
+        ac=ac,
+        logger_cfg=logger_cfg,
+        cfg=cfg.sac,
+    )
+
+    # for ManiSkill 3 baselines, try to modify the wandb config to match other baselines env_cfg setups.
+    if algo.logger.wandb:
+        import wandb as wb
+        sim_backend = "cpu"
+        if cfg.env.env_type == "gym:cpu":
+            sim_backend = "cpu"
+        elif cfg.env.env_type == "gym:gpu":
+            sim_backend = "gpu"
+        def parse_env_cfg(env_cfg):
+            return {
+                "env_id": cfg.env.env_id,
+                "env_kwargs": cfg.env.env_kwargs,
+                "num_envs": cfg.env.num_envs,
+                "env_horizon": cfg.env.max_episode_steps,
+                "sim_backend": sim_backend,
+                "reward_mode": cfg.env.env_kwargs.get("reward_mode"),
+                "obs_mode": cfg.env.env_kwargs.get("obs_mode"),
+                "control_mode": cfg.env.env_kwargs.get("control_mode"),
+            }
+        fixed_wb_cfgs = {"env_cfg": parse_env_cfg(env_cfg), "eval_env_cfg": parse_env_cfg(eval_env_cfg), "num_demos": cfg.train.num_demos, "demo_type": cfg.demo_type}
+        wb.config.update({**fixed_wb_cfgs}, allow_val_change=True)
+        algo.logger.wandb_run.tags = ["rlpd", cfg.config_type]
+    algo.offline_buffer = demo_replay_dataset  # create offline buffer to oversample from
+    rng_key, train_rng_key = jax.random.split(jax.random.PRNGKey(cfg.seed), 2)
+    algo.train(
+        rng_key=train_rng_key,
+        steps=cfg.train.steps,
+        verbose=cfg.verbose,
+    )
+    algo.save(osp.join(algo.logger.model_path, "latest.jx"), with_buffer=False)
+    # with_buffer=True means you can use the checkpoint to easily resume training with the same replay buffer data
+    # algo.save(osp.join(algo.logger.model_path, "latest.jx"), with_buffer=True)
+    env.close(), eval_env.close()
+
+if __name__ == "__main__":
+    cfg = parse_cfg(default_cfg_path=sys.argv[1])
+    main(cfg)
diff --git a/scripts/baselines/sac/.gitignore b/scripts/baselines/sac/.gitignore
new file mode 100644
index 0000000..7a7b787
--- /dev/null
+++ b/scripts/baselines/sac/.gitignore
@@ -0,0 +1,2 @@
+runs
+videos
\ No newline at end of file
diff --git a/scripts/baselines/sac/README.md b/scripts/baselines/sac/README.md
new file mode 100644
index 0000000..69ff563
--- /dev/null
+++ b/scripts/baselines/sac/README.md
@@ -0,0 +1,47 @@
+# Soft Actor Critic (SAC)
+
+Code for running the PPO RL algorithm is adapted from [CleanRL](https://github.com/vwxyzjn/cleanrl/) and our previous [ManiSkill Baselines](https://github.com/tongzhoumu/ManiSkill_Baselines/). It is written to be single-file and easy to follow/read, and supports state-based RL code.
+
+Note that ManiSkill is still in beta, so we have not finalized training scripts for every pre-built task (some of which are simply too hard to solve with RL anyway). We will further organize these scripts and results in a more organized manner in the future.
+
+## State Based RL
+
+Below is a sample of various commands you can run to train a state-based policy to solve various tasks with SAC that are lightly tuned already. The fastest one is the PushCube-v1 task which can take about 5 minutes to train on a GPU.
+
+
+```bash
+python sac.py --env_id="PushCube-v1" \
+  --num_envs=32 --utd=0.5 --buffer_size=200_000 \
+  --total_timesteps=200_000 --eval_freq=50_000
+python sac.py --env_id="PickCube-v1" \
+  --num_envs=32 --utd=0.5 --buffer_size=1_000_000 \
+  --total_timesteps=1_000_000 --eval_freq=50_000
+```
+
+## Citation
+
+If you use this baseline please cite the following
+```
+@inproceedings{DBLP:conf/icml/HaarnojaZAL18,
+  author       = {Tuomas Haarnoja and
+                  Aurick Zhou and
+                  Pieter Abbeel and
+                  Sergey Levine},
+  editor       = {Jennifer G. Dy and
+                  Andreas Krause},
+  title        = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
+                  with a Stochastic Actor},
+  booktitle    = {Proceedings of the 35th International Conference on Machine Learning,
+                  {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July
+                  10-15, 2018},
+  series       = {Proceedings of Machine Learning Research},
+  volume       = {80},
+  pages        = {1856--1865},
+  publisher    = {{PMLR}},
+  year         = {2018},
+  url          = {http://proceedings.mlr.press/v80/haarnoja18b.html},
+  timestamp    = {Wed, 03 Apr 2019 18:17:30 +0200},
+  biburl       = {https://dblp.org/rec/conf/icml/HaarnojaZAL18.bib},
+  bibsource    = {dblp computer science bibliography, https://dblp.org}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/sac/process.py b/scripts/baselines/sac/process.py
new file mode 100644
index 0000000..15f6aea
--- /dev/null
+++ b/scripts/baselines/sac/process.py
@@ -0,0 +1,25 @@
+import numpy as np
+from PIL import Image
+import torch
+from collections import deque
+import matplotlib.pyplot as plt
+from torchvision.transforms.v2 import Grayscale, Resize
+
+def image_preprocess(image, resize_shape:tuple=(200,200), grayscale:bool=True, resize:bool=True):
+    gs = Grayscale()
+    rs = Resize(resize_shape)
+    image = torch.permute(image, (0, 3,1,2))
+    if grayscale: image=gs(image)
+    if resize: image=rs(image)
+    image = image/255
+    return image
+
+def sound_preprocess(sound_tuples):
+    preprocessed_sound_obs = []
+    for sound_tuple in sound_tuples:
+        frequency, amplitude = sound_tuple
+        preprocessed_frequency = frequency / 500.0 #441.0  # Normalize frequency
+        preprocessed_amplitude = amplitude / 1 #255.0  # Normalize amplitude
+        preprocessed_sound_obs.extend([preprocessed_frequency, preprocessed_amplitude])
+    sound = np.array(preprocessed_sound_obs)
+    return sound.tolist()
\ No newline at end of file
diff --git a/scripts/baselines/sac/sac.py b/scripts/baselines/sac/sac.py
new file mode 100644
index 0000000..a4ceefe
--- /dev/null
+++ b/scripts/baselines/sac/sac.py
@@ -0,0 +1,391 @@
+import copy
+import os
+import random
+import time
+
+from mani_skill.utils.wrappers.flatten import FlattenActionSpaceWrapper
+from mani_skill.utils.wrappers.record import RecordEpisode
+from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
+
+import gymnasium as gym
+from gymnasium import spaces
+import numpy as np
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+import torch.optim as optim
+from torch.utils.tensorboard import SummaryWriter
+import tyro
+#from models import SoftQNetwork, Actor
+from models import ActorMultimodal as Actor
+from models import SoftQNetworkMultimodal as SoftQNetwork
+
+import mani_skill.envs
+
+from utils import Args, ReplayBufferSample, get_distance
+from utils import ReplayBufferMultimodal as ReplayBuffer
+from enviroment import EnvMultimodalWrapper
+from process import image_preprocess
+from collections import deque
+
+
+
+def train(**kwargs):
+    '''
+    Needs:
+     + models: (actor, qf1_target, qf2_target)
+     + ReplayBuffer
+    Returns:
+
+    '''
+    envs.reset_mm(seed=args.seed)
+    eval_envs.reset_mm(seed=args.seed)
+
+    global_step = 0
+    global_update = 0
+    learning_has_started = False
+    alpha = kwargs['alpha']
+    global_steps_per_iteration = args.num_envs * (args.steps_per_env)
+
+    ## GLOBAL LOOP
+    while global_step < args.total_timesteps:
+        print(f"Global Step: {global_step}", end='\r')
+
+        ## EVALUATION and SAVING
+        if args.eval_freq > 0 and (global_step - args.training_freq) // args.eval_freq < global_step // args.eval_freq:
+            # evaluate
+            actor.eval()
+            print("Evaluating")
+            (old_eval_img, eval_obs), _ = eval_envs.reset_mm(seed=args.seed)
+            old_action = torch.zeros(eval_envs.action_space.shape)
+            (eval_img, eval_obs), _, _, _, _ = eval_envs.step_mm(eval_envs.action_space.sample())
+
+            z_img, z_obs = actor.get_encodings((
+                torch.cat([image_preprocess(old_eval_img), image_preprocess(eval_img)], dim=1).float(),
+                eval_obs
+            ))
+            z_old = (z_img+z_obs)/2
+
+            returns = []
+            eps_lens = []
+            successes = []
+            failures = []
+            for _ in range(args.num_eval_steps):
+                with torch.no_grad():
+                    state = (torch.cat([image_preprocess(old_eval_img), image_preprocess(eval_img)], dim=1).float(), eval_obs)
+                    action, z_old = actor.get_eval_action(state, z_old, old_action.to(device))
+                    (next_eval_img, eval_obs), _, eval_terminations, eval_truncations, eval_infos = eval_envs.step_mm(action)
+                    old_eval_img = eval_img.clone()
+                    eval_img = next_eval_img.clone()
+                    old_action = action.clone()
+
+                    if "final_info" in eval_infos:
+                        mask = eval_infos["_final_info"]
+                        eps_lens.append(eval_infos["final_info"]["elapsed_steps"][mask].cpu().numpy())
+                        returns.append(eval_infos["final_info"]["episode"]["r"][mask].cpu().numpy())
+                        if "success" in eval_infos:
+                            successes.append(eval_infos["final_info"]["success"][mask].cpu().numpy())
+                        if "fail" in eval_infos:
+                            failures.append(eval_infos["final_info"]["fail"][mask].cpu().numpy())
+            returns = np.concatenate(returns)
+            eps_lens = np.concatenate(eps_lens)
+            print(f"Evaluated {args.num_eval_steps * args.num_eval_envs} steps resulting in {len(eps_lens)} episodes")
+            if len(successes) > 0:
+                successes = np.concatenate(successes)
+                if writer is not None: writer.add_scalar("charts/eval_success_rate", successes.mean(), global_step)
+                print(f"eval_success_rate={successes.mean()}")
+            if len(failures) > 0:
+                failures = np.concatenate(failures)
+                if writer is not None: writer.add_scalar("charts/eval_fail_rate", failures.mean(), global_step)
+                print(f"eval_fail_rate={failures.mean()}")
+
+            print(f"eval_episodic_return={returns.mean()}")
+            if writer is not None:
+                writer.add_scalar("charts/eval_episodic_return", returns.mean(), global_step)
+                writer.add_scalar("charts/eval_episodic_length", eps_lens.mean(), global_step)
+            actor.train()
+            if args.evaluate:
+                break
+
+            if args.save_model:
+                model_path = f"runs/{run_name}/ckpt_{global_step}.pt"
+                torch.save({
+                    'actor': actor.state_dict(),
+                    'qf1': qf1_target.state_dict(),
+                    'qf2': qf2_target.state_dict(),
+                    'log_alpha': log_alpha,
+                }, model_path)
+                print(f"model saved to {model_path}")
+
+
+
+        ## ROLLOUT (try not to modify: CRUCIAL step easy to overlook)
+        rollout_time = time.time()
+        (old_img, obs), info = envs.reset_mm(seed=args.seed)
+        (img, obs), _, _, _, _ = envs.step_mm(envs.action_space.sample())
+
+        for local_step in range(args.steps_per_env):
+            global_step += 1 * args.num_envs
+
+            img_stack = torch.cat([image_preprocess(old_img), image_preprocess(img)], dim=1)
+
+            if not learning_has_started:
+                actions = torch.tensor(envs.action_space.sample(), dtype=torch.float32, device=device)
+            else:
+                state = (img_stack, obs)
+                actions, _, _ = actor.get_train_action(state)
+                actions = actions.detach()
+
+            # TRY NOT TO MODIFY: execute the game and log data.
+            (next_img, next_obs), rewards, terminations, truncations, infos = envs.step_mm(actions)
+            real_next_obs = next_obs.clone()
+            next_img_stack = torch.cat([image_preprocess(img), image_preprocess(next_img)], dim=1)
+
+            if args.bootstrap_at_done == 'always':
+                next_done = torch.zeros_like(terminations).to(torch.float32)
+            else:
+                next_done = (terminations | truncations).to(torch.float32)
+            if "final_info" in infos:
+                final_info = infos["final_info"]
+                done_mask = infos["_final_info"]
+                real_next_obs[done_mask] = infos["final_observation"][done_mask]
+                episodic_return = final_info['episode']['r'][done_mask].cpu().numpy().mean()
+                if "success" in final_info:
+                    writer.add_scalar("charts/success_rate", final_info["success"][done_mask].cpu().numpy().mean(),
+                                      global_step)
+                if "fail" in final_info:
+                    writer.add_scalar("charts/fail_rate", final_info["fail"][done_mask].cpu().numpy().mean(),
+                                      global_step)
+                writer.add_scalar("charts/episodic_return", episodic_return, global_step)
+                writer.add_scalar("charts/episodic_length", final_info["elapsed_steps"][done_mask].cpu().numpy().mean(),
+                                  global_step)
+
+            rb.add(obs, img_stack, real_next_obs, next_img_stack, actions, rewards, next_done)
+
+            obs = next_obs
+            old_img = img.clone()
+            img = next_img.clone()
+
+        rollout_time = time.time() - rollout_time
+
+        ## UPDATING AGENT (ALGO LOGIC: training.)
+        if global_step < args.learning_starts:
+            continue
+
+        update_time = time.time()
+        learning_has_started = True
+        for local_update in range(args.grad_steps_per_iteration):
+            global_update += 1
+            data = rb.sample(args.batch_size)
+
+            # update the value networks
+            with torch.no_grad():
+                next_state_actions, next_state_log_pi, _ = actor.get_train_action((data.next_img,data.next_obs))
+                qf1_next_target = qf1_target(data.next_obs, next_state_actions)
+                qf2_next_target = qf2_target(data.next_obs, next_state_actions)
+                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
+                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
+                # data.dones is "stop_bootstrap", which is computed earlier according to args.bootstrap_at_done
+
+            qf1_a_values = qf1(data.obs, data.actions).view(-1)
+            qf2_a_values = qf2(data.obs, data.actions).view(-1)
+            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
+            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
+            qf_loss = qf1_loss + qf2_loss
+
+            q_optimizer.zero_grad()
+            qf_loss.backward()
+            q_optimizer.step()
+
+            # update the policy network
+            if global_update % args.policy_frequency == 0:  # TD 3 Delayed update support
+                pi, log_pi, _ = actor.get_train_action((data.img,data.obs))
+                qf1_pi = qf1(data.obs, pi)
+                qf2_pi = qf2(data.obs, pi)
+                min_qf_pi = torch.min(qf1_pi, qf2_pi)
+                actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
+
+                # update the encoders
+                (z_img_0, z_snd_0), z_0, (z_img_1, z_snd_1), z_1, z_act_1 = actor.get_representations(data)
+
+                MM_loss = (torch.mean((get_distance(z_img_0, z_snd_0))) + torch.mean((get_distance(z_img_1, z_snd_1))))
+                TF_loss = torch.mean(get_distance(z_act_1, z_1))
+                TC_loss = torch.mean((get_distance(z_0, z_1) - 1.0) ** 2)
+                rnd_idx = np.arange(z_0.shape[0])
+                np.random.shuffle(rnd_idx)
+                NTC_loss = - torch.mean(torch.log(get_distance(z_0, z_1[rnd_idx]) + 1e-6))
+                representation_loss = args.TC_coeff * TC_loss + args.NTC_coeff * NTC_loss + args.TF_coeff * TF_loss + args.MM_coeff * MM_loss
+
+                loss = actor_loss + representation_loss
+
+                actor_optimizer.zero_grad()
+                loss.backward()
+                actor_optimizer.step()
+
+                if args.autotune:
+                    with torch.no_grad():
+                        _, log_pi, _ = actor.get_train_action((data.img,data.obs))
+                    alpha_loss = (-log_alpha.exp() * (log_pi + target_entropy)).mean()
+                    a_optimizer.zero_grad()
+                    alpha_loss.backward()
+                    a_optimizer.step()
+                    alpha = log_alpha.exp().item()
+
+            # update the target networks
+            if global_update % args.target_network_frequency == 0:
+                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):
+                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
+                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
+                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
+        update_time = time.time() - update_time
+
+
+
+        ## LOGGING (Log training-related data)
+        if (global_step - args.training_freq) // args.log_freq < global_step // args.log_freq:
+            writer.add_scalar("losses/qf1_values", qf1_a_values.mean().item(), global_step)
+            writer.add_scalar("losses/qf2_values", qf2_a_values.mean().item(), global_step)
+            writer.add_scalar("losses/qf1_loss", qf1_loss.item(), global_step)
+            writer.add_scalar("losses/qf2_loss", qf2_loss.item(), global_step)
+            writer.add_scalar("losses/qf_loss", qf_loss.item() / 2.0, global_step)
+            writer.add_scalar("losses/actor_loss", actor_loss.item(), global_step)
+            writer.add_scalar("losses/representation_loss", representation_loss.item(), global_step)
+            writer.add_scalar("losses/multimodal_loss", MM_loss.item(), global_step)
+            writer.add_scalar("losses/transfer_loss", TF_loss.item(), global_step)
+            writer.add_scalar("losses/contrastive_positive_loss", TC_loss.item(), global_step)
+            writer.add_scalar("losses/contrastive_negative_loss", NTC_loss.item(), global_step)
+
+            writer.add_scalar("losses/alpha", alpha, global_step)
+            writer.add_scalar("charts/update_time", update_time, global_step)
+            writer.add_scalar("charts/rollout_time", rollout_time, global_step)
+            writer.add_scalar("charts/rollout_fps", global_steps_per_iteration / rollout_time, global_step)
+            if args.autotune:
+                writer.add_scalar("losses/alpha_loss", alpha_loss.item(), global_step)
+
+    return (actor, qf1_target, qf2_target), log_alpha
+
+
+
+if __name__ == "__main__":
+
+    ## CONFIGS
+    args = tyro.cli(Args)
+    args.grad_steps_per_iteration = int(args.training_freq * args.utd)
+    args.steps_per_env = args.training_freq // args.num_envs
+    if args.exp_name is None:
+        args.exp_name = os.path.basename(__file__)[: -len(".py")]
+        run_name = f"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}"
+    else:
+        run_name = args.exp_name
+
+    writer = None
+    if not args.evaluate:
+        print("Running training")
+        if args.track:
+            import wandb
+
+            wandb.init(
+                project=args.wandb_project_name,
+                entity=args.wandb_entity,
+                sync_tensorboard=True,
+                config=vars(args),
+                name=run_name,
+                monitor_gym=True,
+                save_code=True,
+            )
+        writer = SummaryWriter(f"runs/{run_name}")
+        writer.add_text(
+            "hyperparameters",
+            "|param|value|\n|-|-|\n%s" % ("\n".join([f"|{key}|{value}|" for key, value in vars(args).items()])),
+        )
+    else:
+        print("Running evaluation")
+
+    # TRY NOT TO MODIFY: seeding
+    random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+
+    device = torch.device("cuda" if torch.cuda.is_available() and args.cuda else "cpu")
+
+    ## ENVIRONMENTS SETUP
+    env_kwargs = dict(obs_mode="state", control_mode="pd_joint_delta_pos", render_mode="rgb_array", sim_backend="gpu")
+    envs = gym.make(args.env_id, num_envs=args.num_envs if not args.evaluate else 1, **env_kwargs)
+    eval_envs = gym.make(args.env_id, num_envs=args.num_eval_envs, **env_kwargs)
+    if isinstance(envs.action_space, gym.spaces.Dict):
+        envs = FlattenActionSpaceWrapper(envs)
+        eval_envs = FlattenActionSpaceWrapper(eval_envs)
+    if args.capture_video:
+        eval_output_dir = f"runs/{run_name}/videos"
+        if args.evaluate:
+            eval_output_dir = f"{os.path.dirname(args.checkpoint)}/test_videos"
+        print(f"Saving eval videos to {eval_output_dir}")
+        if args.save_train_video_freq is not None:
+            save_video_trigger = lambda x : (x // args.num_steps) % args.save_train_video_freq == 0
+            envs = RecordEpisode(envs, output_dir=f"runs/{run_name}/train_videos", save_trajectory=False, save_video_trigger=save_video_trigger, max_steps_per_video=args.num_steps, video_fps=30)
+        eval_envs = RecordEpisode(eval_envs, output_dir=eval_output_dir, save_trajectory=args.evaluate, trajectory_name="trajectory", max_steps_per_video=args.num_eval_steps, video_fps=30)
+    envs = ManiSkillVectorEnv(envs, args.num_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
+    eval_envs = ManiSkillVectorEnv(eval_envs, args.num_eval_envs, ignore_terminations=not args.partial_reset, **env_kwargs)
+
+    envs = EnvMultimodalWrapper(envs)
+    eval_envs = EnvMultimodalWrapper(eval_envs)
+
+
+    assert isinstance(envs.single_action_space, gym.spaces.Box), "only continuous action space is supported"
+
+
+    max_action = float(envs.single_action_space.high[0])
+
+    actor = Actor(envs).to(device)
+    qf1 = SoftQNetwork(envs).to(device)
+    qf2 = SoftQNetwork(envs).to(device)
+    qf1_target = SoftQNetwork(envs).to(device)
+    qf2_target = SoftQNetwork(envs).to(device)
+    if args.checkpoint is not None:
+        ckpt = torch.load(args.checkpoint)
+        actor.load_state_dict(ckpt['actor'])
+        qf1.load_state_dict(ckpt['qf1'])
+        qf2.load_state_dict(ckpt['qf2'])
+    qf1_target.load_state_dict(qf1.state_dict())
+    qf2_target.load_state_dict(qf2.state_dict())
+    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.q_lr)
+    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.policy_lr)
+
+    # Automatic entropy tuning
+    if args.autotune:
+        target_entropy = -torch.prod(torch.Tensor(envs.single_action_space.shape).to(device)).item()
+        log_alpha = torch.zeros(1, requires_grad=True, device=device)
+        alpha = log_alpha.exp().item()
+        a_optimizer = optim.Adam([log_alpha], lr=args.q_lr)
+    else:
+        alpha = args.alpha
+
+    ## REPLAY BUFFER setup
+    envs.single_observation_space_mm.dtype = np.float32
+    rb = ReplayBuffer(
+        env=envs,
+        num_envs=args.num_envs,
+        buffer_size=args.buffer_size,
+        storage_device=torch.device(args.buffer_device),
+        sample_device=torch.device(device)
+    )
+
+    train_args = {
+        'alpha':alpha
+    }
+
+    ## TRAINING
+    (actor, qf1_target, qf2_target), log_alpha = train(**train_args)
+
+    if not args.evaluate and args.save_model:
+        model_path = f"runs/{run_name}/final_ckpt.pt"
+        torch.save({
+            'actor': actor.state_dict(),
+            'qf1': qf1_target.state_dict(),
+            'qf2': qf2_target.state_dict(),
+            'log_alpha': log_alpha,
+        }, model_path)
+        print(f"model saved to {model_path}")
+        writer.close()
+    envs.close()
diff --git a/scripts/baselines/stable_baselines3/.gitignore b/scripts/baselines/stable_baselines3/.gitignore
new file mode 100644
index 0000000..d68cc74
--- /dev/null
+++ b/scripts/baselines/stable_baselines3/.gitignore
@@ -0,0 +1,2 @@
+/eval_videos
+/*.zip
\ No newline at end of file
diff --git a/scripts/baselines/stable_baselines3/README.md b/scripts/baselines/stable_baselines3/README.md
new file mode 100644
index 0000000..d08c4a9
--- /dev/null
+++ b/scripts/baselines/stable_baselines3/README.md
@@ -0,0 +1,18 @@
+# Stable Baselines 3
+
+The example.py code shows a very simple example of how to use Stable Baselines 3 with ManiSkill 3 via a simple wrapper. These are not tuned as much compared to the [recommended PPO baseline code](https://github.com/haosulab/ManiSkill/tree/main/examples/baselines/ppo) so we cannot guarantee good performance from using Stable Baselines 3. Moreover, currently Stable Baselines 3 is not optimized for GPU vectorized environments, so it will train a bit slower.
+
+If you use Stable Baselines 3 please cite the following 
+
+```
+@article{stable-baselines3,
+  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
+  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
+  journal = {Journal of Machine Learning Research},
+  year    = {2021},
+  volume  = {22},
+  number  = {268},
+  pages   = {1-8},
+  url     = {http://jmlr.org/papers/v22/20-1364.html}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/stable_baselines3/example.py b/scripts/baselines/stable_baselines3/example.py
new file mode 100644
index 0000000..a60ce76
--- /dev/null
+++ b/scripts/baselines/stable_baselines3/example.py
@@ -0,0 +1,33 @@
+from math import e
+import gymnasium as gym
+
+from stable_baselines3 import PPO
+from stable_baselines3.common.env_util import make_vec_env
+from sympy import det
+from mani_skill.utils import gym_utils
+from mani_skill.utils.wrappers.record import RecordEpisode
+
+from mani_skill.vector.wrappers.sb3 import ManiSkillSB3VectorEnv
+
+def main():
+    ms3_vec_env = gym.make("PushCube-v1", num_envs=64)
+    max_episode_steps = gym_utils.find_max_episode_steps_value(ms3_vec_env)
+    vec_env = ManiSkillSB3VectorEnv(ms3_vec_env)
+
+    model = PPO("MlpPolicy", vec_env, gamma=0.8, gae_lambda=0.9, n_steps=50, batch_size=128, n_epochs=8, verbose=1)
+    model.learn(total_timesteps=500_000)
+    model.save("ppo_pushcube")
+    vec_env.close()
+    del model # remove to demonstrate saving and loading
+
+    model = PPO.load("ppo_pushcube")
+
+    eval_vec_env = gym.make("PushCube-v1", num_envs=16, render_mode="rgb_array")
+    eval_vec_env = RecordEpisode(eval_vec_env, output_dir="eval_videos", save_video=True, trajectory_name="eval_trajectory", max_steps_per_video=max_episode_steps)
+    eval_vec_env = ManiSkillSB3VectorEnv(eval_vec_env)
+    obs = eval_vec_env.reset()
+    for i in range(max_episode_steps):
+        action, _states = model.predict(obs, deterministic=True)
+        obs, rewards, dones, info = eval_vec_env.step(action)
+if __name__ == "__main__":
+    main()
diff --git a/scripts/baselines/tdmpc2/README.md b/scripts/baselines/tdmpc2/README.md
new file mode 100644
index 0000000..ec76c9b
--- /dev/null
+++ b/scripts/baselines/tdmpc2/README.md
@@ -0,0 +1,76 @@
+# Temporal Difference Learning for Model Predictive Control 2 (TD-MPC2)
+
+Scalable, robust model-based RL algorithm based on ["TD-MPC2: Scalable, Robust World Models for Continuous Control"](https://arxiv.org/abs/2310.16828). Code adapted from https://github.com/nicklashansen/tdmpc2. It is written to work with the new Maniskill update, and supports vectorized state-based and visual-based RL environment.
+
+## Installation
+We recommend using conda/mamba and you can install the dependencies as so :
+
+```bash
+conda env create -f environment.yaml
+conda activate tdmpc2-ms
+```
+
+or follow the [original repo](https://github.com/nicklashansen/tdmpc2)'s guide to build the docker image.
+
+
+## State Based RL
+
+Simple command to run the algorithm with default configs (5M params, 1M steps, default control mode, 32 envs, state obs mode) :
+```bash
+python train.py env_id=PushCube-v1
+```
+
+More advanced command with optional configs :
+```bash
+python train.py model_size=5 steps=1_000_000 seed=1 exp_name=default \
+  env_id=PushCube-v1 num_envs=32 control_mode=pd_ee_delta_pose obs=state \
+  wandb=true wandb_entity=??? wandb_project=??? wandb_group=??? wandb_name=??? setting_tag=???
+```
+(*) The optional *setting_tag* is for adding a specific tag in the wandb log (e.g. sample_efficient, walltime_efficient, etc.)
+
+## Visual (RGB) Based RL
+
+The visual based RL expects model_size = 5. Also, make sure you have sufficient CPU memory, otherwise lower the buffer_size.
+```bash
+python train.py buffer_size=500_000 steps=5_000_000 seed=1 exp_name=default \
+  env_id=PushCube-v1 num_envs=32 control_mode=pd_ee_delta_pose obs=rgb \
+  wandb=true wandb_entity=??? wandb_project=??? wandb_group=??? wandb_name=??? setting_tag=???
+```
+
+## Replaying Evaluation Trajectories
+
+To create videos of a checkpoint model, use the following command.
+
+```bash
+python evaluate.py model_size=5 seed=1 exp_name=default \ 
+  env_id=PushCube-v1 control_mode=pd_ee_delta_pose obs=state \
+  checkpoint=/absolute/path/to/checkpoint.pt eval_episodes=10 
+```
+
+* Make sure you specify the same control_mode the model was trained on if it's not default.
+* The video are saved under ```logs/{env_id}/{seed}/{exp_name}/videos```
+
+## Some Notes
+
+- Multi-task TD-MPC2 isn't supported for Maniskill at the moment.
+
+## Citation
+
+If you use this baseline please cite the following
+```
+@inproceedings{hansen2024tdmpc2,
+  title={TD-MPC2: Scalable, Robust World Models for Continuous Control}, 
+  author={Nicklas Hansen and Hao Su and Xiaolong Wang},
+  booktitle={International Conference on Learning Representations (ICLR)},
+  year={2024}
+}
+```
+as well as the original TD-MPC paper:
+```
+@inproceedings{hansen2022tdmpc,
+  title={Temporal Difference Learning for Model Predictive Control},
+  author={Nicklas Hansen and Xiaolong Wang and Hao Su},
+  booktitle={International Conference on Machine Learning (ICML)},
+  year={2022}
+}
+```
\ No newline at end of file
diff --git a/scripts/baselines/tdmpc2/common/__init__.py b/scripts/baselines/tdmpc2/common/__init__.py
new file mode 100644
index 0000000..7fa5309
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/__init__.py
@@ -0,0 +1,60 @@
+MODEL_SIZE = { # parameters (M)
+	1:   {'enc_dim': 256,
+		  'mlp_dim': 384,
+		  'latent_dim': 128,
+		  'num_enc_layers': 2,
+		  'num_q': 2},
+	5:   {'enc_dim': 256,
+		  'mlp_dim': 512,
+		  'latent_dim': 512,
+		  'num_enc_layers': 2},
+	19:  {'enc_dim': 1024,
+		  'mlp_dim': 1024,
+		  'latent_dim': 768,
+		  'num_enc_layers': 3},
+	48:  {'enc_dim': 1792,
+		  'mlp_dim': 1792,
+		  'latent_dim': 768,
+		  'num_enc_layers': 4},
+	317: {'enc_dim': 4096,
+		  'mlp_dim': 4096,
+		  'latent_dim': 1376,
+		  'num_enc_layers': 5,
+		  'num_q': 8},
+}
+
+TASK_SET = {
+	'mt30': [
+		# 19 original dmcontrol tasks
+		'walker-stand', 'walker-walk', 'walker-run', 'cheetah-run', 'reacher-easy',
+	    'reacher-hard', 'acrobot-swingup', 'pendulum-swingup', 'cartpole-balance', 'cartpole-balance-sparse',
+		'cartpole-swingup', 'cartpole-swingup-sparse', 'cup-catch', 'finger-spin', 'finger-turn-easy',
+		'finger-turn-hard', 'fish-swim', 'hopper-stand', 'hopper-hop',
+		# 11 custom dmcontrol tasks
+		'walker-walk-backwards', 'walker-run-backwards', 'cheetah-run-backwards', 'cheetah-run-front', 'cheetah-run-back',
+		'cheetah-jump', 'hopper-hop-backwards', 'reacher-three-easy', 'reacher-three-hard', 'cup-spin',
+		'pendulum-spin',
+	],
+	'mt80': [
+		# 19 original dmcontrol tasks
+		'walker-stand', 'walker-walk', 'walker-run', 'cheetah-run', 'reacher-easy',
+	    'reacher-hard', 'acrobot-swingup', 'pendulum-swingup', 'cartpole-balance', 'cartpole-balance-sparse',
+		'cartpole-swingup', 'cartpole-swingup-sparse', 'cup-catch', 'finger-spin', 'finger-turn-easy',
+		'finger-turn-hard', 'fish-swim', 'hopper-stand', 'hopper-hop',
+		# 11 custom dmcontrol tasks
+		'walker-walk-backwards', 'walker-run-backwards', 'cheetah-run-backwards', 'cheetah-run-front', 'cheetah-run-back',
+		'cheetah-jump', 'hopper-hop-backwards', 'reacher-three-easy', 'reacher-three-hard', 'cup-spin',
+		'pendulum-spin',
+		# meta-world mt50
+		'mw-assembly', 'mw-basketball', 'mw-button-press-topdown', 'mw-button-press-topdown-wall', 'mw-button-press',
+		'mw-button-press-wall', 'mw-coffee-button', 'mw-coffee-pull', 'mw-coffee-push', 'mw-dial-turn',
+		'mw-disassemble', 'mw-door-open', 'mw-door-close', 'mw-drawer-close', 'mw-drawer-open',
+		'mw-faucet-open', 'mw-faucet-close', 'mw-hammer', 'mw-handle-press-side', 'mw-handle-press',
+		'mw-handle-pull-side', 'mw-handle-pull', 'mw-lever-pull', 'mw-peg-insert-side', 'mw-peg-unplug-side',
+		'mw-pick-out-of-hole', 'mw-pick-place', 'mw-pick-place-wall', 'mw-plate-slide', 'mw-plate-slide-side',
+		'mw-plate-slide-back', 'mw-plate-slide-back-side', 'mw-push-back', 'mw-push', 'mw-push-wall',
+		'mw-reach', 'mw-reach-wall', 'mw-shelf-place', 'mw-soccer', 'mw-stick-push',
+		'mw-stick-pull', 'mw-sweep-into', 'mw-sweep', 'mw-window-open', 'mw-window-close',
+		'mw-bin-picking', 'mw-box-close', 'mw-door-lock', 'mw-door-unlock', 'mw-hand-insert',
+	],
+}
diff --git a/scripts/baselines/tdmpc2/common/buffer.py b/scripts/baselines/tdmpc2/common/buffer.py
new file mode 100644
index 0000000..5a1510c
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/buffer.py
@@ -0,0 +1,101 @@
+import torch
+from tensordict.tensordict import TensorDict
+from torchrl.data.replay_buffers import ReplayBuffer, LazyTensorStorage
+from torchrl.data.replay_buffers.samplers import SliceSampler
+
+
+class Buffer():
+	"""
+	Replay buffer for TD-MPC2 training. Based on torchrl.
+	Uses CUDA memory if available, and CPU memory otherwise.
+	"""
+
+	def __init__(self, cfg):
+		self.cfg = cfg
+		self._device = torch.device('cuda')
+		self._capacity = min(cfg.buffer_size, cfg.steps)
+		self._sampler = SliceSampler(
+			num_slices=self.cfg.batch_size,
+			end_key=None,
+			traj_key='episode',
+			truncated_key=None,
+			strict_length=True,
+		)
+		self._batch_size = cfg.batch_size * (cfg.horizon+1)
+		self._num_eps = 0
+
+	@property
+	def capacity(self):
+		"""Return the capacity of the buffer."""
+		return self._capacity
+	
+	@property
+	def num_eps(self):
+		"""Return the number of episodes in the buffer."""
+		return self._num_eps
+
+	def _reserve_buffer(self, storage):
+		"""
+		Reserve a buffer with the given storage.
+		"""
+		return ReplayBuffer(
+			storage=storage,
+			sampler=self._sampler,
+			pin_memory=True,
+			prefetch=int(self.cfg.num_envs / self.cfg.steps_per_update),
+			batch_size=self._batch_size,
+		)
+
+	def _init(self, tds):
+		"""Initialize the replay buffer. Use the first episode to estimate storage requirements."""
+		print(f'Buffer capacity: {self._capacity:,}')
+		mem_free, _ = torch.cuda.mem_get_info()
+		bytes_per_step = sum([
+				(v.numel()*v.element_size() if not isinstance(v, TensorDict) \
+				else sum([x.numel()*x.element_size() for x in v.values()])) \
+			for v in tds.values()
+		]) / len(tds)
+		total_bytes = bytes_per_step*self._capacity
+		print(f'Storage required: {total_bytes/1e9:.2f} GB')
+		# Heuristic: decide whether to use CUDA or CPU memory
+		storage_device = 'cuda' if 2.5*total_bytes < mem_free else 'cpu'
+		print(f'Using {storage_device.upper()} memory for storage.')
+		return self._reserve_buffer(
+			LazyTensorStorage(self._capacity, device=torch.device(storage_device))
+		)
+
+	def _to_device(self, *args, device=None):
+		if device is None:
+			device = self._device
+		return (arg.to(device, non_blocking=True) \
+			if arg is not None else None for arg in args)
+
+	def _prepare_batch(self, td):
+		"""
+		Prepare a sampled batch for training (post-processing).
+		Expects `td` to be a TensorDict with batch size TxB.
+		"""
+		obs = td['obs']
+		action = td['action'][1:]
+		reward = td['reward'][1:].unsqueeze(-1)
+		task = td['task'][0] if 'task' in td.keys() else None
+		return self._to_device(obs, action, reward, task)
+
+	def add(self, td):
+		"""Add an episode to the buffer. 
+		Before vec: td[episode_len+1, ..] ..=act_dim, obs_dim, None
+		After: add num_env to the batch dimension
+		Note: for official vec code @51d6b8d, it seems to have batch dimension [episode_len+1, num_env]"""
+
+		for _td in td:
+			_td['episode'] = torch.ones_like(_td['reward'], dtype=torch.int64) * self._num_eps
+			if self._num_eps == 0:
+				self._buffer = self._init(_td)
+			self._buffer.extend(_td)
+			self._num_eps += 1
+		return self._num_eps
+
+	def sample(self):
+		"""Sample a batch of subsequences from the buffer."""
+		td = self._buffer.sample().view(-1, self.cfg.horizon+1).permute(1, 0)
+		return self._prepare_batch(td)
diff --git a/scripts/baselines/tdmpc2/common/init.py b/scripts/baselines/tdmpc2/common/init.py
new file mode 100644
index 0000000..45a3f5e
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/init.py
@@ -0,0 +1,22 @@
+import torch.nn as nn
+
+
+def weight_init(m):
+	"""Custom weight initialization for TD-MPC2."""
+	if isinstance(m, nn.Linear):
+		nn.init.trunc_normal_(m.weight, std=0.02)
+		if m.bias is not None:
+			nn.init.constant_(m.bias, 0)
+	elif isinstance(m, nn.Embedding):
+		nn.init.uniform_(m.weight, -0.02, 0.02)
+	elif isinstance(m, nn.ParameterList):
+		for i,p in enumerate(m):
+			if p.dim() == 3: # Linear
+				nn.init.trunc_normal_(p, std=0.02) # Weight
+				nn.init.constant_(m[i+1], 0) # Bias
+
+
+def zero_(params):
+	"""Initialize parameters to zero."""
+	for p in params:
+		p.data.fill_(0)
diff --git a/scripts/baselines/tdmpc2/common/layers.py b/scripts/baselines/tdmpc2/common/layers.py
new file mode 100644
index 0000000..568396a
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/layers.py
@@ -0,0 +1,153 @@
+import torch
+import torch.nn as nn
+import torch.nn.functional as F
+from functorch import combine_state_for_ensemble
+
+
+class Ensemble(nn.Module):
+	"""
+	Vectorized ensemble of modules. It seems adding 1 extra dimens
+	"""
+
+	def __init__(self, modules, **kwargs):
+		super().__init__()
+		modules = nn.ModuleList(modules)
+		fn, params, _ = combine_state_for_ensemble(modules)
+		self.vmap = torch.vmap(fn, in_dims=(0, 0, None), randomness='different', **kwargs)
+		self.params = nn.ParameterList([nn.Parameter(p) for p in params])
+		self._repr = str(modules)
+
+	def forward(self, *args, **kwargs):
+		return self.vmap([p for p in self.params], (), *args, **kwargs)
+
+	def __repr__(self):
+		return 'Vectorized ' + self._repr
+
+
+class ShiftAug(nn.Module):
+	"""
+	Random shift image augmentation. (N, C, H, W) as input.
+	Adapted from https://github.com/facebookresearch/drqv2
+	"""
+	def __init__(self, pad=3):
+		super().__init__()
+		self.pad = pad
+
+	def forward(self, x):
+		x = x.float()
+		n, _, h, w = x.size()
+		assert h == w
+		padding = tuple([self.pad] * 4)
+		x = F.pad(x, padding, 'replicate')
+		eps = 1.0 / (h + 2 * self.pad)
+		arange = torch.linspace(-1.0 + eps, 1.0 - eps, h + 2 * self.pad, device=x.device, dtype=x.dtype)[:h]
+		arange = arange.unsqueeze(0).repeat(h, 1).unsqueeze(2)
+		base_grid = torch.cat([arange, arange.transpose(1, 0)], dim=2)
+		base_grid = base_grid.unsqueeze(0).repeat(n, 1, 1, 1)
+		shift = torch.randint(0, 2 * self.pad + 1, size=(n, 1, 1, 2), device=x.device, dtype=x.dtype)
+		shift *= 2.0 / (h + 2 * self.pad)
+		grid = base_grid + shift
+		return F.grid_sample(x, grid, padding_mode='zeros', align_corners=False)
+
+
+class PixelPreprocess(nn.Module):
+	"""
+	Normalizes pixel observations to [-0.5, 0.5]. Always work.
+	"""
+
+	def __init__(self):
+		super().__init__()
+
+	def forward(self, x):
+		return x.div_(255.).sub_(0.5)
+
+
+class SimNorm(nn.Module):
+	"""
+	Simplicial normalization. Same shape, don't care batch.
+	Adapted from https://arxiv.org/abs/2204.00616.
+	"""
+	
+	def __init__(self, cfg):
+		super().__init__()
+		self.dim = cfg.simnorm_dim
+	
+	def forward(self, x):
+		shp = x.shape
+		x = x.view(*shp[:-1], -1, self.dim)
+		x = F.softmax(x, dim=-1)
+		return x.view(*shp)
+		
+	def __repr__(self):
+		return f"SimNorm(dim={self.dim})"
+
+
+class NormedLinear(nn.Linear):
+	"""
+	Linear layer with LayerNorm, activation, and optionally dropout. Don't care batch
+	"""
+
+	def __init__(self, *args, dropout=0., act=nn.Mish(inplace=True), **kwargs):
+		super().__init__(*args, **kwargs)
+		self.ln = nn.LayerNorm(self.out_features)
+		self.act = act
+		self.dropout = nn.Dropout(dropout, inplace=True) if dropout else None
+
+	def forward(self, x):
+		x = super().forward(x)
+		if self.dropout:
+			x = self.dropout(x)
+		return self.act(self.ln(x))
+	
+	def __repr__(self):
+		repr_dropout = f", dropout={self.dropout.p}" if self.dropout else ""
+		return f"NormedLinear(in_features={self.in_features}, "\
+			f"out_features={self.out_features}, "\
+			f"bias={self.bias is not None}{repr_dropout}, "\
+			f"act={self.act.__class__.__name__})"
+
+
+def mlp(in_dim, mlp_dims, out_dim, act=None, dropout=0.):
+	"""
+	Basic building block of TD-MPC2.
+	MLP with LayerNorm, Mish activations, and optionally dropout.
+	"""
+	if isinstance(mlp_dims, int):
+		mlp_dims = [mlp_dims]
+	dims = [in_dim] + mlp_dims + [out_dim]
+	mlp = nn.ModuleList()
+	for i in range(len(dims) - 2):
+		mlp.append(NormedLinear(dims[i], dims[i+1], dropout=dropout*(i==0)))
+	mlp.append(NormedLinear(dims[-2], dims[-1], act=act) if act else nn.Linear(dims[-2], dims[-1]))
+	return nn.Sequential(*mlp)
+
+
+def conv(in_shape, num_channels, act=None):
+	"""
+	Basic convolutional encoder for TD-MPC2 with raw image observations.
+	4 layers of convolution with ReLU activations, followed by a linear layer.
+	"""
+	assert in_shape[-1] == 64 # assumes rgb observations to be 64x64
+	layers = [
+		ShiftAug(), PixelPreprocess(),
+		nn.Conv2d(in_shape[0], num_channels, 7, stride=2), nn.ReLU(inplace=True),
+		nn.Conv2d(num_channels, num_channels, 5, stride=2), nn.ReLU(inplace=True),
+		nn.Conv2d(num_channels, num_channels, 3, stride=2), nn.ReLU(inplace=True),
+		nn.Conv2d(num_channels, num_channels, 3, stride=1), nn.Flatten()]
+	if act:
+		layers.append(act)
+	return nn.Sequential(*layers)
+
+
+def enc(cfg, out={}):
+	"""
+	Returns a dictionary of encoders for each observation in the dict.
+	"""
+	for k in cfg.obs_shape.keys():
+		if k == 'state':
+			out[k] = mlp(cfg.obs_shape[k][0] + cfg.task_dim, max(cfg.num_enc_layers-1, 1)*[cfg.enc_dim], cfg.latent_dim, act=SimNorm(cfg))
+		elif k == 'rgb':
+			out[k] = conv(cfg.obs_shape[k], cfg.num_channels, act=SimNorm(cfg))
+		else:
+			raise NotImplementedError(f"Encoder for observation type {k} not implemented.")
+	return nn.ModuleDict(out)
diff --git a/scripts/baselines/tdmpc2/common/logger.py b/scripts/baselines/tdmpc2/common/logger.py
new file mode 100644
index 0000000..c2fbd4a
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/logger.py
@@ -0,0 +1,282 @@
+import os
+import datetime
+import re
+import numpy as np
+import pandas as pd
+from termcolor import colored
+from omegaconf import OmegaConf
+from mani_skill.utils.visualization.misc import tile_images
+
+from common import TASK_SET
+
+
+CONSOLE_FORMAT = [
+	("iteration", "I", "int"),
+	("episode", "E", "int"),
+	("step", "I", "int"),
+	("episode_reward", "R", "float"),
+	("episode_success", "S", "float"),
+	("episode_fail", "F", "float"),
+	("total_time", "T", "time"),
+	# Added for maniskill rl baselines matrics
+	# ("episode_reward_avg", "RA", "float"), 
+	# ("episode_len", "L", "int"), 
+	# ("rollout_time", "RT", "float"), 
+	# ("rollout_fps", "RF", "float"), 
+	# ("update_time", "U", "float"), 
+
+]
+
+CAT_TO_COLOR = {
+	"pretrain": "yellow",
+	"train": "blue",
+	"eval": "green",
+	# Added for maniskill rl baselines matrics
+	"time" : "magenta", 
+}
+
+
+def make_dir(dir_path):
+	"""Create directory if it does not already exist."""
+	try:
+		os.makedirs(dir_path)
+	except OSError:
+		pass
+	return dir_path
+
+
+def print_run(cfg):
+	"""
+	Pretty-printing of current run information.
+	Logger calls this method at initialization.
+	"""
+	prefix, color, attrs = "  ", "green", ["bold"]
+
+	def _limstr(s, maxlen=36):
+		return str(s[:maxlen]) + "..." if len(str(s)) > maxlen else s
+
+	def _pprint(k, v):
+		print(
+			prefix + colored(f'{k.capitalize()+":":<15}', color, attrs=attrs), _limstr(v)
+		)
+
+	observations  = ", ".join([str(v) for v in cfg.obs_shape.values()])
+	kvs = [
+		("task", cfg.env_id),
+		("steps", f"{int(cfg.steps):,}"),
+		("observations", observations),
+		("actions", cfg.action_dim),
+		("experiment", cfg.exp_name),
+	]
+	w = np.max([len(_limstr(str(kv[1]))) for kv in kvs]) + 25
+	div = "-" * w
+	print(div)
+	for k, v in kvs:
+		_pprint(k, v)
+	print(div)
+
+
+def cfg_to_group(cfg, return_list=False):
+	"""
+	Return a wandb-safe group name for logging.
+	Optionally returns group name as list.
+	"""
+	lst = [cfg.env_id, re.sub("[^0-9a-zA-Z]+", "-", cfg.exp_name)]
+	return lst if return_list else "-".join(lst)
+
+
+class VideoRecorder:
+	"""Utility class for logging evaluation videos."""
+
+	def __init__(self, cfg, wandb, fps=15):
+		self.cfg = cfg
+		self.maniskill_video_nrows = int(np.sqrt(cfg.num_envs))
+		self._save_dir = make_dir(cfg.work_dir / 'eval_video')
+		self._wandb = wandb
+		self.fps = fps
+		self.frames = []
+		self.enabled = False
+
+	def init(self, env, enabled=True):
+		self.frames = []
+		self.enabled = self._save_dir and self._wandb and enabled
+		self.record(env)
+
+	def record(self, env):
+		if self.enabled:
+			self.frames.append(env.render())
+
+	def save(self, step, key='videos/eval_video'):
+		if self.enabled and len(self.frames) > 0:
+			self.frames = [tile_images(rgbs, nrows=self.maniskill_video_nrows) for rgbs in self.frames]
+			frames = np.stack(self.frames)
+			return self._wandb.log(
+				{key: self._wandb.Video(frames.transpose(0, 3, 1, 2), fps=self.fps, format='mp4')}, step=step
+			)
+
+
+class Logger:
+	"""Primary logging object. Logs either locally or using wandb."""
+
+	def __init__(self, cfg):
+		self._log_dir = make_dir(cfg.work_dir)
+		self._model_dir = make_dir(self._log_dir / "models")
+		self._save_csv = cfg.save_csv
+		self._save_agent = cfg.save_agent
+		self._group = cfg_to_group(cfg)
+		self._seed = cfg.seed
+		self._eval = []
+		print_run(cfg)
+		self.project = cfg.get("wandb_project", "none")
+		self.entity = cfg.get("wandb_entity", "none")
+		self.name = cfg.get("wandb_name", "none")
+		self.group = cfg.get("wandb_group", "none")
+		if not cfg.wandb or self.project == "none" or self.entity == "none":
+			print(colored("Wandb disabled.", "blue", attrs=["bold"]))
+			cfg.save_agent = False
+			cfg.save_video = False
+			self._wandb = None
+			self._video = None
+			return
+		os.environ["WANDB_SILENT"] = "true" if cfg.wandb_silent else "false"
+		import wandb
+
+		# Modified for Maniskill RL Baseline Logging Convention
+		wandb_tags = cfg_to_group(cfg, return_list=True) + [f"seed:{cfg.seed}"] + ["tdmpc2"]
+		if cfg.setting_tag != 'none':
+			wandb_tags += [cfg.setting_tag]
+		wandb.init(
+			project=self.project,
+			entity=self.entity,
+			name=self.name,
+			group=self.group,
+			tags=wandb_tags,
+			dir=self._log_dir,
+			config=OmegaConf.to_container(cfg, resolve=True),
+		)
+
+		print(colored("Logs will be synced with wandb.", "blue", attrs=["bold"]))
+		self._wandb = wandb
+		self._video = (
+			VideoRecorder(cfg, self._wandb)
+			if self._wandb and cfg.save_video
+			else None
+		)
+
+	@property
+	def video(self):
+		return self._video
+
+	@property
+	def model_dir(self):
+		return self._model_dir
+
+	def save_agent(self, agent=None, identifier='final'):
+		if self._save_agent and agent:
+			fp = self._model_dir / f'{str(identifier)}.pt'
+			agent.save(fp)
+			if self._wandb:
+				artifact = self._wandb.Artifact(
+					self.group + '-' + str(self._seed) + '-' + str(identifier),
+					type='model',
+				)
+				artifact.add_file(fp)
+				self._wandb.log_artifact(artifact)
+
+	def finish(self, agent=None):
+		try:
+			self.save_agent(agent)
+		except Exception as e:
+			print(colored(f"Failed to save model: {e}", "red"))
+		if self._wandb:
+			self._wandb.finish()
+
+	def _format(self, key, value, ty):
+		if ty == "int":
+			return f'{colored(key+":", "blue")} {int(value):,}'
+		elif ty == "float":
+			return f'{colored(key+":", "blue")} {value:.02f}'
+		elif ty == "time":
+			value = str(datetime.timedelta(seconds=int(value)))
+			return f'{colored(key+":", "blue")} {value}'
+		else:
+			raise f"invalid log format type: {ty}"
+
+	def _print(self, d, category):
+		category = colored(category, CAT_TO_COLOR[category])
+		pieces = [f" {category:<14}"]
+		for k, disp_k, ty in CONSOLE_FORMAT:
+			if k in d:
+				pieces.append(f"{self._format(disp_k, d[k], ty):<22}")
+		print("   ".join(pieces))
+
+	# def pprint_multitask(self, d, cfg):
+	# 	"""Pretty-print evaluation metrics for multi-task training."""
+	# 	print(colored(f'Evaluated agent on {len(cfg.tasks)} tasks:', 'yellow', attrs=['bold']))
+	# 	dmcontrol_reward = []
+	# 	metaworld_reward = []
+	# 	metaworld_success = []
+	# 	for k, v in d.items():
+	# 		if '+' not in k:
+	# 			continue
+	# 		task = k.split('+')[1]
+	# 		if task in TASK_SET['mt30'] and k.startswith('episode_reward'): # DMControl
+	# 			dmcontrol_reward.append(v)
+	# 			print(colored(f'  {task:<22}\tR: {v:.01f}', 'yellow'))
+	# 		elif task in TASK_SET['mt80'] and task not in TASK_SET['mt30']: # Meta-World
+	# 			if k.startswith('episode_reward'):
+	# 				metaworld_reward.append(v)
+	# 			elif k.startswith('episode_success'):
+	# 				metaworld_success.append(v)
+	# 				print(colored(f'  {task:<22}\tS: {v:.02f}', 'yellow'))
+	# 	dmcontrol_reward = np.nanmean(dmcontrol_reward)
+	# 	d['episode_reward+avg_dmcontrol'] = dmcontrol_reward
+	# 	print(colored(f'  {"dmcontrol":<22}\tR: {dmcontrol_reward:.01f}', 'yellow', attrs=['bold']))
+	# 	if cfg.task == 'mt80':
+	# 		metaworld_reward = np.nanmean(metaworld_reward)
+	# 		metaworld_success = np.nanmean(metaworld_success)
+	# 		d['episode_reward+avg_metaworld'] = metaworld_reward
+	# 		d['episode_success+avg_metaworld'] = metaworld_success
+	# 		print(colored(f'  {"metaworld":<22}\tR: {metaworld_reward:.01f}', 'yellow', attrs=['bold']))
+	# 		print(colored(f'  {"metaworld":<22}\tS: {metaworld_success:.02f}', 'yellow', attrs=['bold']))
+
+	def log(self, d, category="train"):
+		assert category in CAT_TO_COLOR.keys(), f"invalid category: {category}"
+		if self._wandb:
+			if category in {"train", "eval", "time"}:
+				xkey = "step"
+			elif category == "pretrain":
+				xkey = "iteration"
+			_d = dict()
+			for k, v in d.items():
+
+				# Change wandb logging titles for maniskill rl baselines common metrics
+				if k == 'episode_reward_avg':
+					k = 'reward' # train/reward, eval/reward
+				elif k == 'episode_reward':
+					k = 'return' # train/return, eval/return
+				elif k == 'episode_success':
+					k = 'success' # train/success, eval/success
+				elif k == 'episode_fail' :
+					k = 'fail' # train/fail, eval/fail
+				elif k == 'episode_len':
+					pass # train/episode_len, eval/episode_len
+				elif k == 'step':
+					pass # train/step, eval/step
+				elif k == 'rollout_time':
+					pass # time/rollout_time
+				elif k == 'rollout_fps':
+					pass # time/rollout_fps
+				elif k == 'update_time':
+					pass
+
+				_d[category + "/" + k] = v
+			self._wandb.log(_d, step=d[xkey])
+		if category == "eval" and self._save_csv:
+			keys = ["step", "episode_reward"]
+			self._eval.append(np.array([d[keys[0]], d[keys[1]]]))
+			pd.DataFrame(np.array(self._eval)).to_csv(
+				self._log_dir / "eval.csv", header=keys, index=None
+			)
+		if category != 'time':
+			self._print(d, category)
diff --git a/scripts/baselines/tdmpc2/common/math.py b/scripts/baselines/tdmpc2/common/math.py
new file mode 100644
index 0000000..62b8230
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/math.py
@@ -0,0 +1,95 @@
+import torch
+import torch.nn.functional as F
+
+
+def soft_ce(pred, target, cfg):
+	"""Computes the cross entropy loss between predictions and soft targets."""
+	pred = F.log_softmax(pred, dim=-1)
+	target = two_hot(target, cfg)
+	return -(target * pred).sum(-1, keepdim=True)
+
+
+@torch.jit.script
+def log_std(x, low, dif):
+	return low + 0.5 * dif * (torch.tanh(x) + 1)
+
+
+@torch.jit.script
+def _gaussian_residual(eps, log_std):
+	return -0.5 * eps.pow(2) - log_std
+
+
+@torch.jit.script
+def _gaussian_logprob(residual):
+	return residual - 0.5 * torch.log(2 * torch.pi)
+
+
+def gaussian_logprob(eps, log_std, size=None):
+	"""Compute Gaussian log probability."""
+	residual = _gaussian_residual(eps, log_std).sum(-1, keepdim=True)
+	if size is None:
+		size = eps.size(-1)
+	return _gaussian_logprob(residual) * size
+
+
+@torch.jit.script
+def _squash(pi):
+	return torch.log(F.relu(1 - pi.pow(2)) + 1e-6)
+
+
+def squash(mu, pi, log_pi):
+	"""Apply squashing function."""
+	mu = torch.tanh(mu)
+	pi = torch.tanh(pi)
+	log_pi -= _squash(pi).sum(-1, keepdim=True)
+	return mu, pi, log_pi
+
+
+@torch.jit.script
+def symlog(x):
+	"""
+	Symmetric logarithmic function.
+	Adapted from https://github.com/danijar/dreamerv3.
+	"""
+	return torch.sign(x) * torch.log(1 + torch.abs(x))
+
+
+@torch.jit.script
+def symexp(x):
+	"""
+	Symmetric exponential function.
+	Adapted from https://github.com/danijar/dreamerv3.
+	"""
+	return torch.sign(x) * (torch.exp(torch.abs(x)) - 1)
+
+
+def two_hot(x, cfg):
+	"""Converts a batch of scalars to soft two-hot encoded targets for discrete regression."""
+	if cfg.num_bins == 0:
+		return x
+	elif cfg.num_bins == 1:
+		return symlog(x)
+	x = torch.clamp(symlog(x), cfg.vmin, cfg.vmax).squeeze(1)
+	bin_idx = torch.floor((x - cfg.vmin) / cfg.bin_size).long()
+	bin_offset = ((x - cfg.vmin) / cfg.bin_size - bin_idx.float()).unsqueeze(-1)
+	soft_two_hot = torch.zeros(x.size(0), cfg.num_bins, device=x.device)
+	soft_two_hot.scatter_(1, bin_idx.unsqueeze(1), 1 - bin_offset)
+	soft_two_hot.scatter_(1, (bin_idx.unsqueeze(1) + 1) % cfg.num_bins, bin_offset)
+	return soft_two_hot
+
+
+DREG_BINS = None
+
+
+def two_hot_inv(x, cfg):
+	"""Converts a batch of soft two-hot encoded vectors to scalars."""
+	global DREG_BINS
+	if cfg.num_bins == 0:
+		return x
+	elif cfg.num_bins == 1:
+		return symexp(x)
+	if DREG_BINS is None:
+		DREG_BINS = torch.linspace(cfg.vmin, cfg.vmax, cfg.num_bins, device=x.device)
+	x = F.softmax(x, dim=-1)
+	x = torch.sum(x * DREG_BINS, dim=-1, keepdim=True)
+	return symexp(x)
diff --git a/scripts/baselines/tdmpc2/common/parser.py b/scripts/baselines/tdmpc2/common/parser.py
new file mode 100644
index 0000000..058faa7
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/parser.py
@@ -0,0 +1,75 @@
+import re
+from pathlib import Path
+
+import hydra
+from omegaconf import OmegaConf
+
+from common import MODEL_SIZE, TASK_SET
+
+
+def parse_cfg(cfg: OmegaConf) -> OmegaConf:
+	"""
+	Parses a Hydra config. Mostly for convenience.
+	"""
+
+	# Logic
+	for k in cfg.keys():
+		try:
+			v = cfg[k]
+			if v == None:
+				v = True
+		except:
+			pass
+
+	# Algebraic expressions
+	for k in cfg.keys():
+		try:
+			v = cfg[k]
+			if isinstance(v, str):
+				match = re.match(r"(\d+)([+\-*/])(\d+)", v)
+				if match:
+					cfg[k] = eval(match.group(1) + match.group(2) + match.group(3))
+					if isinstance(cfg[k], float) and cfg[k].is_integer():
+						cfg[k] = int(cfg[k])
+		except:
+			pass
+
+	# Convenience
+	cfg.work_dir = Path(hydra.utils.get_original_cwd()) / 'logs' / cfg.env_id / str(cfg.seed) / cfg.exp_name
+	cfg.bin_size = (cfg.vmax - cfg.vmin) / (cfg.num_bins-1) # Bin size for discrete regression
+
+	# Model size
+	if cfg.get('model_size', None) is not None:
+		assert cfg.model_size in MODEL_SIZE.keys(), \
+			f'Invalid model size {cfg.model_size}. Must be one of {list(MODEL_SIZE.keys())}'
+		for k, v in MODEL_SIZE[cfg.model_size].items():
+			cfg[k] = v
+
+	# Multi-task
+	cfg.multitask = cfg.env_id in TASK_SET.keys()
+	if cfg.multitask:
+		# Account for slight inconsistency in task_dim for the mt30 experiments
+		cfg.task_dim = 96 if cfg.env_id == 'mt80' or cfg.model_size in {1, 317} else 64
+	else:
+		cfg.task_dim = 0
+	cfg.tasks = TASK_SET.get(cfg.env_id, [cfg.env_id])
+
+
+
+	# Maniskill
+	cfg.env_cfg.env_id = cfg.eval_env_cfg.env_id = cfg.env_id
+	cfg.env_cfg.obs_mode = cfg.eval_env_cfg.obs_mode = cfg.obs # state or rgb
+	cfg.env_cfg.reward_mode = cfg.eval_env_cfg.reward_mode = 'normalized_dense'
+	cfg.env_cfg.num_envs = cfg.eval_env_cfg.num_envs = cfg.num_envs # eval_env num_envs can change (tbd)
+	if cfg.num_envs == 1:
+		cfg.env_cfg.sim_backend = cfg.eval_env_cfg.sim_backend = 'cpu'
+	else:
+		cfg.env_cfg.sim_backend = cfg.eval_env_cfg.sim_backend = 'gpu'
+	
+	cfg.eval_env_cfg.num_eval_episodes = cfg.eval_episodes
+		
+	# cfg.(eval_)env_cfg.control_mode is defined in maniskill.py
+	# cfg.(eval_)env_cfg.env_horizon is defined in maniskill.py
+	# cfg.discount is defined in tdmpc2.py
+
+	return cfg
diff --git a/scripts/baselines/tdmpc2/common/scale.py b/scripts/baselines/tdmpc2/common/scale.py
new file mode 100644
index 0000000..63f0bb2
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/scale.py
@@ -0,0 +1,48 @@
+import torch
+
+
+class RunningScale:
+	"""Running trimmed scale estimator."""
+
+	def __init__(self, cfg):
+		self.cfg = cfg
+		self._value = torch.ones(1, dtype=torch.float32, device=torch.device('cuda'))
+		self._percentiles = torch.tensor([5, 95], dtype=torch.float32, device=torch.device('cuda'))
+
+	def state_dict(self):
+		return dict(value=self._value, percentiles=self._percentiles)
+
+	def load_state_dict(self, state_dict):
+		self._value.data.copy_(state_dict['value'])
+		self._percentiles.data.copy_(state_dict['percentiles'])
+
+	@property
+	def value(self):
+		return self._value.cpu().item()
+
+	def _percentile(self, x):
+		x_dtype, x_shape = x.dtype, x.shape
+		x = x.view(x.shape[0], -1)
+		in_sorted, _ = torch.sort(x, dim=0)
+		positions = self._percentiles * (x.shape[0]-1) / 100
+		floored = torch.floor(positions)
+		ceiled = floored + 1
+		ceiled[ceiled > x.shape[0] - 1] = x.shape[0] - 1
+		weight_ceiled = positions-floored
+		weight_floored = 1.0 - weight_ceiled
+		d0 = in_sorted[floored.long(), :] * weight_floored[:, None]
+		d1 = in_sorted[ceiled.long(), :] * weight_ceiled[:, None]
+		return (d0+d1).view(-1, *x_shape[1:]).type(x_dtype)
+
+	def update(self, x):
+		percentiles = self._percentile(x.detach())
+		value = torch.clamp(percentiles[1] - percentiles[0], min=1.)
+		self._value.data.lerp_(value, self.cfg.tau)
+
+	def __call__(self, x, update=False):
+		if update:
+			self.update(x)
+		return x * (1/self.value)
+
+	def __repr__(self):
+		return f'RunningScale(S: {self.value})'
diff --git a/scripts/baselines/tdmpc2/common/seed.py b/scripts/baselines/tdmpc2/common/seed.py
new file mode 100644
index 0000000..5c8972e
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/seed.py
@@ -0,0 +1,12 @@
+import random
+
+import numpy as np
+import torch
+
+
+def set_seed(seed):
+	"""Set seed for reproducibility."""
+	random.seed(seed)
+	np.random.seed(seed)
+	torch.manual_seed(seed)
+	torch.cuda.manual_seed_all(seed)
diff --git a/scripts/baselines/tdmpc2/common/world_model.py b/scripts/baselines/tdmpc2/common/world_model.py
new file mode 100644
index 0000000..e9283d3
--- /dev/null
+++ b/scripts/baselines/tdmpc2/common/world_model.py
@@ -0,0 +1,178 @@
+from copy import deepcopy
+
+import numpy as np
+import torch
+import torch.nn as nn
+
+from common import layers, math, init
+
+
+class WorldModel(nn.Module):
+	"""
+	TD-MPC2 implicit world model architecture.
+	Can be used for both single-task and multi-task experiments.
+	"""
+
+	def __init__(self, cfg):
+		super().__init__()
+		self.cfg = cfg
+		if cfg.multitask:
+			self._task_emb = nn.Embedding(len(cfg.tasks), cfg.task_dim, max_norm=1)
+			self._action_masks = torch.zeros(len(cfg.tasks), cfg.action_dim)
+			for i in range(len(cfg.tasks)):
+				self._action_masks[i, :cfg.action_dims[i]] = 1.
+		self._encoder = layers.enc(cfg)
+		self._dynamics = layers.mlp(cfg.latent_dim + cfg.action_dim + cfg.task_dim, 2*[cfg.mlp_dim], cfg.latent_dim, act=layers.SimNorm(cfg))
+		self._reward = layers.mlp(cfg.latent_dim + cfg.action_dim + cfg.task_dim, 2*[cfg.mlp_dim], max(cfg.num_bins, 1))
+		self._pi = layers.mlp(cfg.latent_dim + cfg.task_dim, 2*[cfg.mlp_dim], 2*cfg.action_dim)
+		self._Qs = layers.Ensemble([layers.mlp(cfg.latent_dim + cfg.action_dim + cfg.task_dim, 2*[cfg.mlp_dim], 
+										 	   max(cfg.num_bins, 1), dropout=cfg.dropout) for _ in range(cfg.num_q)])
+		self.apply(init.weight_init)
+		init.zero_([self._reward[-1].weight, self._Qs.params[-2]])
+		self._target_Qs = deepcopy(self._Qs).requires_grad_(False)
+		self.log_std_min = torch.tensor(cfg.log_std_min)
+		self.log_std_dif = torch.tensor(cfg.log_std_max) - self.log_std_min
+
+	@property
+	def total_params(self):
+		return sum(p.numel() for p in self.parameters() if p.requires_grad)
+		
+	def to(self, *args, **kwargs):
+		"""
+		Overriding `to` method to also move additional tensors to device.
+		"""
+		super().to(*args, **kwargs)
+		if self.cfg.multitask:
+			self._action_masks = self._action_masks.to(*args, **kwargs)
+		self.log_std_min = self.log_std_min.to(*args, **kwargs)
+		self.log_std_dif = self.log_std_dif.to(*args, **kwargs)
+		return self
+	
+	def train(self, mode=True):
+		"""
+		Overriding `train` method to keep target Q-networks in eval mode.
+		"""
+		super().train(mode)
+		self._target_Qs.train(False)
+		return self
+
+	def track_q_grad(self, mode=True):
+		"""
+		Enables/disables gradient tracking of Q-networks.
+		Avoids unnecessary computation during policy optimization.
+		This method also enables/disables gradients for task embeddings.
+		"""
+		for p in self._Qs.parameters():
+			p.requires_grad_(mode)
+		if self.cfg.multitask:
+			for p in self._task_emb.parameters():
+				p.requires_grad_(mode)
+
+	def soft_update_target_Q(self):
+		"""
+		Soft-update target Q-networks using Polyak averaging.
+		"""
+		with torch.no_grad():
+			for p, p_target in zip(self._Qs.parameters(), self._target_Qs.parameters()):
+				p_target.data.lerp_(p.data, self.cfg.tau)
+	
+	def task_emb(self, x, task):
+		"""
+		Continuous task embedding for multi-task experiments.
+		Retrieves the task embedding for a given task ID `task`
+		and concatenates it to the input `x`.
+		"""
+		if isinstance(task, int):
+			task = torch.tensor([task], device=x.device)
+		emb = self._task_emb(task.long())
+		if x.ndim == 3:
+			emb = emb.unsqueeze(0).repeat(x.shape[0], 1, 1)
+		elif emb.shape[0] == 1:
+			emb = emb.repeat(x.shape[0], 1)
+		return torch.cat([x, emb], dim=-1)
+
+	def encode(self, obs, task):
+		"""
+		Encodes an observation into its latent representation. Online trainer obs is [1, obs_shape], task is None
+		This implementation assumes a single state-based observation. Should be already batched.
+		Should be ok.
+		"""
+		if self.cfg.multitask:
+			obs = self.task_emb(obs, task)
+		if self.cfg.obs == 'rgb' and obs.ndim == 5:
+			return torch.stack([self._encoder[self.cfg.obs](o) for o in obs])
+		return self._encoder[self.cfg.obs](obs)
+
+	def next(self, z, a, task):
+		"""
+		z[]
+		Predicts the next latent state given the current latent state and action.
+		"""
+		if self.cfg.multitask:
+			z = self.task_emb(z, task)
+		z = torch.cat([z, a], dim=-1)
+		return self._dynamics(z)
+	
+	def reward(self, z, a, task):
+		"""
+		Predicts instantaneous (single-step) reward.
+		"""
+		if self.cfg.multitask:
+			z = self.task_emb(z, task)
+		z = torch.cat([z, a], dim=-1)
+		return self._reward(z)
+
+	def pi(self, z, task):
+		"""
+		z[~, 1]
+		Return  mu[~, action_dim], pi[~, action_dim], log_pi[~, 1], log_std[~, action_dim]
+
+		Samples an action from the policy prior.
+		The policy prior is a Gaussian distribution with
+		mean and (log) std predicted by a neural network.
+		"""
+		if self.cfg.multitask:
+			z = self.task_emb(z, task)
+
+		# Gaussian policy prior
+		mu, log_std = self._pi(z).chunk(2, dim=-1)
+		log_std = math.log_std(log_std, self.log_std_min, self.log_std_dif)
+		eps = torch.randn_like(mu)
+
+		if self.cfg.multitask: # Mask out unused action dimensions
+			mu = mu * self._action_masks[task]
+			log_std = log_std * self._action_masks[task]
+			eps = eps * self._action_masks[task]
+			action_dims = self._action_masks.sum(-1)[task].unsqueeze(-1)
+		else: # No masking
+			action_dims = None
+
+		log_pi = math.gaussian_logprob(eps, log_std, size=action_dims)
+		pi = mu + eps * log_std.exp()
+		mu, pi, log_pi = math.squash(mu, pi, log_pi)
+
+		return mu, pi, log_pi, log_std
+
+	def Q(self, z, a, task, return_type='min', target=False):
+		"""
+		Predict state-action value. z[~, latent_dim], a[~, action_dim] -> [num_q, ~, num_bins] if all else [~, 1]
+		`return_type` can be one of [`min`, `avg`, `all`]:
+			- `min`: return the minimum of two randomly subsampled Q-values.
+			- `avg`: return the average of two randomly subsampled Q-values.
+			- `all`: return all Q-values.
+		`target` specifies whether to use the target Q-networks or not.
+		"""
+		assert return_type in {'min', 'avg', 'all'}
+
+		if self.cfg.multitask:
+			z = self.task_emb(z, task)
+			
+		z = torch.cat([z, a], dim=-1)
+		out = (self._target_Qs if target else self._Qs)(z)
+
+		if return_type == 'all':
+			return out
+
+		Q1, Q2 = out[np.random.choice(self.cfg.num_q, 2, replace=False)]
+		Q1, Q2 = math.two_hot_inv(Q1, self.cfg), math.two_hot_inv(Q2, self.cfg)
+		return torch.min(Q1, Q2) if return_type == 'min' else (Q1 + Q2) / 2
diff --git a/scripts/baselines/tdmpc2/config.yaml b/scripts/baselines/tdmpc2/config.yaml
new file mode 100644
index 0000000..dba81df
--- /dev/null
+++ b/scripts/baselines/tdmpc2/config.yaml
@@ -0,0 +1,116 @@
+defaults:
+    - override hydra/launcher: submitit_local
+
+# environment
+env_id: PushCube-v1
+obs: state # or rgb
+control_mode: default # or pd_joint_delta_pos or pd_ee_delta_pose
+num_envs: 32
+render_mode: rgb_array # ['rgb_array' for quality, or 'sensors' for speed]
+render_size: 64
+setting_tag: none # ['none', 'walltime_efficient', 'sample_efficient', ...] for wandb tags
+
+# evaluation
+checkpoint: ???
+eval_episodes: 10
+eval_freq: 50000
+
+# training
+steps: 1_000_000
+batch_size: 256
+reward_coef: 0.1
+value_coef: 0.1
+consistency_coef: 20
+rho: 0.5
+lr: 3e-4
+enc_lr_scale: 0.3
+grad_clip_norm: 20
+tau: 0.01
+discount_denom: 5
+discount_min: 0.95
+discount_max: 0.995
+buffer_size: 1_000_000
+exp_name: default
+data_dir: ???
+steps_per_update: 1
+
+# planning
+mpc: true
+iterations: 6
+num_samples: 512
+num_elites: 64
+num_pi_trajs: 24
+horizon: 3
+min_std: 0.05
+max_std: 2
+temperature: 0.5
+
+# actor
+log_std_min: -10
+log_std_max: 2
+entropy_coef: 1e-4
+
+# critic
+num_bins: 101
+vmin: -10
+vmax: +10
+
+# architecture
+model_size: ???
+num_enc_layers: 2
+enc_dim: 256
+num_channels: 32
+mlp_dim: 512
+latent_dim: 512
+task_dim: 0
+num_q: 5
+dropout: 0.01
+simnorm_dim: 8
+
+# logging
+wandb_project:
+wandb_group: 
+wandb_name:
+wandb_entity: 
+wandb_silent: false
+wandb: false # enable wandb
+save_csv: true
+
+# misc
+save_video: true
+save_agent: true
+seed: 1
+
+# convenience
+work_dir: ???
+task_title: ???
+multitask: ???
+tasks: ???
+obs_shape: ???
+action_dim: ???
+episode_length: ???
+obs_shapes: ???
+action_dims: ???
+episode_lengths: ???
+seed_steps: ???
+bin_size: ???
+
+# Added for Maniskill RL Baselines Config Convention (don't assign to them)
+env_cfg:
+    env_id: ???
+    control_mode: ??? # pd_joint_delta_pos or pd_ee_delta_pose
+    obs_mode: ???
+    reward_mode: ??? 
+    num_envs: ???
+    sim_backend: ??? # cpu or gpu
+    env_horizon: ???
+eval_env_cfg:
+    env_id: ???
+    control_mode: ???
+    obs_mode: ???
+    reward_mode: ???
+    num_envs: ???
+    sim_backend: ???
+    env_horizon: ???
+    num_eval_episodes: ???
+discount: ???
\ No newline at end of file
diff --git a/scripts/baselines/tdmpc2/environment.yaml b/scripts/baselines/tdmpc2/environment.yaml
new file mode 100644
index 0000000..64ae789
--- /dev/null
+++ b/scripts/baselines/tdmpc2/environment.yaml
@@ -0,0 +1,67 @@
+name: tdmpc2-ms
+channels:
+  - pytorch-nightly
+  - nvidia
+  - conda-forge
+  - defaults
+dependencies:
+  - cudatoolkit=11.7
+  - glew=2.1.0
+  - glib=2.68.4
+  - pip=21.0
+  - python=3.9.0
+  - pytorch>=2.2.2
+  - torchvision>=0.16.2
+  - pip:
+    - absl-py==2.0.0
+    - "cython<3"
+    - dm-control==1.0.8
+    - ffmpeg==1.4
+    - glfw==2.6.4
+    - hydra-core==1.3.2
+    - hydra-submitit-launcher==1.2.0
+    - imageio==2.33.1
+    - imageio-ffmpeg==0.4.9
+    - kornia==0.7.1
+    - moviepy==1.0.3
+    - mujoco==2.3.1
+    - mujoco-py==2.1.2.14
+    - numpy==1.23.5
+    - omegaconf==2.3.0
+    - open3d==0.18.0
+    - opencv-contrib-python==4.9.0.80
+    - opencv-python==4.9.0.80
+    - pandas==2.1.4
+    - sapien==3.0.0.b1
+    - submitit==1.5.1
+    - setuptools==65.5.0
+    - patchelf==0.17.2.1
+    - protobuf==4.25.2
+    - pillow==10.2.0
+    - pyquaternion==0.9.9
+    - tensordict-nightly==2024.3.26
+    - termcolor==2.4.0
+    - torchrl-nightly==2024.3.26
+    - transforms3d==0.4.1
+    - trimesh==4.0.9
+    - tqdm==4.66.1
+    - wandb==0.16.2
+    - wheel==0.38.0
+    - mani_skill>=3.0.0b9
+    ####################
+    # Gym:
+    # (unmaintained but required for maniskill2/meta-world/myosuite)
+    # - gym==0.21.0
+    ####################
+    # ManiSkill2:
+    # (requires gym==0.21.0 which occasionally breaks)
+    # - mani-skill2==0.4.1
+    ####################
+    # Meta-World:
+    # (requires gym==0.21.0 which occasionally breaks)
+    # - git+https://github.com/Farama-Foundation/Metaworld.git@04be337a12305e393c0caf0cbf5ec7755c7c8feb
+    ####################
+    # MyoSuite:
+    # (requires gym==0.13 which conflicts with meta-world / mani-skill2)
+    # - myosuite
+    ####################
diff --git a/scripts/baselines/tdmpc2/envs/__init__.py b/scripts/baselines/tdmpc2/envs/__init__.py
new file mode 100644
index 0000000..8e8af1b
--- /dev/null
+++ b/scripts/baselines/tdmpc2/envs/__init__.py
@@ -0,0 +1,33 @@
+from copy import deepcopy
+import warnings
+
+import gymnasium as gym
+
+from envs.wrappers.pixels import PixelWrapper
+
+def missing_dependencies(task):
+	raise ValueError(f'Missing dependencies for task {task}; install dependencies to use this environment.')
+
+try:
+	from envs.maniskill import make_envs as make_maniskill_vec_env
+except:
+	make_maniskill_env = missing_dependencies
+
+
+warnings.filterwarnings('ignore', category=DeprecationWarning)
+
+def make_envs(cfg):
+	from envs.maniskill import make_envs as make_maniskill_vec_env
+	env = make_maniskill_vec_env(cfg)
+
+	if cfg.get('obs', 'state') == 'rgb':
+		env = PixelWrapper(cfg, env)
+
+	try: # Dict
+		cfg.obs_shape = {k: v.shape[1:] for k, v in env.observation_space.spaces.items()}
+	except: # Box
+		cfg.obs_shape = {cfg.get('obs', 'state'): env.observation_space.shape[1:]}
+	cfg.action_dim = env.action_space.shape[1]
+	cfg.episode_length = env.max_episode_steps
+	cfg.seed_steps = max(1000, cfg.num_envs * cfg.episode_length)
+	return env
\ No newline at end of file
diff --git a/scripts/baselines/tdmpc2/envs/maniskill.py b/scripts/baselines/tdmpc2/envs/maniskill.py
new file mode 100644
index 0000000..9e515ac
--- /dev/null
+++ b/scripts/baselines/tdmpc2/envs/maniskill.py
@@ -0,0 +1,34 @@
+import gymnasium as gym
+import numpy as np
+from mani_skill.vector.wrappers.gymnasium import ManiSkillVectorEnv
+
+import mani_skill.envs
+
+def make_envs(cfg):
+	"""
+	Make ManiSkill3 environment.
+	"""
+	#assert cfg.obs == 'state', 'This task only supports state observations.'
+	if cfg.control_mode == 'default':
+		env = gym.make(
+			cfg.env_id,
+			obs_mode=cfg.obs, 
+			render_mode=cfg.render_mode,
+			sensor_configs=dict(width=cfg.render_size, height=cfg.render_size),
+			num_envs=cfg.num_envs
+		)
+	else:
+		env = gym.make(
+			cfg.env_id,
+			obs_mode=cfg.obs,
+			control_mode=cfg.control_mode,
+			render_mode=cfg.render_mode,
+			sensor_configs=dict(width=cfg.render_size, height=cfg.render_size),
+			num_envs=cfg.num_envs
+		)
+
+	cfg.env_cfg.control_mode = cfg.eval_env_cfg.control_mode = env.control_mode
+	env.max_episode_steps = env._max_episode_steps
+	cfg.env_cfg.env_horizon = cfg.eval_env_cfg.env_horizon = env.max_episode_steps
+	env = ManiSkillVectorEnv(env, ignore_terminations=True)
+	return env
\ No newline at end of file
diff --git a/scripts/baselines/tdmpc2/envs/wrappers/__init__.py b/scripts/baselines/tdmpc2/envs/wrappers/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/scripts/baselines/tdmpc2/envs/wrappers/multitask.py b/scripts/baselines/tdmpc2/envs/wrappers/multitask.py
new file mode 100644
index 0000000..08dd4eb
--- /dev/null
+++ b/scripts/baselines/tdmpc2/envs/wrappers/multitask.py
@@ -0,0 +1,57 @@
+import gym
+import numpy as np
+import torch
+
+
+class MultitaskWrapper(gym.Wrapper):
+	"""
+	Wrapper for multi-task environments.
+	"""
+
+	def __init__(self, cfg, envs):
+		super().__init__(envs[0])
+		self.cfg = cfg
+		self.envs = envs
+		self._task = cfg.tasks[0]
+		self._task_idx = 0
+		self._obs_dims = [env.observation_space.shape[0] for env in self.envs]
+		self._action_dims = [env.action_space.shape[0] for env in self.envs]
+		self._episode_lengths = [env.max_episode_steps for env in self.envs]
+		self._obs_shape = (max(self._obs_dims),)
+		self._action_dim = max(self._action_dims)
+		self.observation_space = gym.spaces.Box(
+			low=-np.inf, high=np.inf, shape=self._obs_shape, dtype=np.float32
+		)
+		self.action_space = gym.spaces.Box(
+			low=-1, high=1, shape=(self._action_dim,), dtype=np.float32
+		)
+	
+	@property
+	def task(self):
+		return self._task
+	
+	@property
+	def task_idx(self):
+		return self._task_idx
+	
+	@property
+	def _env(self):
+		return self.envs[self.task_idx]
+
+	def rand_act(self):
+		return torch.from_numpy(self.action_space.sample().astype(np.float32))
+
+	def _pad_obs(self, obs):
+		if obs.shape != self._obs_shape:
+			obs = torch.cat((obs, torch.zeros(self._obs_shape[0]-obs.shape[0], dtype=obs.dtype, device=obs.device)))
+		return obs
+	
+	def reset(self, task_idx=-1):
+		self._task_idx = task_idx
+		self._task = self.cfg.tasks[task_idx]
+		self.env = self._env
+		return self._pad_obs(self.env.reset())
+
+	def step(self, action):
+		obs, reward, done, info = self.env.step(action[:self.env.action_space.shape[0]])
+		return self._pad_obs(obs), reward, done, info
diff --git a/scripts/baselines/tdmpc2/envs/wrappers/pixels.py b/scripts/baselines/tdmpc2/envs/wrappers/pixels.py
new file mode 100644
index 0000000..74430dd
--- /dev/null
+++ b/scripts/baselines/tdmpc2/envs/wrappers/pixels.py
@@ -0,0 +1,48 @@
+from collections import deque
+
+import gymnasium as gym
+import numpy as np
+import torch
+
+
+class PixelWrapper(gym.Wrapper):
+	"""
+	Wrapper for pixel observations. Works with Maniskill vectorized environments
+	"""
+
+	def __init__(self, cfg, env, num_frames=3):
+		super().__init__(env)
+		self.cfg = cfg
+		self.env = env
+		self.observation_space = gym.spaces.Box(
+			low=0, high=255, shape=(cfg.num_envs, num_frames*3, cfg.render_size, cfg.render_size), dtype=np.uint8
+		)
+		self._frames = deque([], maxlen=num_frames)
+		self._render_size = cfg.render_size
+
+		# # Using tensor to mimick self._frames = deque([], maxlen=num_frames) so the data remain on the same device **turned out to be slower**
+		# self._frames = torch.zeros((cfg.num_envs, num_frames*3, cfg.render_size, cfg.render_size)).to(self.env.device)
+		# self._frames_idx = 0
+		# self._frames_maxlen = num_frames
+		# self._render_size = cfg.render_size
+
+	def _get_obs(self, obs):
+		frame = obs['sensor_data']['base_camera']['rgb'].cpu().permute(0,3,1,2)
+		self._frames.append(frame)
+		return torch.from_numpy(np.concatenate(self._frames, axis=1)).to(self.env.device)
+	
+		# frame = obs['sensor_data']['base_camera']['rgb'].permute(0,3,1,2)
+		# self._frames[:, self._frames_idx*3:self._frames_idx*3+3, ...] = frame
+		# self._frames_idx = (self._frames_idx + 1) % self._frames_maxlen
+		
+		# return torch.cat((self._frames[:,self._frames_idx*3:,:], self._frames[:,:self._frames_idx*3,:]), 1) # reorder so obs is from old to new
+
+	def reset(self):
+		obs, info = self.env.reset()
+		for _ in range(self._frames.maxlen):
+			obs_frames = self._get_obs(obs)
+		return obs_frames, info
+
+	def step(self, action):
+		obs, reward, terminated, truncated, info = self.env.step(action)
+		return self._get_obs(obs), reward, terminated, truncated, info
diff --git a/scripts/baselines/tdmpc2/envs/wrappers/tensor.py b/scripts/baselines/tdmpc2/envs/wrappers/tensor.py
new file mode 100644
index 0000000..5054989
--- /dev/null
+++ b/scripts/baselines/tdmpc2/envs/wrappers/tensor.py
@@ -0,0 +1,41 @@
+from collections import defaultdict
+
+import gymnasium as gym
+import numpy as np
+import torch
+
+
+class TensorWrapper(gym.Wrapper):
+	"""
+	Wrapper for converting numpy arrays to torch tensors.
+	"""
+
+	def __init__(self, env):
+		super().__init__(env)
+	
+	def rand_act(self):
+		return torch.from_numpy(self.action_space.sample().astype(np.float32))
+
+	def _try_f32_tensor(self, x):
+		if isinstance(x, np.ndarray):
+			x = torch.from_numpy(x)
+			if x.dtype == torch.float64:
+				x = x.float()
+		return x
+
+	def _obs_to_tensor(self, obs):
+		if isinstance(obs, dict):
+			for k in obs.keys():
+				obs[k] = self._try_f32_tensor(obs[k])
+		else:
+			obs = self._try_f32_tensor(obs)
+		return obs
+
+	def reset(self, task_idx=None):
+		return self._obs_to_tensor(self.env.reset())
+
+	def step(self, action):
+		obs, reward, done, info = self.env.step(action.numpy())
+		info = defaultdict(float, info)
+		info['success'] = float(info['success'])
+		return self._obs_to_tensor(obs), torch.tensor(reward, dtype=torch.float32), done, info
diff --git a/scripts/baselines/tdmpc2/envs/wrappers/time_limit.py b/scripts/baselines/tdmpc2/envs/wrappers/time_limit.py
new file mode 100644
index 0000000..78c2461
--- /dev/null
+++ b/scripts/baselines/tdmpc2/envs/wrappers/time_limit.py
@@ -0,0 +1,72 @@
+"""
+Wrapper for limiting the time steps of an environment.
+Source: https://github.com/openai/gym/blob/3498617bf031538a808b75b932f4ed2c11896a3e/gym/wrappers/time_limit.py
+"""
+from typing import Optional
+
+import gymnasium as gym
+
+
+class TimeLimit(gym.Wrapper):
+    """This wrapper will issue a `done` signal if a maximum number of timesteps is exceeded.
+
+    Oftentimes, it is **very** important to distinguish `done` signals that were produced by the
+    :class:`TimeLimit` wrapper (truncations) and those that originate from the underlying environment (terminations).
+    This can be done by looking at the ``info`` that is returned when `done`-signal was issued.
+    The done-signal originates from the time limit (i.e. it signifies a *truncation*) if and only if
+    the key `"TimeLimit.truncated"` exists in ``info`` and the corresponding value is ``True``.
+
+    Example:
+       >>> from gym.envs.classic_control import CartPoleEnv
+       >>> from gym.wrappers import TimeLimit
+       >>> env = CartPoleEnv()
+       >>> env = TimeLimit(env, max_episode_steps=1000)
+    """
+
+    def __init__(self, env: gym.Env, max_episode_steps: Optional[int] = None):
+        """Initializes the :class:`TimeLimit` wrapper with an environment and the number of steps after which truncation will occur.
+
+        Args:
+            env: The environment to apply the wrapper
+            max_episode_steps: An optional max episode steps (if ``one``, ``env.spec.max_episode_steps`` is used)
+        """
+        super().__init__(env)
+        if max_episode_steps is None and self.env.spec is not None:
+            max_episode_steps = env.spec.max_episode_steps
+        if self.env.spec is not None:
+            self.env.spec.max_episode_steps = max_episode_steps
+        self._max_episode_steps = max_episode_steps
+        self._elapsed_steps = None
+
+    def step(self, action):
+        """Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.
+
+        Args:
+            action: The environment step action
+
+        Returns:
+            The environment step ``(observation, reward, done, info)`` with "TimeLimit.truncated"=True
+            when truncated (the number of steps elapsed >= max episode steps) or
+            "TimeLimit.truncated"=False if the environment terminated
+        """
+        observation, reward, done, info = self.env.step(action)
+        self._elapsed_steps += 1
+        if self._elapsed_steps >= self._max_episode_steps:
+            # TimeLimit.truncated key may have been already set by the environment
+            # do not overwrite it
+            episode_truncated = not done or info.get("TimeLimit.truncated", False)
+            info["TimeLimit.truncated"] = episode_truncated
+            done = True
+        return observation, reward, done, info
+
+    def reset(self, **kwargs):
+        """Resets the environment with :param:`**kwargs` and sets the number of steps elapsed to zero.
+
+        Args:
+            **kwargs: The kwargs to reset the environment with
+
+        Returns:
+            The reset environment
+        """
+        self._elapsed_steps = 0
+        return self.env.reset(**kwargs)
diff --git a/scripts/baselines/tdmpc2/evaluate.py b/scripts/baselines/tdmpc2/evaluate.py
new file mode 100644
index 0000000..738d9c2
--- /dev/null
+++ b/scripts/baselines/tdmpc2/evaluate.py
@@ -0,0 +1,123 @@
+import os
+os.environ['MUJOCO_GL'] = 'egl'
+import warnings
+warnings.filterwarnings('ignore')
+
+import hydra
+import imageio
+import numpy as np
+import torch
+from termcolor import colored
+
+from common.parser import parse_cfg
+from common.seed import set_seed
+from envs import make_envs
+from tdmpc2 import TDMPC2
+
+torch.backends.cudnn.benchmark = True
+
+
+@hydra.main(config_name='config', config_path='.')
+def evaluate(cfg: dict):
+	"""
+	Script for evaluating a single-task / multi-task TD-MPC2 checkpoint.
+
+	Most relevant args:
+		`env_id`: task name (eg. PickCube-v0)
+		`model_size`: model size, must be one of `[1, 5, 19, 48, 317]` (default: 5)
+		`checkpoint`: path to model checkpoint to load
+		`eval_episodes`: number of episodes to evaluate on per task (default: 10)
+		`save_video`: whether to save a video of the evaluation (default: True)
+		`seed`: random seed (default: 1)
+	
+	See config.yaml for a full list of args.
+
+	Example usage:
+	````
+		$ python evaluate.py task=mt80 model_size=48 checkpoint=/path/to/mt80-48M.pt
+		$ python evaluate.py task=mt30 model_size=317 checkpoint=/path/to/mt30-317M.pt
+		$ python evaluate.py task=dog-run checkpoint=/path/to/dog-1.pt save_video=true
+	```
+	"""
+	assert torch.cuda.is_available()
+	assert cfg.eval_episodes > 0, 'Must evaluate at least 1 episode.'
+	cfg.num_envs = 1 # to keep the code similar and logging video simpler
+	cfg = parse_cfg(cfg)
+	assert not cfg.multitask, colored('Warning: multi-task models is not currently supported for maniskill.', 'red', attrs=['bold'])
+	set_seed(cfg.seed)
+	print(colored(f'Task: {cfg.env_id}', 'blue', attrs=['bold']))
+	print(colored(f'Model size: {cfg.get("model_size", "default")}', 'blue', attrs=['bold']))
+	print(colored(f'Checkpoint: {cfg.checkpoint}', 'blue', attrs=['bold']))
+
+	# Make environment
+	env = make_envs(cfg)
+
+	# Load agent
+	agent = TDMPC2(cfg)
+	assert os.path.exists(cfg.checkpoint), f'Checkpoint {cfg.checkpoint} not found! Must be a valid filepath.'
+	agent.load(cfg.checkpoint)
+	
+	# Evaluate
+	if cfg.multitask:
+		print(colored(f'Evaluating agent on {len(cfg.tasks)} tasks:', 'yellow', attrs=['bold']))
+	else:
+		print(colored(f'Evaluating agent on {cfg.env_id}:', 'yellow', attrs=['bold']))
+	if cfg.save_video:
+		video_dir = os.path.join(cfg.work_dir, 'videos')
+		os.makedirs(video_dir, exist_ok=True)
+	scores = []
+	tasks = cfg.tasks if cfg.multitask else [cfg.env_id]
+	for task_idx, task in enumerate(tasks):
+		if not cfg.multitask:
+			task_idx = None
+		has_success, has_fail = False, False # if task has success or/and fail (added for maniskill)
+		ep_rewards, ep_successes, ep_fails = [], [], []
+		for i in range(cfg.eval_episodes):
+			obs, _ = env.reset()
+			done = False # ms3: done is truncated since the ms3 ignore_terminations.
+			ep_reward, t = 0, 0
+			if cfg.save_video:
+				frames = [env.render().squeeze()]
+			while not done: # done is truncated and should be the same
+				action = agent.act(obs, t0=t==0)
+				obs, reward, terminated, truncated, info = env.step(action)
+				done = terminated | truncated
+				ep_reward += reward
+				t += 1
+				if cfg.save_video:
+					frames.append(env.render().squeeze())
+			ep_rewards.append(ep_reward.mean().item())
+			if 'success' in info: 
+				has_success = True
+				ep_successes.append(info['success'].float().mean().item())
+			if 'fail' in info:
+				has_fail = True
+				ep_fails.append(info['fail'].float().mean().item())
+			if cfg.save_video:
+				imageio.mimsave(
+					os.path.join(video_dir, f'{task}-{i}.mp4'), frames, fps=15)
+		ep_rewards = np.nanmean(ep_rewards)
+		ep_successes = np.nanmean(ep_successes)
+		ep_fails = np.nanmean(ep_fails)
+		if cfg.multitask:
+			scores.append(ep_successes*100 if task.startswith('mw-') else ep_rewards/10)
+		if has_success and has_fail:
+			print(colored(f'  {task:<22}' \
+				f'\tR: {ep_rewards:.01f}  ' \
+				f'\tS: {ep_successes:.02f}' \
+				f'\tF: {ep_fails:.02f}', 'yellow'))
+		elif has_success:
+			print(colored(f'  {task:<22}' \
+				f'\tR: {ep_rewards:.01f}  ' \
+				f'\tS: {ep_successes:.02f}', 'yellow'))
+		elif has_fail:
+			print(colored(f'  {task:<22}' \
+				f'\tR: {ep_rewards:.01f}  ' \
+				f'\tF: {ep_fails:.02f}', 'yellow'))
+		
+	if cfg.multitask:
+		print(colored(f'Normalized score: {np.mean(scores):.02f}', 'yellow', attrs=['bold']))
+
+
+if __name__ == '__main__':
+	evaluate()
diff --git a/scripts/baselines/tdmpc2/tdmpc2.py b/scripts/baselines/tdmpc2/tdmpc2.py
new file mode 100644
index 0000000..73b76d3
--- /dev/null
+++ b/scripts/baselines/tdmpc2/tdmpc2.py
@@ -0,0 +1,306 @@
+import numpy as np
+import torch
+import torch.nn.functional as F
+
+from common import math
+from common.scale import RunningScale
+from common.world_model import WorldModel
+
+
+class TDMPC2:
+	"""
+	TD-MPC2 agent. Implements training + inference.
+	Can be used for both single-task and multi-task experiments,
+	and supports both state and pixel observations.
+	"""
+
+	def __init__(self, cfg):
+		self.cfg = cfg
+		self.device = torch.device('cuda')
+		self.model = WorldModel(cfg).to(self.device)
+		self.optim = torch.optim.Adam([
+			{'params': self.model._encoder.parameters(), 'lr': self.cfg.lr*self.cfg.enc_lr_scale},
+			{'params': self.model._dynamics.parameters()},
+			{'params': self.model._reward.parameters()},
+			{'params': self.model._Qs.parameters()},
+			{'params': self.model._task_emb.parameters() if self.cfg.multitask else []}
+		], lr=self.cfg.lr)
+		self.pi_optim = torch.optim.Adam(self.model._pi.parameters(), lr=self.cfg.lr, eps=1e-5)
+		self.model.eval()
+		self.scale = RunningScale(cfg)
+		self.cfg.iterations += 2*int(cfg.action_dim >= 20) # Heuristic for large action spaces
+		self.discount = torch.tensor(
+			[self._get_discount(ep_len) for ep_len in cfg.episode_lengths], device='cuda'
+		) if self.cfg.multitask else self._get_discount(cfg.episode_length)
+		
+		self.cfg.discount = self.discount
+
+	def _get_discount(self, episode_length):
+		"""
+		Returns discount factor for a given episode length.
+		Simple heuristic that scales discount linearly with episode length.
+		Default values should work well for most tasks, but can be changed as needed.
+
+		Args:
+			episode_length (int): Length of the episode. Assumes episodes are of fixed length.
+
+		Returns:
+			float: Discount factor for the task.
+		"""
+		frac = episode_length/self.cfg.discount_denom
+		return min(max((frac-1)/(frac), self.cfg.discount_min), self.cfg.discount_max)
+
+	def save(self, fp):
+		"""
+		Save state dict of the agent to filepath.
+		
+		Args:
+			fp (str): Filepath to save state dict to.
+		"""
+		torch.save({"model": self.model.state_dict()}, fp)
+
+	def load(self, fp):
+		"""
+		Load a saved state dict from filepath (or dictionary) into current agent.
+		
+		Args:
+			fp (str or dict): Filepath or state dict to load.
+		"""
+		state_dict = fp if isinstance(fp, dict) else torch.load(fp)
+		self.model.load_state_dict(state_dict["model"])
+
+	@torch.no_grad()
+	def act(self, obs, t0=False, eval_mode=False, task=None):
+		"""
+		Before: obs is 1d, return seems to be mu(1, action_dim)
+		After: obs is batched with num_env, return still 2d
+
+		Select an action by planning in the latent space of the world model.
+		
+		Args:
+			obs (torch.Tensor): Observation from the environment. 1d for online trainer
+			t0 (bool): Whether this is the first observation in the episode.
+			eval_mode (bool): Whether to use the mean of the action distribution.
+			task (int): Task index (only used for multi-task experiments).
+		
+		Returns:
+			torch.Tensor: Action to take in the environment.
+		"""
+		obs = obs.to(self.device, non_blocking=True)
+		if task is not None:
+			task = torch.tensor([task], device=self.device)
+		z = self.model.encode(obs, task) # [num_envs, latent_dim]
+		if self.cfg.mpc:
+			a = self.plan(z, t0=t0, eval_mode=eval_mode, task=task)
+		else:
+			a = self.model.pi(z, task)[int(not eval_mode)] # [int(not eval_mode)] selects mu or pi
+		return a.cpu()
+
+	@torch.no_grad()
+	def _estimate_value(self, z, actions, task):
+		"""z[num_samples, latent_dim], actions[horizon, num_samples, action_dim] -> [num_samples, 1]
+		Estimate value of a trajectory starting at latent state z and executing given actions."""
+		G, discount = 0, 1
+		for t in range(self.cfg.horizon):
+			reward = math.two_hot_inv(self.model.reward(z, actions[:, t], task), self.cfg)
+			z = self.model.next(z, actions[:, t], task)
+			G += discount * reward
+			discount *= self.discount[torch.tensor(task)] if self.cfg.multitask else self.discount
+		return G + discount * self.model.Q(z, self.model.pi(z, task)[1], task, return_type='avg')
+
+	@torch.no_grad()
+	def plan(self, z, t0=False, eval_mode=False, task=None):
+		"""
+		Before: For online, z[1, latent_dim]
+		After: For online z[num_envs, latent_dim]. Should be ok
+		Plan a sequence of actions using the learned world model.
+		
+		Args:
+			z (torch.Tensor): Latent state from which to plan.
+			t0 (bool): Whether this is the first observation in the episode.
+			eval_mode (bool): Whether to use the mean of the action distribution.
+			task (Torch.Tensor): Task index (only used for multi-task experiments).
+
+		Returns:
+			torch.Tensor: Action to take in the environment.
+		"""		
+		# Sample policy trajectories
+		if self.cfg.num_pi_trajs > 0:
+			pi_actions = torch.empty(self.cfg.num_envs, self.cfg.horizon, self.cfg.num_pi_trajs, self.cfg.action_dim, device=self.device)
+			_z = z.unsqueeze(1).repeat(1, self.cfg.num_pi_trajs, 1) # (num_envs, num_pi_trajs, latent_dim)
+			for t in range(self.cfg.horizon-1):
+				pi_actions[:, t] = self.model.pi(_z, task)[1]
+				_z = self.model.next(_z, pi_actions[:, t], task)
+			pi_actions[:, -1] = self.model.pi(_z, task)[1]
+
+		# Initialize state and parameters
+		z = z.unsqueeze(1).repeat(1, self.cfg.num_samples, 1) # (num_envs, num_samples, latent_dim)
+		mean = torch.zeros(self.cfg.num_envs, self.cfg.horizon, self.cfg.action_dim, device=self.device)
+		std = self.cfg.max_std*torch.ones(self.cfg.num_envs, self.cfg.horizon, self.cfg.action_dim, device=self.device)
+		if not t0 and hasattr(self, '_prev_mean'):
+			mean[:, :-1] = self._prev_mean[:, 1:]
+		actions = torch.empty(self.cfg.num_envs, self.cfg.horizon, self.cfg.num_samples, 
+						self.cfg.action_dim, device=self.device) # # (num_envs, horizon, num_samples, latent_dim)
+		if self.cfg.num_pi_trajs > 0:
+			actions[:, :, :self.cfg.num_pi_trajs] = pi_actions
+	
+		# Iterate MPPI
+		for _ in range(self.cfg.iterations):
+
+			# Sample actions
+			actions[:, :, self.cfg.num_pi_trajs:] = (mean.unsqueeze(2) + std.unsqueeze(2) * \
+				torch.randn(self.cfg.num_envs, self.cfg.horizon, self.cfg.num_samples-self.cfg.num_pi_trajs, self.cfg.action_dim, device=std.device)) \
+				.clamp(-1, 1)
+			if self.cfg.multitask:
+				actions = actions * self.model._action_masks[task]
+
+			# Compute elite actions
+			value = self._estimate_value(z, actions, task).nan_to_num_(0) # (num_envs, num_samples, 1)
+			elite_idxs = torch.topk(value.squeeze(2), self.cfg.num_elites, dim=1).indices # (num_envs, num_elites)
+			elite_value = value[torch.arange(self.cfg.num_envs).unsqueeze(1), elite_idxs] # (num_envs, num_elites, 1)
+			# elite_actions = torch.zeros(self.cfg.num_envs, self.cfg.horizon, self.cfg.num_elites, self.cfg.action_dim, dtype=actions.dtype, device=actions.device)
+			# for j, curr_elites in enumerate(elite_idxs):
+			# 	elite_actions[j] = actions[j, :, curr_elites]
+			elite_actions = torch.gather(actions, 2, elite_idxs.unsqueeze(1).unsqueeze(3).expand(-1, self.cfg.horizon, -1, self.cfg.action_dim))
+
+			# Update parameters
+			max_value = elite_value.max(1)[0] # (num_envs, 1)
+			score = torch.exp(self.cfg.temperature*(elite_value - max_value.unsqueeze(1)))
+			score /= score.sum(1, keepdim=True) # (num_envs, num_elites, 1)
+			mean = torch.sum(score.unsqueeze(1) * elite_actions, dim=2) / (score.sum(1, keepdim=True) + 1e-9)  # (num_envs, horizon, action_dim)
+			std = torch.sqrt(torch.sum(score.unsqueeze(1) * (elite_actions - mean.unsqueeze(2)) ** 2, dim=2) / (score.sum(1, keepdim=True) + 1e-9)) \
+				.clamp_(self.cfg.min_std, self.cfg.max_std) # (num_envs, horizon, action_dim)
+			if self.cfg.multitask:
+				mean = mean * self.model._action_masks[task]
+				std = std * self.model._action_masks[task]
+
+		# Select action
+		score = score.squeeze(2).cpu().numpy() # (num_envs, num_elites)
+		# (num_envs, horizon, num_elites, action_dim) for elite_actions
+		actions = torch.zeros(self.cfg.num_envs, self.cfg.horizon, self.cfg.action_dim, dtype=actions.dtype, device=actions.device)
+		for i in range(len(score)):
+			actions[i] = elite_actions[i, :, np.random.choice(np.arange(score.shape[1]), p=score[i])]
+		self._prev_mean = mean # (num_envs, horizon, action_dim)
+		a, std = actions[:, 0], std[:, 0]
+		if not eval_mode:
+			a += std * torch.randn(self.cfg.num_envs, self.cfg.action_dim, device=std.device)
+		return a.clamp_(-1, 1)
+		
+	def update_pi(self, zs, task):
+		"""
+		Update policy using a sequence of latent states.
+		
+		Args:
+			zs (torch.Tensor): Sequence of latent states.
+			task (torch.Tensor): Task index (only used for multi-task experiments).
+
+		Returns:
+			float: Loss of the policy update.
+		"""
+		self.pi_optim.zero_grad(set_to_none=True)
+		self.model.track_q_grad(False)
+		_, pis, log_pis, _ = self.model.pi(zs, task)
+		qs = self.model.Q(zs, pis, task, return_type='avg')
+		self.scale.update(qs[0])
+		qs = self.scale(qs)
+
+		# Loss is a weighted sum of Q-values
+		rho = torch.pow(self.cfg.rho, torch.arange(len(qs), device=self.device))
+		pi_loss = ((self.cfg.entropy_coef * log_pis - qs).mean(dim=(1,2)) * rho).mean()
+		pi_loss.backward()
+		torch.nn.utils.clip_grad_norm_(self.model._pi.parameters(), self.cfg.grad_clip_norm)
+		self.pi_optim.step()
+		self.model.track_q_grad(True)
+
+		return pi_loss.item()
+
+	@torch.no_grad()
+	def _td_target(self, next_z, reward, task):
+		"""
+		Compute the TD-target from a reward and the observation at the following time step.
+		
+		Args:
+			next_z (torch.Tensor): Latent state at the following time step.
+			reward (torch.Tensor): Reward at the current time step.
+			task (torch.Tensor): Task index (only used for multi-task experiments).
+		
+		Returns:
+			torch.Tensor: TD-target.
+		"""
+		pi = self.model.pi(next_z, task)[1]
+		discount = self.discount[task].unsqueeze(-1) if self.cfg.multitask else self.discount
+		return reward + discount * self.model.Q(next_z, pi, task, return_type='min', target=True)
+
+	def update(self, buffer):
+		"""
+		Main update function. Corresponds to one iteration of model learning.
+		
+		Args:
+			buffer (common.buffer.Buffer): Replay buffer.
+		
+		Returns:
+			dict: Dictionary of training statistics.
+		"""
+		obs, action, reward, task = buffer.sample()
+	
+		# Compute targets
+		with torch.no_grad():
+			next_z = self.model.encode(obs[1:], task)
+			td_targets = self._td_target(next_z, reward, task)
+
+		# Prepare for update
+		self.optim.zero_grad(set_to_none=True)
+		self.model.train()
+
+		# Latent rollout
+		zs = torch.empty(self.cfg.horizon+1, self.cfg.batch_size, self.cfg.latent_dim, device=self.device)
+		z = self.model.encode(obs[0], task)
+		zs[0] = z
+		consistency_loss = 0
+		for t in range(self.cfg.horizon):
+			z = self.model.next(z, action[t], task)
+			consistency_loss += F.mse_loss(z, next_z[t]) * self.cfg.rho**t
+			zs[t+1] = z
+
+		# Predictions
+		_zs = zs[:-1]
+		qs = self.model.Q(_zs, action, task, return_type='all')
+		reward_preds = self.model.reward(_zs, action, task)
+		
+		# Compute losses
+		reward_loss, value_loss = 0, 0
+		for t in range(self.cfg.horizon):
+			reward_loss += math.soft_ce(reward_preds[t], reward[t], self.cfg).mean() * self.cfg.rho**t
+			for q in range(self.cfg.num_q):
+				value_loss += math.soft_ce(qs[q][t], td_targets[t], self.cfg).mean() * self.cfg.rho**t
+		consistency_loss *= (1/self.cfg.horizon)
+		reward_loss *= (1/self.cfg.horizon)
+		value_loss *= (1/(self.cfg.horizon * self.cfg.num_q))
+		total_loss = (
+			self.cfg.consistency_coef * consistency_loss +
+			self.cfg.reward_coef * reward_loss +
+			self.cfg.value_coef * value_loss
+		)
+
+		# Update model
+		total_loss.backward()
+		grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.cfg.grad_clip_norm)
+		self.optim.step()
+
+		# Update policy
+		pi_loss = self.update_pi(zs.detach(), task)
+
+		# Update target Q-functions
+		self.model.soft_update_target_Q()
+
+		# Return training statistics
+		self.model.eval()
+		return {
+			"consistency_loss": float(consistency_loss.mean().item()),
+			"reward_loss": float(reward_loss.mean().item()),
+			"value_loss": float(value_loss.mean().item()),
+			"pi_loss": pi_loss,
+			"total_loss": float(total_loss.mean().item()),
+			"grad_norm": float(grad_norm),
+			"pi_scale": float(self.scale.value),
+		}
diff --git a/scripts/baselines/tdmpc2/train.py b/scripts/baselines/tdmpc2/train.py
new file mode 100644
index 0000000..6a24643
--- /dev/null
+++ b/scripts/baselines/tdmpc2/train.py
@@ -0,0 +1,65 @@
+import os
+os.environ['MUJOCO_GL'] = 'egl'
+os.environ['LAZY_LEGACY_OP'] = '0'
+import warnings
+warnings.filterwarnings('ignore')
+import torch
+
+import hydra
+from termcolor import colored
+
+from common.parser import parse_cfg
+from common.seed import set_seed
+from common.buffer import Buffer
+from envs import make_envs
+from tdmpc2 import TDMPC2
+from trainer.offline_trainer import OfflineTrainer
+from trainer.online_trainer import OnlineTrainer
+from common.logger import Logger
+
+import gymnasium as gym
+
+torch.backends.cudnn.benchmark = True
+
+
+@hydra.main(config_name='config', config_path='.')
+def train(cfg: dict):
+	"""
+	Script for training single-task / multi-task TD-MPC2 agents.
+
+	Most relevant args:
+		`task`: task name (or mt30/mt80 for multi-task training)
+		`model_size`: model size, must be one of `[1, 5, 19, 48, 317]` (default: 5)
+		`steps`: number of training/environment steps (default: 10M)
+		`seed`: random seed (default: 1)
+
+	See config.yaml for a full list of args.
+
+	Example usage:
+	```
+		$ python train.py task=mt80 model_size=48
+		$ python train.py task=mt30 model_size=317
+		$ python train.py task=dog-run steps=7000000
+	```
+	"""
+	assert torch.cuda.is_available()
+	assert cfg.steps > 0, 'Must train for at least 1 step.'
+	cfg = parse_cfg(cfg)
+	assert not cfg.multitask, colored('Warning: multi-task models is not currently supported for maniskill.', 'red', attrs=['bold'])
+	set_seed(cfg.seed)
+	print(colored('Work dir:', 'yellow', attrs=['bold']), cfg.work_dir)
+
+	trainer_cls = OnlineTrainer # OfflineTrainer if cfg.multitask else OnlineTrainer
+	trainer = trainer_cls(
+		cfg=cfg,
+		env=make_envs(cfg),
+		agent=TDMPC2(cfg),
+		buffer=Buffer(cfg),
+		logger=Logger(cfg),
+	)
+	trainer.train()
+	print('\nTraining completed successfully')
+
+
+if __name__ == '__main__':
+	train()
diff --git a/scripts/baselines/tdmpc2/trainer/__init__.py b/scripts/baselines/tdmpc2/trainer/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/scripts/baselines/tdmpc2/trainer/base.py b/scripts/baselines/tdmpc2/trainer/base.py
new file mode 100644
index 0000000..b7aa63a
--- /dev/null
+++ b/scripts/baselines/tdmpc2/trainer/base.py
@@ -0,0 +1,22 @@
+from tdmpc2 import TDMPC2
+from common.buffer import Buffer
+
+class Trainer:
+	"""Base trainer class for TD-MPC2."""
+
+	def __init__(self, cfg, env, agent, buffer, logger):
+		self.cfg = cfg
+		self.env = env
+		self.agent: TDMPC2 = agent
+		self.buffer: Buffer = buffer
+		self.logger = logger
+		print('Architecture:', self.agent.model)
+		print("Learnable parameters: {:,}".format(self.agent.model.total_params))
+
+	def eval(self):
+		"""Evaluate a TD-MPC2 agent."""
+		raise NotImplementedError
+
+	def train(self):
+		"""Train a TD-MPC2 agent."""
+		raise NotImplementedError
diff --git a/scripts/baselines/tdmpc2/trainer/offline_trainer.py b/scripts/baselines/tdmpc2/trainer/offline_trainer.py
new file mode 100644
index 0000000..46fd231
--- /dev/null
+++ b/scripts/baselines/tdmpc2/trainer/offline_trainer.py
@@ -0,0 +1,92 @@
+import os
+from copy import deepcopy
+from time import time
+from pathlib import Path
+from glob import glob
+
+import numpy as np
+import torch
+from tqdm import tqdm
+
+from common.buffer import Buffer
+from trainer.base import Trainer
+
+
+class OfflineTrainer(Trainer):
+	"""Trainer class for multi-task offline TD-MPC2 training. Not currently supported for Maniskill"""
+
+	def __init__(self, *args, **kwargs):
+		super().__init__(*args, **kwargs)
+		self._start_time = time()
+	
+	def eval(self):
+		"""Evaluate a TD-MPC2 agent."""
+		results = dict()
+		for task_idx in tqdm(range(len(self.cfg.tasks)), desc='Evaluating'):
+			ep_rewards, ep_successes = [], []
+			for _ in range(self.cfg.eval_episodes):
+				obs, done, ep_reward, t = self.env.reset(task_idx), False, 0, 0
+				while not done:
+					action = self.agent.act(obs, t0=t==0, eval_mode=True, task=task_idx)
+					obs, reward, done, info = self.env.step(action)
+					ep_reward += reward
+					t += 1
+				ep_rewards.append(ep_reward)
+				ep_successes.append(info['success'])
+			results.update({
+				f'episode_reward+{self.cfg.tasks[task_idx]}': np.nanmean(ep_rewards),
+				f'episode_success+{self.cfg.tasks[task_idx]}': np.nanmean(ep_successes),})
+		return results
+				
+	def train(self):
+		"""Train a TD-MPC2 agent."""
+		assert self.cfg.multitask and self.cfg.task in {'mt30', 'mt80'}, \
+			'Offline training only supports multitask training with mt30 or mt80 task sets.'
+
+		# Load data
+		assert self.cfg.task in self.cfg.data_dir, \
+			f'Expected data directory {self.cfg.data_dir} to contain {self.cfg.task}, ' \
+			f'please double-check your config.'
+		fp = Path(os.path.join(self.cfg.data_dir, '*.pt'))
+		fps = sorted(glob(str(fp)))
+		assert len(fps) > 0, f'No data found at {fp}'
+		print(f'Found {len(fps)} files in {fp}')
+	
+		# Create buffer for sampling
+		_cfg = deepcopy(self.cfg)
+		_cfg.episode_length = 101 if self.cfg.task == 'mt80' else 501
+		_cfg.buffer_size = 550_450_000 if self.cfg.task == 'mt80' else 345_690_000
+		_cfg.steps = _cfg.buffer_size
+		self.buffer = Buffer(_cfg)
+		for fp in tqdm(fps, desc='Loading data'):
+			td = torch.load(fp)
+			assert td.shape[1] == _cfg.episode_length, \
+				f'Expected episode length {td.shape[1]} to match config episode length {_cfg.episode_length}, ' \
+				f'please double-check your config.'
+			for i in range(len(td)):
+				self.buffer.add(td[i])
+		assert self.buffer.num_eps == self.buffer.capacity, \
+			f'Buffer has {self.buffer.num_eps} episodes, expected {self.buffer.capacity} episodes.'
+		
+		print(f'Training agent for {self.cfg.steps} iterations...')
+		metrics = {}
+		for i in range(self.cfg.steps):
+
+			# Update agent
+			train_metrics = self.agent.update(self.buffer)
+
+			# Evaluate agent periodically
+			if i % self.cfg.eval_freq == 0 or i % 10_000 == 0:
+				metrics = {
+					'iteration': i,
+					'total_time': time() - self._start_time,
+				}
+				metrics.update(train_metrics)
+				if i % self.cfg.eval_freq == 0:
+					metrics.update(self.eval())
+					self.logger.pprint_multitask(metrics, self.cfg)
+					if i > 0:
+						self.logger.save_agent(self.agent, identifier=f'{i}')
+				self.logger.log(metrics, 'pretrain')
+			
+		self.logger.finish(self.agent)
diff --git a/scripts/baselines/tdmpc2/trainer/online_trainer.py b/scripts/baselines/tdmpc2/trainer/online_trainer.py
new file mode 100644
index 0000000..2a320d1
--- /dev/null
+++ b/scripts/baselines/tdmpc2/trainer/online_trainer.py
@@ -0,0 +1,171 @@
+from time import time
+
+import numpy as np
+import torch
+from tensordict.tensordict import TensorDict
+
+from trainer.base import Trainer
+
+
+class OnlineTrainer(Trainer):
+	"""Trainer class for single-task online TD-MPC2 training."""
+
+	def __init__(self, *args, **kwargs):
+		super().__init__(*args, **kwargs)
+		self._step = 0
+		self._ep_idx = 0
+		self._start_time = time()
+
+	def common_metrics(self):
+		"""Return a dictionary of current metrics."""
+		return dict(
+			step=self._step,
+			episode=self._ep_idx,
+			total_time=time() - self._start_time,
+		)
+
+	def eval(self):
+		"""Evaluate a TD-MPC2 agent."""
+		has_success, has_fail = False, False # if task has success or/and fail (added for maniskill)
+		ep_rewards, ep_successes, ep_fails = [], [], []
+		for i in range((self.cfg.eval_episodes - 1) // self.cfg.num_envs + 1):
+			obs, _ = self.env.reset()
+			done = torch.full((self.cfg.num_envs, ), False, device=obs.device) # ms3: done is truncated since the ms3 ignore_terminations.
+			ep_reward, t = torch.zeros((self.cfg.num_envs, ), device=obs.device), 0
+			if self.cfg.save_video:
+				self.logger.video.init(self.env, enabled=(i==0))
+			while not done[0]: # done is truncated and should be the same
+				action = self.agent.act(obs, t0=t==0, eval_mode=True)
+				obs, reward, terminated, truncated, info = self.env.step(action)
+				done = terminated | truncated
+				ep_reward += reward
+				t += 1
+				if self.cfg.save_video:
+					self.logger.video.record(self.env)
+			ep_rewards.append(ep_reward.mean().item())
+
+			if 'success' in info: 
+				has_success = True
+				ep_successes.append(info['success'].float().mean().item())
+			
+			if 'fail' in info:
+				has_fail = True
+				ep_fails.append(info['fail'].float().mean().item())
+
+			if self.cfg.save_video:
+				self.logger.video.save(self._step)
+		eval_metrics = dict(
+			episode_reward=np.nanmean(ep_rewards),
+			episode_len=self.env.max_episode_steps,
+			episode_reward_avg=np.nanmean(ep_rewards)/self.env.max_episode_steps,
+		)
+		if has_success:
+			eval_metrics.update(episode_success=np.nanmean(ep_successes))
+		if has_fail:
+			eval_metrics.update(episode_fail=np.nanmean(ep_fails))
+		return eval_metrics
+
+	def to_td(self, obs, num_envs, action=None, reward=None):
+		"""Before: Creates a TensorDict for a new episode. Return a td with batch (1, ), with obs, action, reward
+		After vectorization: added 1 argument: num_envs, now have batch size (num_envs, 1)"""
+		if isinstance(obs, dict): 
+			obs = TensorDict(obs, batch_size=(), device='cpu') # before vectorization, obs must have its first dimension=1
+		else:
+			obs = obs.unsqueeze(1).cpu()
+		if action is None:
+			action = torch.full((num_envs, self.cfg.action_dim), float('nan')) 
+		if reward is None:
+			reward = torch.full((num_envs,), float('nan'))
+		td = TensorDict(dict(
+			obs=obs,
+			action=action.unsqueeze(1),
+			reward=reward.cpu().unsqueeze(1),
+		), batch_size=(num_envs, 1))
+		return td
+
+	def train(self):
+		"""Train a TD-MPC2 agent."""
+		train_metrics, time_metrics, vec_done, eval_next = {}, {}, [True], True
+		seed_finish = False
+
+		rollout_times = []
+
+		while self._step <= self.cfg.steps:
+
+			# Evaluate agent periodically
+			if self._step % self.cfg.eval_freq < self.cfg.num_envs:
+				eval_next = True
+
+			# Reset environment
+			if vec_done[0]:
+				if eval_next:
+					eval_metrics = self.eval()
+					eval_metrics.update(self.common_metrics())
+					self.logger.log(eval_metrics, 'eval')
+					eval_next = False
+
+				if self._step > 0:
+					tds = torch.cat(self._tds, dim=1) # [num_envs, episode_len + 1, ..]. Note it's different from official vectorized code
+					train_metrics.update(
+						episode_reward=tds['reward'].nansum(1).mean(), # first NaN is dropped by nansum
+						episode_len=self.env.max_episode_steps,
+						episode_reward_avg=tds['reward'].nansum(1).mean()/self.env.max_episode_steps,
+					)
+					if 'success' in vec_info:
+						train_metrics.update(episode_success=vec_info['success'].float().mean().item())
+					if 'fail' in vec_info:
+						train_metrics.update(episode_fail=vec_info['fail'].float().mean().item())
+
+					if seed_finish:
+						time_metrics.update(
+							rollout_time=np.mean(rollout_times),
+							rollout_fps=self.cfg.num_envs/np.mean(rollout_times), # self.cfg.num_envs * len(rollout_times)@steps_per_env@ /sum(rollout_times)
+							update_time=update_time,
+						)
+						time_metrics.update(self.common_metrics())
+						self.logger.log(time_metrics, 'time')
+						rollout_times = []
+
+					train_metrics.update(self.common_metrics())
+					self.logger.log(train_metrics, 'train')
+
+					assert len(self._tds) == self.env.max_episode_steps + 1, f"{len(self._tds)} instead" # ManiSkillVectorEnv wrapper required
+					self._ep_idx = self.buffer.add(tds) 
+
+				obs, _ = self.env.reset()
+				self._tds = [self.to_td(obs, self.cfg.num_envs)]
+
+			# Collect experience
+			rollout_time = time()
+			if self._step > self.cfg.seed_steps:
+				action = self.agent.act(obs, t0=len(self._tds)==1) # t0 unchanged since all envs have same episode length
+			else:
+				# action = torch.rand((self.cfg.num_envs, self.cfg.action_dim)) # self.env.rand_act()
+				action = torch.from_numpy(self.env.action_space.sample())
+			obs, reward, vec_terminated, vec_truncated, vec_info = self.env.step(action)
+
+			vec_done = vec_terminated | vec_truncated
+			if vec_done[0] and self.cfg.obs == 'state': # added in vectorization
+				obs = vec_info["final_observation"]
+
+			self._tds.append(self.to_td(obs, self.cfg.num_envs, action, reward))
+			rollout_time = time() - rollout_time
+			rollout_times.append(rollout_time)
+			
+			# Update agent
+			if self._step >= self.cfg.seed_steps:
+				update_time = time()
+				if not seed_finish:
+					seed_finish = True
+					num_updates = int(self.cfg.seed_steps / self.cfg.steps_per_update)
+					print('Pretraining agent on seed data...')
+				else:
+					num_updates = max(1, int(self.cfg.num_envs / self.cfg.steps_per_update))
+				for _ in range(num_updates):
+					_train_metrics = self.agent.update(self.buffer)
+				train_metrics.update(_train_metrics)
+				update_time = time() - update_time
+
+			self._step += self.cfg.num_envs
+	
+		self.logger.finish(self.agent)
