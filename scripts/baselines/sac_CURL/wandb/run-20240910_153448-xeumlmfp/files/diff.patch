diff --git a/scripts/baselines/sac_AMDF/sac.py b/scripts/baselines/sac_AMDF/sac.py
index f9a2dc5..b65f8dc 100644
--- a/scripts/baselines/sac_AMDF/sac.py
+++ b/scripts/baselines/sac_AMDF/sac.py
@@ -230,7 +230,8 @@ def train(**kwargs):
             obs_stack = real_next_obs_stack
             obs = real_next_obs
             old_obs = obs
-            old_action = copy.deepcopy(actions)
+            state = copy.deepcopy(next_state)
+            old_action = actions.clone()
 
         rollout_time = time.time() - rollout_time
 
diff --git a/scripts/baselines/sac_CURL/sac.py b/scripts/baselines/sac_CURL/sac.py
index a174afb..830a9fc 100644
--- a/scripts/baselines/sac_CURL/sac.py
+++ b/scripts/baselines/sac_CURL/sac.py
@@ -63,7 +63,6 @@ def train(**kwargs):
     ## GLOBAL LOOP
     while global_step < args.total_timesteps:
         print(f"Global Step: {global_step}", end='\r')
-        print(global_step)
         ## EVALUATION and SAVING
         if args.eval_freq > 0 and (global_step - args.training_freq) // args.eval_freq < global_step // args.eval_freq:
             # evaluate
@@ -146,6 +145,7 @@ def train(**kwargs):
         old_obs, info = envs.reset_mm(seed=args.seed)
         obs, _, _, _, _ = envs.step_mm(envs.action_space.sample())
         obs_stack = process_obs_dict(obs, old_obs, args.modes, device)
+        state = envs.get_state()
 
         ## ROLLOUT (try not to modify: CRUCIAL step easy to overlook)
         for local_step in range(args.steps_per_env):
@@ -180,10 +180,19 @@ def train(**kwargs):
                 writer.add_scalar("charts/episodic_return", episodic_return, global_step)
                 writer.add_scalar("charts/episodic_length", final_info["elapsed_steps"][done_mask].cpu().numpy().mean(),
                                   global_step)
-
+            next_state = envs.get_state()
             real_next_obs_stack = process_obs_dict(next_obs, obs, args.modes, device)
-            rb.add(obs_stack, real_next_obs_stack, actions, rewards, next_done)
+            rb.add(
+                states=state,
+                obs=obs_stack,
+                next_states=next_state,
+                next_obs=real_next_obs_stack,
+                action=actions,
+                reward=rewards,
+                done=next_done
+            )
             obs_stack = real_next_obs_stack
+            state=next_state.clone()
         rollout_time = time.time() - rollout_time
 
         ## UPDATING AGENT (ALGO LOGIC: training.)
@@ -206,14 +215,14 @@ def train(**kwargs):
             with torch.no_grad():
 
                 next_state_actions, next_state_log_pi, _ = actor.get_train_action(transform(data.next_obs, obs_trans))
-                qf1_next_target = qf1_target(transform(data.next_obs, state_trans)[0], next_state_actions)
-                qf2_next_target = qf2_target(transform(data.next_obs, state_trans)[0], next_state_actions)
+                qf1_next_target = qf1_target(data.next_states, next_state_actions)
+                qf2_next_target = qf2_target(data.next_states, next_state_actions)
                 min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
                 next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
                 # data.dones is "stop_bootstrap", which is computed earlier according to args.bootstrap_at_done
 
-            qf1_a_values = qf1(transform(data.next_obs, state_trans)[0], data.actions).view(-1)
-            qf2_a_values = qf2(transform(data.next_obs, state_trans)[0], data.actions).view(-1)
+            qf1_a_values = qf1(data.states, data.actions).view(-1)
+            qf2_a_values = qf2(data.states, data.actions).view(-1)
             qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
             qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
             qf_loss = qf1_loss + qf2_loss
@@ -225,8 +234,8 @@ def train(**kwargs):
             # update the policy network and the critic
             if global_update % args.policy_frequency == 0:  # TD 3 Delayed update support
                 pi, log_pi, _ = actor.get_train_action(transform(data.next_obs, obs_trans))
-                qf1_pi = qf1(transform(data.next_obs, state_trans)[0], pi)
-                qf2_pi = qf2(transform(data.next_obs, state_trans)[0], pi)
+                qf1_pi = qf1(data.states, pi)
+                qf2_pi = qf2(data.states, pi)
                 min_qf_pi = torch.min(qf1_pi, qf2_pi)
                 actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
 
@@ -454,17 +463,13 @@ if __name__ == "__main__":
         action_space=envs.single_action_space,
         modes=args.modes
     ).to(device)
-    main_mode=args.modes[0]
-    i = envs.obs_modes.index(main_mode)
-    input_dim = (*args.crop_size, envs.single_observation_space_mm[i].shape[-1] )
-    qf1 = SoftQNetwork(input_dim=input_dim,
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
-    qf2 = SoftQNetwork(input_dim=input_dim,
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
-    qf1_target = SoftQNetwork(input_dim=input_dim,
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
-    qf2_target = SoftQNetwork(input_dim=input_dim,
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
+    state_dim = envs.single_state_shape
+    act_dim = envs.single_action_space.shape[0]
+    main_mode='state'
+    qf1 = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
+    qf2 = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
+    qf1_target = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
+    qf2_target = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
 
     if args.checkpoint is not None:
         ckpt = torch.load(args.checkpoint)
@@ -490,6 +495,7 @@ if __name__ == "__main__":
     print('... replay buffer setup', end='\r')
     envs.single_observation_space_mm.dtype = np.float32
     rb = ReplayBuffer(
+        state_shape=state_dim,
         obs_shape=[envs.single_observation_space.spaces['sensor_data'][envs.data_source][mode].shape[:2] for mode in args.modes],
         act_shape=envs.single_action_space.shape,
         num_envs=args.num_envs,
diff --git a/scripts/baselines/sac_CURL/utils.py b/scripts/baselines/sac_CURL/utils.py
index b1c1dd6..7b9a38a 100644
--- a/scripts/baselines/sac_CURL/utils.py
+++ b/scripts/baselines/sac_CURL/utils.py
@@ -101,84 +101,18 @@ class Args:
     NTC_coeff: float = 1.0
     """cefficients for the representation loss"""
 
-
-
-
-@dataclass
-class ReplayBufferSample:
-    obs: torch.Tensor
-    next_obs: torch.Tensor
-    actions: torch.Tensor
-    rewards: torch.Tensor
-    dones: torch.Tensor
-class ReplayBuffer:
-    def __init__(self, env, num_envs: int, buffer_size: int, storage_device: torch.device, sample_device: torch.device):
-        self.buffer_size = buffer_size
-        self.pos = 0
-        self.full = False
-        self.num_envs = num_envs
-        self.storage_device = storage_device
-        self.sample_device = sample_device
-        self.obs = torch.zeros((buffer_size, num_envs) + env.single_observation_space.shape).to(storage_device)
-        self.next_obs = torch.zeros((buffer_size, num_envs) + env.single_observation_space.shape).to(storage_device)
-        self.actions = torch.zeros((buffer_size, num_envs) + env.single_action_space.shape).to(storage_device)
-        self.logprobs = torch.zeros((buffer_size, num_envs)).to(storage_device)
-        self.rewards = torch.zeros((buffer_size, num_envs)).to(storage_device)
-        self.dones = torch.zeros((buffer_size, num_envs)).to(storage_device)
-        self.values = torch.zeros((buffer_size, num_envs)).to(storage_device)
-
-    def add(self, obs: torch.Tensor, next_obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, done: torch.Tensor):
-        if self.storage_device == torch.device("cpu"):
-            obs = obs.cpu()
-            next_obs = next_obs.cpu()
-            action = action.cpu()
-            reward = reward.cpu()
-            done = done.cpu()
-
-        self.obs[self.pos] = obs
-        self.next_obs[self.pos] = next_obs
-
-        self.actions[self.pos] = action
-        self.rewards[self.pos] = reward
-        self.dones[self.pos] = done
-
-        self.pos += 1
-        if self.pos == self.buffer_size:
-            self.full = True
-            self.pos = 0
-    def sample(self, batch_size: int):
-        if self.full:
-            batch_inds = torch.randint(0, self.buffer_size, size=(batch_size, ))
-        else:
-            batch_inds = torch.randint(0, self.pos, size=(batch_size, ))
-        env_inds = torch.randint(0, self.num_envs, size=(batch_size, ))
-        return ReplayBufferSample(
-            obs=self.obs[batch_inds, env_inds].to(self.sample_device),
-            next_obs=self.next_obs[batch_inds, env_inds].to(self.sample_device),
-            actions=self.actions[batch_inds, env_inds].to(self.sample_device),
-            rewards=self.rewards[batch_inds, env_inds].to(self.sample_device),
-            dones=self.dones[batch_inds, env_inds].to(self.sample_device)
-        )
-
-
-
-'''
-obs: torch.Tensor
-img: torch.Tensor
-next_obs: torch.Tensor
-next_img: torch.Tensor
-'''
-
 ########## MULTIMODAL
 @dataclass
 class ReplayBufferMultimodalSample:
+    states: torch.Tensor
+    next_states: torch.Tensor
     obs: tuple[torch.Tensor]
     next_obs: tuple[torch.Tensor]
     actions: torch.Tensor
     rewards: torch.Tensor
     dones: torch.Tensor
 class ReplayBufferMultimodal:
-    def __init__(self, obs_shape:tuple, act_shape:tuple, num_envs: int, buffer_size: int, storage_device: torch.device, sample_device: torch.device, frames:int = 2):
+    def __init__(self, state_shape:tuple, obs_shape:tuple, act_shape:tuple, num_envs: int, buffer_size: int, storage_device: torch.device, sample_device: torch.device, frames:int = 2):
         self.buffer_size = buffer_size
         self.pos = 0
         self.full = False
@@ -188,6 +122,8 @@ class ReplayBufferMultimodal:
 
         obs_shape = [(buffer_size, num_envs) + (frames, *s) for s in obs_shape]
 
+        self.states = torch.zeros((buffer_size, num_envs) + state_shape).to(storage_device)
+        self.next_states = torch.zeros((buffer_size, num_envs) + state_shape).to(storage_device)
         self.obs = [torch.zeros(s).to(storage_device) for s in obs_shape]
         self.next_obs = [torch.zeros(s).to(storage_device) for s in obs_shape]
         self.actions = torch.zeros((buffer_size, num_envs) + act_shape).to(storage_device)
@@ -196,9 +132,13 @@ class ReplayBufferMultimodal:
         self.dones = torch.zeros((buffer_size, num_envs)).to(storage_device)
         self.values = torch.zeros((buffer_size, num_envs)).to(storage_device)
 
-    def add(self, obs: tuple[torch.Tensor], next_obs: tuple[torch.Tensor],
+    def add(self, states, obs: tuple[torch.Tensor], next_states, next_obs: tuple[torch.Tensor],
             action: torch.Tensor, reward: torch.Tensor, done: torch.Tensor):
 
+        self.states[self.pos] = states.to(self.storage_device)
+        self.next_states[self.pos] = next_states.to(self.storage_device)
+
+
         for i, (o, on) in enumerate(zip(obs,next_obs)):
             self.obs[i][self.pos] = o.to(self.storage_device)
             self.next_obs[i][self.pos] = on.to(self.storage_device)
@@ -219,13 +159,12 @@ class ReplayBufferMultimodal:
             batch_inds = torch.randint(0, self.pos, size=(batch_size, ))
         env_inds = torch.randint(0, self.num_envs, size=(batch_size, ))
         return ReplayBufferMultimodalSample(
+            states=self.states[batch_inds, env_inds].to(self.sample_device),
+            next_states=self.next_states[batch_inds, env_inds].to(self.sample_device),
             obs=tuple([o[batch_inds, env_inds].to(self.sample_device) for o in self.obs]),
             next_obs=tuple([o[batch_inds, env_inds].to(self.sample_device) for o in self.next_obs]),
-            #obs=self.obs[batch_inds, env_inds].to(self.sample_device),
-            #img=self.img[batch_inds, env_inds].to(self.sample_device),
-            #next_obs=self.next_obs[batch_inds, env_inds].to(self.sample_device),
-            #next_img=self.next_img[batch_inds, env_inds].to(self.sample_device),
             actions=self.actions[batch_inds, env_inds].to(self.sample_device),
             rewards=self.rewards[batch_inds, env_inds].to(self.sample_device),
             dones=self.dones[batch_inds, env_inds].to(self.sample_device)
         )
+
diff --git a/scripts/baselines/sac_CURL/wandb/debug-internal.log b/scripts/baselines/sac_CURL/wandb/debug-internal.log
index 8b05464..d28b1ba 120000
--- a/scripts/baselines/sac_CURL/wandb/debug-internal.log
+++ b/scripts/baselines/sac_CURL/wandb/debug-internal.log
@@ -1 +1 @@
-run-20240909_132947-2lgklg11/logs/debug-internal.log
\ No newline at end of file
+run-20240910_153448-xeumlmfp/logs/debug-internal.log
\ No newline at end of file
diff --git a/scripts/baselines/sac_CURL/wandb/debug.log b/scripts/baselines/sac_CURL/wandb/debug.log
index e7fe55b..4fdc6c0 120000
--- a/scripts/baselines/sac_CURL/wandb/debug.log
+++ b/scripts/baselines/sac_CURL/wandb/debug.log
@@ -1 +1 @@
-run-20240909_132947-2lgklg11/logs/debug.log
\ No newline at end of file
+run-20240910_153448-xeumlmfp/logs/debug.log
\ No newline at end of file
diff --git a/scripts/baselines/sac_CURL/wandb/latest-run b/scripts/baselines/sac_CURL/wandb/latest-run
index 4a52ea5..39c058a 120000
--- a/scripts/baselines/sac_CURL/wandb/latest-run
+++ b/scripts/baselines/sac_CURL/wandb/latest-run
@@ -1 +1 @@
-run-20240909_132947-2lgklg11
\ No newline at end of file
+run-20240910_153448-xeumlmfp
\ No newline at end of file
diff --git a/scripts/baselines/sac_TWN/sac.py b/scripts/baselines/sac_TWN/sac.py
index 3f6b9d7..e636a16 100644
--- a/scripts/baselines/sac_TWN/sac.py
+++ b/scripts/baselines/sac_TWN/sac.py
@@ -12,6 +12,7 @@ import torch.nn.functional as F
 import torch.optim as optim
 from torch.utils.tensorboard import SummaryWriter
 import tyro
+from tqdm import tqdm
 from models import ActorMultimodal as Actor
 from models import SoftQNetworkMultimodal as SoftQNetwork
 
@@ -57,8 +58,9 @@ def train(**kwargs):
             actor.eval()
             print("Evaluating")
             old_eval_obs, _ = eval_envs.reset_mm(seed=args.seed)
-            old_action = torch.zeros(eval_envs.action_space.shape)
-            eval_obs, _, _, _, _ = eval_envs.step_mm(eval_envs.action_space.sample())
+            actions = eval_envs.action_space.sample()
+            eval_obs, _, _, _, _ = eval_envs.step_mm(actions)
+            old_actions = torch.from_numpy(actions).float().to(device)
 
             z_old = torch.stack(actor.get_encodings(process_obs_dict(eval_obs, old_eval_obs, args.modes, device)), dim=0).mean(dim=0)
             old_eval_obs = copy.deepcopy(eval_obs)
@@ -72,9 +74,9 @@ def train(**kwargs):
             #EVALUATION
             for _ in range(args.num_eval_steps):
                 with torch.no_grad():
-                    action, z_old = actor.get_eval_action(process_obs_dict(eval_obs, old_eval_obs, args.modes, device), z_old, old_action.to(device))
+                    action, z_old = actor.get_eval_action(process_obs_dict(eval_obs, old_eval_obs, args.modes, device), z_old, old_actions.to(device))
                     next_eval_obs, _, eval_terminations, eval_truncations, eval_infos = eval_envs.step_mm(action)
-                    old_action = action.clone()
+                    old_actions = action.clone()
                     old_eval_obs = copy.deepcopy(eval_obs)
                     eval_obs = copy.deepcopy(next_eval_obs)
 
@@ -134,9 +136,12 @@ def train(**kwargs):
         ## ROLLOUT (try not to modify: CRUCIAL step easy to overlook)
         rollout_time = time.time()
         old_obs, info = envs.reset_mm(seed=args.seed)
-        obs, _, _, _, _ = envs.step_mm(envs.action_space.sample())
+        actions = eval_envs.action_space.sample()
+        obs, _, _, _, _ = envs.step_mm(actions)
+        old_actions = torch.from_numpy(actions).float().to(device)
 
         obs_stack = process_obs_dict(obs, old_obs, args.modes, device)
+        state = envs.get_state()
 
         for local_step in range(args.steps_per_env):
             global_step += 1 * args.num_envs
@@ -171,10 +176,21 @@ def train(**kwargs):
                 writer.add_scalar("charts/episodic_length", final_info["elapsed_steps"][done_mask].cpu().numpy().mean(),
                                   global_step)
 
+            next_state = envs.get_state()
             real_next_obs_stack = process_obs_dict(next_obs, obs, args.modes, device)
 
-            rb.add(obs_stack, real_next_obs_stack, actions, rewards, next_done)
+            rb.add(
+                states=state,
+                obs=obs_stack,
+                next_states=next_state,
+                next_obs=real_next_obs_stack,
+                old_action=old_actions,
+                action=actions,
+                reward=rewards,
+                done=next_done
+            )
             obs_stack = real_next_obs_stack
+            old_actions = actions.clone()
 
         rollout_time = time.time() - rollout_time
 
@@ -182,8 +198,7 @@ def train(**kwargs):
         if global_step < args.learning_starts:
             continue
 
-        #print(f'Updating at step {global_step}')
-
+        pbar = tqdm(total=args.grad_steps_per_iteration, desc='Updating')
         update_time = time.time()
         learning_has_started = True
         for local_update in range(args.grad_steps_per_iteration):
@@ -193,14 +208,14 @@ def train(**kwargs):
             # update the value networks
             with torch.no_grad():
                 next_state_actions, next_state_log_pi, _ = actor.get_train_action(data.next_obs)
-                qf1_next_target = qf1_target(data.next_obs[0], next_state_actions)
-                qf2_next_target = qf2_target(data.next_obs[0], next_state_actions)
+                qf1_next_target = qf1_target(data.next_states, next_state_actions)
+                qf2_next_target = qf2_target(data.next_states, next_state_actions)
                 min_qf_next_target = torch.min(qf1_next_target, qf2_next_target) - alpha * next_state_log_pi
                 next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)
                 # data.dones is "stop_bootstrap", which is computed earlier according to args.bootstrap_at_done
 
-            qf1_a_values = qf1(data.obs[0], data.actions).view(-1)
-            qf2_a_values = qf2(data.obs[0], data.actions).view(-1)
+            qf1_a_values = qf1(data.states, data.actions).view(-1)
+            qf2_a_values = qf2(data.states, data.actions).view(-1)
             qf1_loss = F.mse_loss(qf1_a_values, next_q_value)
             qf2_loss = F.mse_loss(qf2_a_values, next_q_value)
             qf_loss = qf1_loss + qf2_loss
@@ -212,13 +227,12 @@ def train(**kwargs):
             # update the policy network
             if global_update % args.policy_frequency == 0:  # TD 3 Delayed update support
                 pi, log_pi, _ = actor.get_train_action(data.obs)
-                qf1_pi = qf1(data.obs[0], pi)
-                qf2_pi = qf2(data.obs[0], pi)
+                qf1_pi = qf1(data.states, pi)
+                qf2_pi = qf2(data.states, pi)
                 min_qf_pi = torch.min(qf1_pi, qf2_pi)
                 actor_loss = ((alpha * log_pi) - min_qf_pi).mean()
 
                 # update the encoders
-
                 if len(args.modes)>1:
                     z_obs_0, z_0, z_obs_1, z_1, z_act_1 = actor.get_representations(data)
 
@@ -257,6 +271,9 @@ def train(**kwargs):
                     target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
                 for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):
                     target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)
+
+            pbar.update()
+        pbar.close()
         update_time = time.time() - update_time
 
         ## LOGGING (Log training-related data)
@@ -392,15 +409,13 @@ if __name__ == "__main__":
     max_action = float(envs.single_action_space.high[0])
 
     actor = Actor(envs).to(device)
-    main_mode=args.modes[0]
-    qf1 = SoftQNetwork(input_dim=envs.single_observation_space.spaces['sensor_data'][envs.data_source][main_mode],
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
-    qf2 = SoftQNetwork(input_dim=envs.single_observation_space.spaces['sensor_data'][envs.data_source][main_mode],
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
-    qf1_target = SoftQNetwork(input_dim=envs.single_observation_space.spaces['sensor_data'][envs.data_source][main_mode],
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
-    qf2_target = SoftQNetwork(input_dim=envs.single_observation_space.spaces['sensor_data'][envs.data_source][main_mode],
-                       output_dim = envs.single_action_space.shape[0], mode=main_mode).to(device)
+    state_dim = envs.single_state_shape
+    act_dim = envs.single_action_space.shape[0]
+    main_mode='state'
+    qf1 = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
+    qf2 = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
+    qf1_target = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
+    qf2_target = SoftQNetwork(input_dim=state_dim[0],  output_dim = act_dim, mode=main_mode).to(device)
 
     if args.checkpoint is not None:
         ckpt = torch.load(args.checkpoint)
@@ -426,6 +441,7 @@ if __name__ == "__main__":
     print('... replay buffer setup', end='\r')
     envs.single_observation_space_mm.dtype = np.float32
     rb = ReplayBuffer(
+        state_shape=state_dim,
         obs_shape=[envs.single_observation_space.spaces['sensor_data'][envs.data_source][mode].shape[:2] for mode in args.modes],
         act_shape=envs.single_action_space.shape,
         num_envs=args.num_envs,
diff --git a/scripts/baselines/sac_TWN/utils.py b/scripts/baselines/sac_TWN/utils.py
index 7bd92e8..05a83e8 100644
--- a/scripts/baselines/sac_TWN/utils.py
+++ b/scripts/baselines/sac_TWN/utils.py
@@ -172,13 +172,16 @@ next_img: torch.Tensor
 ########## MULTIMODAL
 @dataclass
 class ReplayBufferMultimodalSample:
+    states: torch.Tensor
+    next_states: torch.Tensor
     obs: tuple[torch.Tensor]
     next_obs: tuple[torch.Tensor]
+    old_actions: torch.Tensor
     actions: torch.Tensor
     rewards: torch.Tensor
     dones: torch.Tensor
 class ReplayBufferMultimodal:
-    def __init__(self, obs_shape:tuple, act_shape:tuple, num_envs: int, buffer_size: int, storage_device: torch.device, sample_device: torch.device, frames:int = 2):
+    def __init__(self, state_shape:tuple, obs_shape:tuple, act_shape:tuple, num_envs: int, buffer_size: int, storage_device: torch.device, sample_device: torch.device, frames:int = 2):
         self.buffer_size = buffer_size
         self.pos = 0
         self.full = False
@@ -188,21 +191,29 @@ class ReplayBufferMultimodal:
 
         obs_shape = [(buffer_size, num_envs) + (frames, *s) for s in obs_shape]
 
+        self.states = torch.zeros((buffer_size, num_envs) + state_shape).to(storage_device)
+        self.next_states = torch.zeros((buffer_size, num_envs) + state_shape).to(storage_device)
         self.obs = [torch.zeros(s).to(storage_device) for s in obs_shape]
         self.next_obs = [torch.zeros(s).to(storage_device) for s in obs_shape]
+        self.old_actions = torch.zeros((buffer_size, num_envs) + act_shape).to(storage_device)
         self.actions = torch.zeros((buffer_size, num_envs) + act_shape).to(storage_device)
         self.logprobs = torch.zeros((buffer_size, num_envs)).to(storage_device)
         self.rewards = torch.zeros((buffer_size, num_envs)).to(storage_device)
         self.dones = torch.zeros((buffer_size, num_envs)).to(storage_device)
         self.values = torch.zeros((buffer_size, num_envs)).to(storage_device)
 
-    def add(self, obs: tuple[torch.Tensor], next_obs: tuple[torch.Tensor],
-            action: torch.Tensor, reward: torch.Tensor, done: torch.Tensor):
+    def add(self, states, obs: tuple[torch.Tensor], next_states, next_obs: tuple[torch.Tensor],
+            old_action: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, done: torch.Tensor):
+
+        self.states[self.pos] = states.to(self.storage_device)
+        self.next_states[self.pos] = next_states.to(self.storage_device)
+
 
         for i, (o, on) in enumerate(zip(obs,next_obs)):
             self.obs[i][self.pos] = o.to(self.storage_device)
             self.next_obs[i][self.pos] = on.to(self.storage_device)
 
+        self.old_actions[self.pos] = old_action.to(self.storage_device)
         self.actions[self.pos] = action.to(self.storage_device)
         self.rewards[self.pos] = reward.to(self.storage_device)
         self.dones[self.pos] = done.to(self.storage_device)
@@ -219,12 +230,11 @@ class ReplayBufferMultimodal:
             batch_inds = torch.randint(0, self.pos, size=(batch_size, ))
         env_inds = torch.randint(0, self.num_envs, size=(batch_size, ))
         return ReplayBufferMultimodalSample(
+            states=self.states[batch_inds, env_inds].to(self.sample_device),
+            next_states=self.next_states[batch_inds, env_inds].to(self.sample_device),
             obs=tuple([o[batch_inds, env_inds].to(self.sample_device) for o in self.obs]),
             next_obs=tuple([o[batch_inds, env_inds].to(self.sample_device) for o in self.next_obs]),
-            #obs=self.obs[batch_inds, env_inds].to(self.sample_device),
-            #img=self.img[batch_inds, env_inds].to(self.sample_device),
-            #next_obs=self.next_obs[batch_inds, env_inds].to(self.sample_device),
-            #next_img=self.next_img[batch_inds, env_inds].to(self.sample_device),
+            old_actions=self.old_actions[batch_inds, env_inds].to(self.sample_device),
             actions=self.actions[batch_inds, env_inds].to(self.sample_device),
             rewards=self.rewards[batch_inds, env_inds].to(self.sample_device),
             dones=self.dones[batch_inds, env_inds].to(self.sample_device)
